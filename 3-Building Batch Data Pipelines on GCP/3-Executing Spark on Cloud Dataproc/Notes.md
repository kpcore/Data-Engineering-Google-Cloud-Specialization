## 1. The Hadoop ecosystem

* Welcome to the module and executing Apache Spark on Cloud Dataproc. In this module, we'll review the parts of the Hadoop ecosystem. How to run Hadoop on Cloud Dataproc. Why you should consider using GCS instead of HDFS for your storage. How to optimize Cloud Dataproc. And lastly, a lab for you to practice what you've just learned. It helps to place the services you'll be learning about in a historical context. Before 2006, big data simply meant big databases. Database design came from a time where storage was relatively cheap, and processing, the compute power, was rather expensive. So it made sense to copy the data from its storage location to the processor to perform data processing. Then the result would then be copied back to storage. Around 2006, distributed processing of that big data became practical with Apache Hadoop. The idea behind Hadoop is to create a cluster of computers and leverage distributed processing. HDFS, Hadoop's distributed file system, stored the data on the machines in the cluster, and MapReduce provided distributed processing or compute over all of that data. A whole ecosystem of Hadoop-related software grew up around Hadoop, including Hive, Pig, and Spark. So what is Hadoop used for these days? Organizations use Hadoop for on-premises big data workloads using distributed data processing via MapReduce. They make use of a range of applications that run on Hadoop clusters, such as Presto. But a lot of customers also use Spark. Spark provides a high-performance analytics engine for processing batch and streaming data. Spark can be up to a hundred times faster than equivalent Hadoop jobs, because it leverages in-memory processing. Spark also provides a couple of abstractions for dealing with data, including resilient distributed data sets and data frames. Spark in particular is very powerful and expressive. It's used for a lot of those Hadoop workloads. So where did Apache Spark come from? Wouldn't it be better to just run MapReduce directly on the Hadoop cluster? Well, running MapReduce directly on tons of Hadoop clusters, that's very useful, but it has the complication that the Hadoop system has to be tuned for the kind of job being run to make efficient use of the underlying hardware, those resources of the cluster. Imagine a job working on millions of pieces of sensor data coming in from an internet of things or IoT application. And then imagine a job working on those huge photos from our previous example. Trying to do both things at the same time time efficiently is really complicated. One important innovation that's helped out is Spark. And a simple explanation of Spark is that it's able to mix different kinds of applications and to adjust how it uses the available resources on your cluster. You have to learn to program Spark differently from traditional programming, because you can't tell it how exactly to do things. To give spark the flexibility it needs to determine how to use the resources that are available you have to describe what you want to do and let spark determine how it actually does it to make it happen. This is called declarative programming versus imperative programming. In imperative programming, you tell the system exactly what to do and how to do it. In declarative programming, you tell the system what you want and it figures out how to actually do the implementation. You'll be using these concepts inside of your Spark labs later on in this course. There's even a full SQL implementation on top of Spark. There's also a common data frame model that works across Scala, Java, Python, SQL, and R. And lastly, there's a distributed machine learning library that you may have heard of called Spark MLLib. So where does a Hadoop cluster store its data? Well, within the Hadoop distributed file system, or HDFS. HDFS is the main file system Hadoop uses for distributing work to the nodes on its cluster. It's part of the cluster, which means even if you're not running jobs that use the compute hardware on the cluster, you still have to pay for that power for the cluster to persist all of that storage. This is the disadvantage of tying together compute and storage, which we'll talk about how to address by running Hadoop in the cloud later on. You as a data engineer. If you're using on-premise Hadoop, you're the one responsible for managing cluster resources via the yarn utility that Hadoop provides. If a job is demanding too many resources or if there are issues with your hardware or software, it's ultimately your responsibility to manage this on premise and keep your cluster going. Those are two common issues with OSS or the open source version of Hadoop, cluster tuning and utilization. A company will typically have several Hadoop clusters that are shared by several organizations and run a wide variety of jobs. Hadoop experts have to then adjust many configuration settings in the collection of underlying open-source project software to optimize the cluster for varying kinds of work it's being asked to perform. That's what we mean when we say the tuning problem. Hadoop clusters also tend to have a lot of dedicated Hardware, which makes them expensive when they're not being used. That's the utilization problem. Hadoop administrators may find that they're searching throughout people in the organization to find their data processing jobs so they can then increase cluster utilization. If they're successful, then the capacity of the cluster will start to be consumed, and it may be time to order more hardware. This cycle of tuning, underutilization, overutilization, and expansion creates a significant overhead for a data engineer running on-premise Hadoop. We'll see how doing Hadoop in the cloud addresses these concerns. Note that there are many other components to the entire Hadoop ecosystem to support Big Data workloads. If you're currently using Hadoop, a lot of these names might look familiar. Let's recap some of the limitations of doing Hadoop on-premise. First, those workloads in the cluster, they're not elastic. This means you're bound to the compute power and storage capacity of your on-premise cluster. If you have a huge reporting and analytics workload to run at the same time another ML team wants to use your cluster, you may run into long processing times for those jobs. On the other hand, if you have no jobs running, your cluster will still, again, need that power and just sit there idle. Why, again, is that? Because the compute and the storage are tied together. If you don't power your cluster 24/7, you'll lose the state of your storage. If you want to run your Hadoop and Spark jobs in the cloud, you can use Cloud Dataproc. It has built-in support for Hadoop, and it's a fully managed service on GCP. That means you don't have to worry about hardware or software updates and installs. That's all done and managed for you. Also, if you need a larger cluster, you don't need to wait and order more machines. You can simply add or remove nodes in your cluster via the Cloud Dataproc UI in just minutes. Dataproc also has simplified version management. Keeping all of your open source tools up-to-date and working together is one of the most complex part when managing a Hadoop cluster. When you use Cloud Dataproc, much of that work is managed for you with Cloud Dataproc's versioning system. Lastly, Dataproc has flexible job configuration. A typical on-premise Hadoop setup uses a single cluster that serves many purposes. When you move to GCP, you can focus on individual tasks, creating as many clusters as you need. This removes much of the complexity of maintaining a single cluster with growing dependencies and differing software configuration interactions. What that last point really means is that you can spin up a particular cluster just for a given workload or job. Then you run that workload and turn down the cluster when you don't need those compute resources anymore. This is especially true when you need to persist data off-cluster instead of inside HDFS on the cluster, a topic we'll discuss in greater detail soon.

## 2. Running Hadoop on Cloud Dataproc

* Next, we'll discuss how and why you should consider processing your same Hadoop job code in the cloud using Cloud Dataproc on GCP. So what are the advantages to running Hadoop and Spark? We're closed on Cloud Dataproc. First, low-cost, Cloud Dataproc is priced at $0.01 per virtual CPU per cluster per hour on top of any other GCP resources you use. In addition Cloud Dataproc clusters can include preemptible instances or VMs that are short-lived if you don't need them. They have lower compute prices, you use in pay for things only when you need them. So Cloud Dataproc charge is second by second billing with of course a one-minute minimum billing period. Next, it's super fast, Cloud Dataproc clusters are quick to start to scale and shut down. With each of these operations taking about 90 seconds or less on average. Next, they're resizable in your clusters. Clusters can be created and scaled quickly with a variety of virtual machine types, disc sizes, numbers of nodes and different networking options as you're going to see later. Open source ecosystem. You can use Spark and Hadoop tools libraries and documentation with Cloud Dataproc. Cloud Dataproc provides frequent updates to native versions of Spark, Hadoop, Pig and Hive, so there's no need to learn new tools or APIs. And it's possible to move existing projects or ETL pipelines without redeveloping any code. It's managed, you can easily interact with clusters in Spark or Hadoop jobs without the assistance of an administrator or special software. Through the GCP console the cloud SDK or the Cloud Dataproc REST API. When you're done with a cluster, simply turn it off. So money isn't spent on idle cluster resources. Cloud Dataproc supports of versioning. Image versioning allows you to switch between different versions of Apache Spark, Apache Hadoop and other tools. It's integrated, it has built-in integration with Cloud Storage, BigQuery and Cloud Big Table to ensure data will never be lost. This together with StackDriver Logging and StackDriver Monitoring provides a complete data platform, and not just a Spark or Hadoop cluster. For example, you can use Cloud Dataproc to effortlessly ETL terabytes of raw log data directly into BigqQery for business reporting. In addition to that, you get a few more benefits. The clusters are highly available, you can run clusters with multiple master nodes and set jobs to restart on failure to ensure your cluster is in jobs or highly available. It has developer tools, you have multiple ways that you can manage your cluster including the GCP console the Cloud SDK, RESTful APIs and Direct SSH access. You have initialization actions, you can run these actions to install or customize the settings in libraries that you need when you're clusters first created. We'll look at those more in detail later. It supports automatic or manual configuration of the cluster. Cloud Dataproc automatically configures the hardware and the software on the clusters for you. But if you want to go in there yourself manually update it, you can. Cloud Dataproc has two ways to customize clusters, optional components and initialization actions. Pre-configured optional components can be selected when deployed via the console or the command line and include Anaconda Jupyter notebook, Zeppelin notebook, Presto and Zookeeper. Initialization actions lets you customize your cluster by specifying executables or scripts that Cloud Dataproc will run on all nodes in your Cloud Dataproc cluster immediately after the cluster is set up. You can define your own initialization scripts or select from a wide range of frequency used ones and other sample initialization scripts that are available. Here's an example of how you can create a Dataproc cluster using the Cloud SDK. And we're going to specify an hbase shell script to run on the clusters initialization. There are a lot of pre-built startup scripts that you can leverage for common Hadoop cluster set of tasks like Flink, Jupyter and more. You can check out the GitHub repo link to learn more. Do you see the additional parameter for the number of master and worker nodes in the script? Let's talk more about the architecture of the cluster. The standard setup architecture is much like you would expect on-premise. You have a cluster of virtual machines for processing and then persistent disks for storage via HDFS. As you can see here, you've got your master node VMS in a set of worker nodes. Worker nodes can also be part of a managed instance group, which is just another way of ensuring that VMS within that group are all of the same template. The advantages here is that you'll soon see in your lab. You can spin up more VMS than you need to automatically resize your cluster based on the demands. It also only takes a few minutes to upgrade or downgrade your cluster. But generally you shouldn't think of a Cloud Dataproc cluster as long-lived. Instead you should spin them up when you need compute processing for a job and then simply turn them down. Of course, you could persist them indefinitely if you wanted to. So what happens to HDFS storage on disk when you turn those clusters down? Well, the storage would go away too, which is why it's a best practice to use storage that's off cluster by connecting to other GCP products. Here we've extended the diagram to show you what that could look like. Instead of using native HDFS on cluster, you could simply use a cluster of bucket on Google Cloud Storage via the HDFS connector. It's pretty easy to adapt existing Hadoop code to use GCS instead of HDFS. It's just a matter of changing the prefix for this storage from hdfs// to gs//. What about hbase off cluster? Well, consider writing in the Cloud Big Table instead. What about large analytical workloads? Well, consider reading that data into BigQuery and doing those analytical work loads there. Using Cloud Dataproc involves this sequence of events, Setup, Configuration, Optimization, Utilization and monitoring. Setup means creating a cluster, and you can do that through the console from the command line using the gcloud command. You can also export a YAML file from an existing cluster or create a cluster from a YAML file. You can create a cluster from the deployment manager template as well, or if you wanted to you can use the REST API. For configuration the cluster can be set up as a single VM, which is usually to keep costs down for development and experimentation. Standard is with a single master node and high availability has three master nodes. You can choose between a region and a zone or select the global region and allow the service to choose the zone for you. The cluster defaults to a global endpoint but defining a regional endpoint may offer increased isolation and in certain cases lower latency. The master node is where the HDFS name node runs as well as the yarn node and job drivers. HDFS replication defaults to to in Cloud Dataproc. Optional components from the Hadoop ecosystem include Anaconda, which is your Python distribution in package manager, Web H CAD, Jupyter Notebook and Zeppelin Notebook as well. Cluster properties are runtime values that can be used by configuration files for more dynamic startup options. And user labels can be used to tag your cluster for your own solutions or your reporting purposes. The master node, worker nodes and preemptible worker nodes if enabled have separate VM options such as vCPU, memory and storage. Preemptible nodes include yarn node manager, but they don't run HDFS. There are a minimum number of worker nodes. The default is two, the maximum number of worker knows is determined by a quota and the number of SS Divs attached to each worker. You can also specify initialization actions such as an initialization script that we saw earlier. It can further customize your worker nodes on startup. And metadata can be defined, so the VM share state information between each other. This may be the first time you saw a preemptible nodes as an option for your cluster. When and why would you use them if they can be preempted by a different job? Well, the main reason to use preemptible VMs or PVMs is to lower costs for fault-tolerant workloads. PVMs can be pulled from service at any time within 24 hours. But if your workload in your cluster architecture is a healthy mix of VMs and PVMs, you may be able to withstand the interruption and get a great discount in the cost of running your job. Custom machine types allow you to specify the balance of memory and CPU to tune the VM to the load, so you're not wasting resources. A custom image can be used to pre-install software. So it takes less time for the customized node become operational, then if you install the software boot time using an initialization script. You can get a persistent SSD boot disk for faster cluster startup. So how do you submit a job to Cloud Dataproc for processing? Jobs can be submitted through the console the gcloud command or via the REST API. They can also be sorted by orchestration services such as Cloud Dataproc workflow and Cloud Composer. Don't use Hadoop's direct interfaces to submit jobs because the metadata will not be available to Cloud Dataproc for job and cluster management. And for security, they're disabled by default. By default jobs are not restartable. However, you can create restartable jobs through the command line or REST API. Restartable jobs must be designed to be item potent and to detect successorship and restore state. Lastly, after you submit your job you'll want to monitor it, you can do so using StackDriver. Or you can also build a custom dashboard with graphs and set up monitoring of alert policies to send emails for example, where you can notify if incidents happen. Any details from HDFS, YARN, metrics about a particular job or overall metrics for the cluster like CPU utilization, disk and network usage can all be monitored and alerted on with StackDriver.

## 3. GCS instead of HDFS

* Let's discuss more about using Google Cloud Storage, instead of the native Hadoop file system, or HDFS. Hadoop was built on Google's original MapReduce design from the 2004 white paper, which was written in a world where data was local to the compute machine. Back in 2004, network speeds were originally pretty slow, and that's why data was kept as close as possible to the processor. Now, with petabit networking speeds, you can treat storage and compute independently, and still move traffic quickly over the network. Although your on-premise Hadoop clusters need local storage on its disk, since the same server runs, computes, and storage jobs in the Cloud, that's one of the first areas for optimization. Why is that? Well, naturally, you can run HDFS in the Cloud just by lifting and shifting your Hadoop workloads to Cloud Dataproc. This is often the first step to the Cloud, and requires no code changes, it just works, but HDFS on the Cloud is a sub-par solution in the long run. This is because of how HDFS works on the clusters, with block size, the data locality, and the replication of the data in HDFS. For block size in HDFS, you're tying the performance of input and output to the actual hardware the server is running on. Again, storage is not elastic in this scenario, you're on the cluster. If you run out of persistent disk space on your cluster, you'll need a re-size, even if you don't need the extra compute power. For data locality, there are similar concerns about storing data on individual persistent disks. This is especially true when it comes to replication. In order for HDFS to be highly available, it replicates three copies of each block out to storage. Wouldn't it be better to have a storage solution that's separately managed from the constraints of your cluster, than where the computing is done? What would it take to achieve such a solution? Well first, you need a super-fast way to connect, compute, and storage, if they're not in the same place. Google's network enables new solutions for big data. The Jupiter networking fabric within a Google data center delivers over one petabit per second of bandwidth. To put that in perspective, that's about twice the amount of traffic exchanged on the entire public internet. If you draw a line somewhere in a network, bisectional bandwidth is the rate of communication at which each server on one side of the line can communicate with servers on the other side. With enough bisectional bandwidth, any server can communicate with any other server at full network speeds, and with petabit bisectional bandwidth, the communication is so fast that it no longer makes sense to transfer files and store them locally on a single server. Instead, it makes sense to use the data from where it's stored separately. Here's how that equates to Google Cloud Platform's products. Inside of a Google datacenter, the internal name for the massively distributed storage layer is called Colossus, and the network inside the datacenter is Jupiter. Cloud Dataproc clusters get the advantage of scaling up and down VMs that they need to do the compute, while passing off persistent storage needs with the ultra-fast Jupiter network to a storage product like GCS, which is ran by Colossus behind the scenes. So you've seen this graph before, and now here's where running Hadoop in the Cloud, and the next generation for Hadoop workloads really gets you those benefits. With Cloud native tools like Cloud Dataproc, they encourage companies to leave the storage and the processing power to Google's massive datacenters, and that gives you the advantage of scale and cost efficiency. As I mentioned before, to take advantage of Cloud Storage instead of HDFS, you can simply change your Hadoop job code from HDFS// to GS// as you see here. Additionally, consider using BigQuery for your Data Processing and analytical workloads, instead of performing them on cluster. Remember, one of the biggest benefits of Hadoop in the cloud is that separation of compute and storage. With Cloud Storage as the back end, you can treat clusters themselves as ephemeral resources, which allows you not to pay for compute capacity when you're not running any jobs. Also, Cloud Storage is its own completely scalable and durable storage service, which is connected to many other GCP projects. You can even persist HDFS Data in GCS, inquiry it directly from BigQuery via federated query. So let's recap. Cloud storage could be a drop-in replacement for your HDFS, back end for Hadoop. The rest of your code would just work. Also, you can use the Cloud Storage connector manually on your non-cloud Hadoop clusters if you didn't want to migrate your entire cluster the Cloud yet, maybe just Storage. Cloud Storage is optimized for large, bulk parallel operations, it has very high throughput, but it has significant latency. This is definitely something to consider. This is where if you have large jobs that are running lots of tiny little blocks, you may be better off with HDFS. Additionally, you want to avoid iterating over sequentially many nested directories in a single job. Another thing to keep in mind is that Cloud Storage is at its core an object store, it only simulates a file directory. So directory renames in HDFS are not the same as they are in Cloud Storage, but new objects store oriented output committers mitigate this, as you see here. Lastly, to get your data to the cloud, you can use DistCp. In general, you want to use a push-based model for any data that you know you'll need, while a pull-based may be a useful model if there's a lot of data that you might not ever need to migrate.

## 4. Optimizing Dataproc

* Next, let's look at optimizing Dataproc. Where is your data, and where is your cluster? Knowing your data locality can have a major impact on your performance. You want to be sure that your data's region and your cluster zone are physically close in distance. When using Cloud Dataproc, you can omit the zone and have a Cloud Dataproc Auto zone feature slot a zone for you in the region you choose. While this handy feature can optimize on where to put your cluster, it does not know how to anticipate the location of the data your cluster will be accessing. Makes sure that the Cloud Storage bucket is in the same regional location as your Dataproc region. Is your network traffic being funneled? Be sure that you don't have any network rules are routes that funnel Cloud Storage traffic through a small number of VPN gateways before it ultimately reaches your cluster. There are large network pipes between Cloud Storage and Compute Engine. So you don't want to throttle with bandwidth by sending traffic into a bottleneck via your GCP networking configuration. How many input files and Hadoop partitions are you trying to deal with? Make sure you're not dealing with more than around say 10,000 input files. If you do find yourself in this situation, try to combine or union the data into larger file sizes. If this file volume reduction, means that now you're working with larger data sets, like more than 50,000 Hadoop partitions, you should consider adjusting the settings for your block size to a larger value accordingly. Now, that configuration parameter is fs.gs.block size, and then helps the job performance splits. When used in conjunction with Cloud storage data, it becomes a fake value since Cloud Storage does not expose the actual block sizes. The default value of 64 megabytes is solely for helping Hadoop decide how to perform these splits. Therefore, if your files are larger than 512 megabytes, you may find that you can achieve better performance by manually increasing this value up to one gigabyte or even two gigabytes. Unlike standard persistent disks, the IOPS performance of SSD persistent disks depends on the number of these CPUs in the instance. Is the size of your persistent disk limiting your throughput? Oftentimes, when getting started with Google Cloud, you may have just a small table that you want to benchmark. It's generally a good approach as long as you don't choose a persistent disk that its size to such a small quantity of data. It will likely limit your performance. Standard persistent disks scale linearly with a volume size. Did you allocate enough virtual machines or VMs to your cluster? It's a question that often comes up when migrating from on-premises hardware to Google Cloud. How accurately did you size the number of VMs that you needed? Understanding your workload is key to identifying a size for your cluster. Running prototypes and bench marking with real data and real jobs is absolutely crucial to informing the actual VM allocation decision. Luckily, the ephemeral nature of the Cloud makes it really easy to write size your clusters for a specific task at hand instead of trying to foresee and purchase the hardware upfront, thus you can easily resize your cluster as you need to. Employing jobs scoped clusters is a very common strategy for Dataproc. Even though we know clusters can easily scale up or down, it's still can be useful to have some back of the napkin calculations as we approach our cluster sizes. For an example, calculation, take the one here. We're going to be migrating 50 physical nodes, each with 12 physical cores having two hyper threads per core. It's important to understand that on Compute Engine, each of virtual CPU is implemented as a single hardware hyper thread on one of many available CPU platforms. In our example of 50 nodes, each with 12 physical cores, you'd have two options that you might configure on Compute Engine. Option 1, 1,200 4-vCPU VMs, or option 2, 600 8-vCPU VMs. When considering which option to choose, it's also important to factor in the storage implications associated with your choice. There are limits to the total amount of persistent disk that you can add to each VM. Most instance types have a 64 terabyte limit, which means that option 2 would limit the data on your cluster to 225 terabytes, and again, good or bad depending upon your job. In our example, we should consider this is enough or if you would prefer to have more VMs and thus increase our storage size. Since Cosmos typically move the vast majority of their long-term data from HDFS into Cloud storage when they actually migrate to the Cloud, usually the persistent drive limits are more than sufficient on cluster.

## 5. Optimizing Dataproc Storage

* Maybe you're going to ask the question, when is local HDFS a good option instead of using Google Cloud Storage? Local HDFS is a good option if your job requires a lot of metadata operations. For example, you have thousands of partitions and directories, and each file size is relatively small. Or you modify the HDFS data frequently or rename directories often. Cloud storage objects are immutable, so renaming a directory is an expensive operation. Because it consists of copying all objects to a new key and then deleting them afterwards. Or you heavily use the append operations on HDFS files? Or you have workloads that involve very heavy input output. For example, you have a lot of partitioned rights. Or you have IO workloads that are especially sensitive to latency. For example, you require a single digit millisecond latency per storage operation. But in general we recommend using cloud storage as the initial and final source of data in your big data pipeline. For example, if a workflow contains five spark jobs in a series, the first job retrieves the initial data from cloud storage then writes shuffled data, and the immediate job output is the HDFS. The final spark job will then write its results to cloud storage. Using cloud data proc with cloud storage allows you to reduce the disk requirements and save costs by putting your data there instead of persisting it on HDFS. When you keep your data on cloud storage and don't store it locally on your cluster and HDFS, you can use smaller discs for your cluster. And by making your cluster truly on-demand, you're able to as we talked about before, separate the compute in the storage which helps you reduce costs significantly. Even if you store all of your data in cloud storage, your cloud data proc cluster does need HDFS for certain operations such as storing control and recovery files or aggregating logs. It also needs non HDFS local disk space for shuffling. You can reduce the disk size per worker if you're not heavily using local HDFS. Here are some options to adjust the size of your local HDFS for you to consider. You can decrease the total size of your local HDFS by decreasing the size of the primary persistent disks for the master and worker nodes. The primary persistent disk also contains the boot volume and system libraries. So be sure to allocate at least a hundred gigabytes. You can increase the total size of a local HDFS by increasing the size of the primary persistent disks for workers. Consider this option very carefully. It's rare to have workloads that get better performance by using HDFS with standard persistent disks in comparison with doing it on cloud storage, or local HDFS with an SSD. You can attach up to eight SSDs or 375 gigabytes each to each worker, and you can use these discs for HDFS. This is a very good option if you need the HDFS for very high input/output intensive workloads, and you need to get to that single digit millisecond latency. Make sure that you use a machine type that is enough CPU and memory on the worker to support this many discs. Use SSD persistent disks or PD_SSDs, currently in beta for your master or your workers as a primary disk. You should understand the repercussions of geography and regions before you configure your data and jobs. Many GCP services require you to both specify regions or zones to allocate those resources. The latency of requests can increase when the requests are made from a different region than the one where the resources are actually being stored. Additionally, if the services resources and your persistent data are located in different regions. Some calls to GCP services might copy all the required data from one zone to another before processing. This can have a severe impact on your performance. Cloud storage is the primary way to store unstructured data in GCP, but it isn't the only storage option. Some of your data might be better suited to different storage products designed explicitly for big data. You can use cloud big table to store a large amount of sparse data. Cloud big table is an HBase compliant API that offers low latency and high scalability to adapt to your jobs. For data warehousing and analytical work loads, you can consider using BigQuery. Because cloud data proc runs Hadoop on GCP, using a persistent cloud data proc cluster to replicate your on-premise setup might seem like the easiest solution. However, there are some limitations to this lift and shift approach. Keeping your data in persistent HDFS clusters using cloud data proc is more expensive than storing your data inside of cloud storage, which is generally what we recommend. Keeping your data in HDFS cluster also limits your ability to use the data with other GCP products, it's isolated on your cluster. Augmenting or replacing some of your open source based tools with other related GCP services can be more efficient or economical for particular use cases. Using a single persistent cloud data proc cluster for your jobs is more difficult to manage than shifting to targeted clusters that serve an individual's job or job area. The most cost effective and flexible way to migrate your Hadoop system to GCP is to shift away from thinking in terms of large multi-purpose persistent clusters. And instead, think of small short-lived clusters that are designed exclusively to run specific jobs. You store your data and cloud storage to persist multiple temporary processing clusters data. This model is often called the ephemeral model because the cluster is used for processing those jobs are allocated as needed, and then released, and turned down as the job's finished. You've got efficient utilization, don't pay for resources that you don't use. A fixed amount of time after the cluster enters an idle state, you can automatically set a timer. You can give it a time stamp, and the count starts immediately once the expiration has been set. You can set a duration, the time in seconds to wait before automatically turning down the cluster. You can range from ten minutes as a minimum, to 14 days as a maximum at a granularity of one second. It's currently available from the command line and rest API but not through the console. The biggest shift in your approach between running an on-premise Hadoop workflow and running the same workflow on GCP is that shift away from your monolithic persistent clusters to the ephemeral clusters,. You just spin up the cluster when you need to run the job and then delete that cluster when the job completes. The resources required by our jobs are active only when they're being used, so you only pay for what you use. This approach enables you to tailor your clusters configurations for individual jobs. Because you are maintaining and configuring a persistent cluster, you reduce the cost of resource use and cluster administration. This section describes how to move your existing Hadoop infrastructure to an ephemeral model. To get the most from cloud data proc, you have to move that ephemeral model if only using them in your clusters when you need them. This can be scary because a persistent cluster is comfortable. With GCS data persistence and the fast boot of cloud data proc, a persistent cluster is a waste of resources. But if you do need a persistent cluster, make it small, and your clusters can be resized anytime. The ephemeral model is the recommended route, but it requires that storage to be decoupled by compute. You'll want to separate jobs shapes and separate your clusters. You can decompose even further with job scoped clusters. How does this work? You want to isolate your Dev, your staging and your production environments. Run them all in separate clusters, read data from the same underlying data source in GCS, no problem. But run it on separate clusters that are ephemeral again. Of course, you can add appropriate ACLs to your service accounts to protect your data. The point of your ephemeral clusters, use them only for the jobs lifetime. When it's time to run a job, follow this process. Create a properly configured cluster, run your job, setting the output to cloud storage or another persistent location. Delete your cluster, use the job output however you need to. And lastly, view those logs and stack driver, or cloud storage. If you can't accomplish your work without a persistent cluster, sure, you can create one. This option may be costly, and it's not recommended if there's a way to get your job done on an ephemeral cluster. You can minimize the cost if a persistent or long-lived cluster by creating the smallest cluster you can. Scoping your work on that cluster for the smallest amount of jobs. And scaling the cluster to the minimum amount of worker nodes, adding more dynamically to meet the demand.

## 6. Optimizing Dataproc Templates and Autoscaling

* Let's talk about templates. The cloud dataproc workflow template is a YAML file that's processed through a directed acyclic graph or DAG. It can create a new cluster, select from existing cluster, submit jobs, hold jobs for submission until dependencies can complete, and it can also delete a cluster when a job is done. It's currently available through the gcloud command and the rest API but not through the console. The workflow template becomes active when it's instantiated into the DAG. The template can be submitted multiple times with different parameter values. You can also write a template in line in the gcloud command, and you can list workflows and workflow metadata to help diagnose issues. Here's an example of a cloud data proc workflow template. We first get together all the things that need to be installed in the cluster like using our startup scripts and manually echoing pip install commands like this one to install matplotlib. You can have multiple startup shell scripts run like you see in our example here. Next we use the gcloud command for creating a new cluster in advance of running our job. We specify cluster parameters like the template to be used in our desired architecture and what machine types and image versions we want for hardware and software. After that, we need to add a job to the newly created cluster. Here we have a spark job written in Python that exists in a GCS bucket that we control. Let's add this job to the workflow template. Lastly we need to submit this template itself as a new workflow template as you see with the command here. After you submit your jobs to the cluster, one of the features you can take advantage of with data proc is autoscaling the number of nodes in your cluster to meet the demands of your job. With autoscaling there's no need to manually intervene when a cluster is over or under capacity. You can choose between this standard and preemptible workers, and save on resources that your quota in cost at any point in time. Autoscaling policies provide fine grained control. This is based on a difference between yarn pending and available memory. If more memory is needed, then you scale up. And if there's excess memory you can scale down. Note that autoscaling does not support Spark structured streaming, which is a streaming service built on top of Spark SQL additionally. It's not designed to scale to zero. So it's not the best for sparsely utilized or idle clusters. If your are heavily relying on data product autoscaling for your jobs, it may be time to consider migrating your workload to cloud dataflow, which autoscales more efficiently and by default. A topic which we'll cover more in greater detail soon. One of the things that you want to consider when working with autoscaling is setting the initial workers. It's set from worker nodes, and the cluster must meet that minimum. Setting this value ensures that the cluster comes up to basic capacity faster than if you just let autoscaling handle it. That's because autoscaling might require multiple autoscale periods to scale up to that number. The primary minimum number of workers may be the same as a cluster nodes minimum. There is a maximum that caps the number of worker nodes. Now there's a heavy load on the cluster and auto-scaling determines it's time to scale up. The scale up factor determining how many nodes to launch, this would commonly be one node. But if you knew that a lot of demand would occur at once, maybe you wanted to scale up faster. After this action, there's a cool down period to let things settle before autoscaling evaluates and occurs again. The cooldown period reduces the chances that the cluster will start and terminate nodes at the same time. In this example, the extra capacity isn't needed, and there's a graceful decommission time out to give running jobs a chance to complete before the node goes out of service. Notice there is a scale down factor. In this case it's scaling down by one node at a time for a more leisurely reduction of capacity. After this action, there's another cool down period. After the second scale down it results in the number of minimum workers that we expected. A secondary minimum number of workers and maximum number of workers controls the scale of preemptible workers.

## 7. Optimizing Dataproc Monitoring

* In GCP, you can use Stackdriver logging and Stackdriver monitoring to view and customized logs and to monitor jobs and resource utilization. The best way to find out what error caused your Spark job to fail is to look at the driver output and the logs generated by Spark. Note, if you submit this Spark job by connecting directly to the master node using SSH, it's not possible to get the driver output. You can retrieve the driver program output by calling the Google Cloud Platform console or by using the G-Cloud command. The output is also stored in the Cloud storage bucket of the Cloud Dataproc cluster itself. Another note, when you view the driver program output in the GCP console, the progress bar is stored as a long line without a new line at the end. Therefore, the first thing that the driver program prints will appear at the very end of this line. To view this output more easily and to save yourself a lot of headaches, click "Line Wrapping" to get that to be a more visible. All other logs are located in the different files inside the machines of the cluster. It's possible to see the logs for each container from the Spark app Web UI or from the history server after the program ends in the executer's tab. You need to browse through each Spark Container to view each log. If you write logs or print to standard out or standard error and your application code, those logs are saved in the redirection of standard out or standard error. In a Cloud Dataproc cluster, YARN is configured to collect all of these logs by default. They're available in Stackdriver logging. Logging provides a consolidated and concise view of all logs so you don't need to spend time browsing among container logs to find those errors. This screen shows the login page in the GCP console. You can view all logs from your Cloud Dataproc cluster by selecting the cluster's name in the selector menu. Again, don't forget to expand the time duration in the time range selector. You can get logs from a Spark application by filtering for its ID. You can get the application ID from the driver's output. To find logs faster, you can create and use your own labels for each cluster or for each Cloud Dataproc job. For example, you can create a label with the key environment or ENV as the value in the exploration and then use it as part of your data exploration job. You can then get logs for all exploration jobs by creating and filtering with the label environment with a value exploration in logging. Note that this filter will not return all logs for this job. Only the resource creation logs. You can set the driver log level using the following G-Cloud command, gcloud dataproc jobs submit hadoop with the parameter driver-log-levels. You set the log level for the rest of the application from the Spark context. For example, spark.sparkContext.setLogLevel. For here we'll just say the example is debug. Stackdriver monitoring can monitor the cluster CPU, disk, network usage and YARN resources. You can create a custom dashboard to get up-to-date charts for these in other metrics. Cloud Dataproc runs on top of compute engine. So if you want to visualize CPU usage, disk I/O, or networking metrics in a chart, you can select a Compute Engine VM instance as the resource type, and then filter by the cluster name in Stackdriver. This diagram shows an example of that output. To view metrics for Spark queries or jobs or stages or tasks, connect to the Spark's applications Web UI.

## 8. Summary

* Let's do a brief recap. You saw how you can run your entire Hadoop ecosystem on the Cloud, with Cloud Dataproc. We covered the advantages of separating compute and storage for cost efficiency and performance by using GCS instead of HDFS. Lastly, we discussed how you can optimize Cloud Dataproc by resizing your cluster as your needs change and enabled smart features like automatically turning down the cluster after a certain period of non-use.

## QuizNotes

* Which of the following statements are true about Cloud Dataproc?
	* Lets you run Spark and Hadoop clusters with minimal administration
	* Helps you create job-specific clusters without HDFS
* Cloud Dataproc provides the ability for Spark programs to separate compute & storage by:
	* Reading and writing data directory from/to Cloud Storage

## Resources