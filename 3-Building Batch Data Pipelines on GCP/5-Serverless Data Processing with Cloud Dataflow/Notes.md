## 1. Cloud Dataflow

* Earlier in the course, you saw how to do batch data processing with Cloud Dataproc and other methods. Now it's time to introduce you to a key serverless tool that should be in your data engineering toolkit, Cloud Dataflow. This entire module will cover batch Dataflow pipelines and why Dataflow is a commonly used Data Pipeline tool on GCP. Not to give away too much of the answer, but you can write the same code to do both batch and streaming pipelines with Dataflow. We'll cover streaming pipelines later. So the topics we will address are; how to decide between Cloud Dataflow and Cloud Dataproc, why customers value Dataflow, and pipelines, templates, and how you can now run SQL on Dataflow too. Let's get started. Cloud Dataflow is the serverless execution service for data-processing pipelines written using Apache Beam. Apache Beam is open-source. You author your pipeline and then give it to a runner. Beam supports multiple runners like Flink and Spark if you wanted to run your Beam pipeline on-prem or in another Cloud. You don't even have to run it on Cloud Dataflow. This means your pipeline code is portable. So why run Beam on Cloud Dataflow? It is the most effective execution environment upon which to run Apache Beam. The Dataflow team actively contribute to that open source library for Beam and build features around the latest Beam offerings. So why not just sake with running Hadoop or Spark jobs on Dataproc? Well, with Dataflow, you don't have to manage clusters at all. Unlike with Cloud Dataproc, the auto-scaling in Dataflow scale step-by-step, it's very fine grained. Plus, as we'll see in the next course, Dataflow allows you to use the same code for both batch and stream. That's where a Beam gets to scene by the way. Batch and stream together make Beam. So how do you decide between the two? When building a new data processing pipeline, we recommend that you use Dataflow. If on the other hand you have existing pipelines written using Hadoop technologies, it may not be worthwhile to rewrite everything. Migrate it over to Google Cloud using Dataproc and then modernize it as necessary. As a data engineer, we recommend that you learn both Dataflow and Dataproc. This way you can make the choice based on what's best for your specific use case. What are some scenarios in which you might choose Dataproc over Dataflow? If the project has existing Hadoop or Spark dependencies, then it might make sense to use Dataproc, and sometimes the production team is a lot more comfortable with a DevOps approach. They want to provision machines themselves rather than going with the serverless approach where Google does it for you. In this case, Dataproc might be the right choice as it provides greater control. Now, if you don't care about Streaming and your primary goal is to move existing workloads, then Dataproc could be fine too. However, Dataflow is our recommended approach for building pipelines and the rest of this module will explain why. So a recap what we've covered so far. Cloud Dataflow provides a serverless way to execute pipelines on batch and streaming data. Serverless means there are no servers for you to manage, the service just works. For example, if you have a ton of data to process, Dataflow will intelligently call upon more virtual machines to help. Since Dataflow also support streaming, this makes your pipeline low-latency, meaning you can process data as soon as it comes in. Let's dive into how this whole streaming versus batch processing got started. This ability to process batch and stream with the same code is rather unique to Apache Beam. For a long time, batch programming and data processing used to be two very separate and different things. Batch programming dates to the 1940's, and the early days of computing where it was realized that you can think of two separate concepts, code and data. Use code to process data. Of course both of these were on punch cards. So that's what we were processing. A box of punch cards called a batch. It was a job that started and ended when the data was fully processed. Stream processing on the other hand is more fluid. It arose in the 1970's with the idea that data processing was something that is ongoing, like a stream of water in a pipe. The idea is that data keeps coming in and you process the data. The processing itself tended to be done in micro batches. So what about today? How do these two distinct processes get combined? This is the genius of Apache Beam. It provides abstractions that unify traditional batch programming concepts and traditional data processing concepts. Unifying programming and processing is a big innovation in Data Engineering. The four main concepts are Ptransforms, Pcollections, Pipelines, and Pipeline Runners. Let's drill into each of these concepts in more detail. A pipeline identifies the data to be processed and the actions to be taken on the data. The data is held in a distributed data abstraction called a Pcollection. The Pcollection is immutable. Any change that happens in a pipeline and just one Pcollection and creates a new one. It does not change the incoming Pcollection. The actions or code is contained in an abstraction called a Ptransform. The Ptransform handles input, transformations, and output of the data. The data in a Pcollection is passed along the graph from one Ptransform to another. Pipeline runners are analogous to container home such as Kubernetes Engine. The identical pipeline can be run on a local computer, datacenter VM or on a service such as Cloud Dataflow in the Cloud. The only difference is scale and access to the platform specific services. The services that Runner uses to execute the code is called a backend system. Immutable data is one of the key differences between batch programming and data processing. The assumption and the Von Neumann architecture was that data would be operated on and change in place, which was very memory efficient. That made sense when memory was very expensive and scarce. Immutable data, where each transform results in a new copy, it means that there is no need to coordinate access control or sharing of the original and just the data. So it enables or at least simplifies distributed processing. The shape of a pipeline is not actually just a single linear progression, but rather a directed graph with branches and aggregations. For historical reasons, we refer to it as a pipeline but a data graph or Dataflow might be a more accurate description. Now what happens in each of these green boxes? They represent transforms on Pcollections. A Pcollection represents both streaming data and batch data. There's no size limits your Pcollection either bounded or unbounded. That's why it's called a Pcollection or parallel collection. The more data, the more it's simply distributed in parallel across more workers. For streaming data, the Pcollection is simply without bounds. It has no end. Each element inside a Pcollection can be individually accessed and processed. This how distributed processing of the Pcollection is implemented. So you define the pipeline and that transforms on the Pcollection and the Runner handles implementing the transforms on each element, distributing the work as needed for scale and with all available resources. Elements represent different datatypes. Once an element is created in a Pcollection, it is immutable, so it can never be changed or deleted. In traditional programs, a datatype is stored in memory with a format that favors processing. Integers in memory are different from characters which are different from strings and compound data types. In a Pcollection, all data types are stored in a serialized state as byte strings. This way there is no need to serialize data prior to network transfer and de-serialize it when it's received. Instead, the data moves to the system in a serialized state and is only deserialized when necessary for the actions of a Ptransform.

## 2. Why customers value Dataflow

* Now that we've introduced you to the topic, let's see why data engineer's value Dataflow over other alternatives for data processing. It helps to understand a bit about how Cloud Dataflow works, and why it provides an efficient execution mechanism for Apache Beam. The Beam pipelines specifies what has to be done. The Dataflow Services choose how to run the pipeline. The pipeline typically consists of reading data from one or more sources, applying processing the data, and writing it to one or more sinks. In order to execute the pipeline, the Dataflow service first optimizes the graph in various ways. For example, fusing transforms together. It then breaks the jobs into units of work, and schedule them to various workers. One of the cool things about Dataflow is that the optimization is always ongoing, and that units of worker continually rebalance dynamically. What that means is, if a certain VM is taking too long for a processing task, Dataflow service can rebalance mid job. It looks like this, say you had 21 workers processing, and all but three finish their jobs. You don't have to wait for these busy workers. Cloud Dataflow will automatically balance out the Workload to idle workers mid-job. This way, the overall job finishes faster and Dataflow is using the collections of VMs that has more efficiently. You can see the items on the left, where the gray lines extend to the top, there are processes still running. So if you're waiting to proceed to the next step, you're paying for these items to continue to run. What you see on the right, is dynamic rebalancing. Now, this is only taking 65 minutes to do the same amount of work as opposed to 100 minutes in the example on the left. This will in turn save your organization a lot of money and resources. What about if data arrives late into you're streaming pipeline? Cloud Dataflow allows you the flexibility to handle lagged and late data with intelligent watermarking. We'll talk more about this in our course on Streaming Data Processing. Lastly, if some of the VMs fail during the course of the job, the service will auto heal and distribute the load to productive workers without your intervention. You can monitor the service in the UI, and by looking at the logs that Cloud Dataflow will output during and after processing as well. To summarize, the advantages of Dataflow are first, Dataflow is fully managed and auto configured. You just deploy your pipeline. Second, Dataflow doesn't just execute Apache Beam transforms as is, it optimizes the graph, fusing operations efficiently as we see with C and D here combining into a single step. Also, it doesn't wait for a previous step to finish before starting a new SAP, as long as there is no dependency. We see with this with A, and the groupByKey operation happening at the bottom. Third, autoscaling happens step by step in the middle of a job. As a job needs more resources, it receives more resources automatically. You don't have to manually scale resources to match job needs, and you don't pay for VM resources that aren't being used. Dataflow will turn down the workers, as the job demand decrease. All this happens while maintaining strong streaming semantics. Aggregations, like sums and counts, are correct even if the input source sends duplicate records. As we mentioned, Dataflow is also able to handle late arriving records with intelligent watermarking. Finally, Dataflow functions as the glue that ties together many of the services on GCP, it's interoperable. Do you need to read data from BigQuery and write to Bigtable? Use Dataflow. Do you need to read from pub/sub and write to Cloud SQL? Yes, use Dataflow.

## 3. Building Cloud Dataflow Pipelines in code

* Let's look in greater detail at an example Dataflow pipeline encode. What we want to do, is take an input PCollection here on the left, and pass it through 3P transforms and get the output P collection on the right. We'll look at how to do this in both Java and Python. For Python code, as you see here, you have desired output, that's PCollection_outs that is equal to the input with 3P transforms apply. You can tell they're applied because Python uses the pipe symbol, the vertical line, as applying the transformation. Keep in mind, order matters. PTransform_1 comes before PTransform_2 and so on. Note that there is an output after each transform step, we just don't give it a name until the end of the third step. Again, remember that the PCollection_out at the end, is a fundamentally different collection than what came in. We're actually not processing the input PCollection in place, but rather giving a brand's new transformed output PCollection that didn't exist before. In Java, as you see here, it is the same thing except instead of the pipe symbol, we use the apply method multiple times, you can see as chain it's together here. If you want to do branching, meaning to do two things on one input for example, just send the same PCollection through two different transforms. As you see here, we simply gave the output PCollection variable in each case a different name. For example, we take the PCollection_in and pass the collection through both PTransform_1 and PTransform_2 in parallel. The result in the first case, it's stored as PCollection_out_1 and the second case, it's stored as PCollection_out_2. What we showed you so far was the middle part of a pipeline. You already had a PCollection, and you applied a bunch of transforms and you end up with a new PCollection. But where does the pipeline start? How do you get the first PCollection? You get it from a source. What does a pipeline due with the final PCollection, typically? It writes out to a sync. That's what we're showing you here, and this is in Python. We create a PCollection by taking the pipeline object P, and passing it over a text file in Google Cloud storage, that's the read from text line. Then, we apply the PTransform called FlatMap to the lines read from the text file. What FlatMap does, is applies a function to each row of the input and concatenate all of the outputs. When the function is applied to a row, it may return zero or more elements that go to the output PCollection. The function in this case, is a function called CountWords. It takes the line of texts and returns an integer. The output PCollection then consists of a set of integers. These integers are written to a text file and Google Cloud storage. Because the pipeline was created in a width clause, and because this is not a streaming pipeline, exiting the width clause automatically stops the pipeline. Once you've written the pipeline, it's time to run it. Executing the Python program on the previous slide, will run that program. By default, the program is run using the default runner, which runs on the same machine that the Python program was executed on. When you create the pipeline, you can pass in a set of options as you see here. One of these options is the runner. Here we've chosen Dataflow, which will have the pipeline run on Google Cloud. Of course, normally, you'll setup command line parameters to transparently switch between local and Cloud. Calling the file locally, will run at local and adding Cloud parameters will submit the job to the Cloud. You'll practice this in your next lab.

## 4. Key considerations with designing pipelines

* To design pipelines you need to know how each step works on the individual data elements contained inside of a P collection. Let's start with the input and outputs of the pipeline. First, we set up our Beam pipeline with Beam.pipeline and pass through any options. Here we'll call the pipeline P. Now it's time to get some data as input. If we wanted to read a series of CSV files in Cloud Storage, we could use beam.io.ReadFromText and simply pass in the GCS bucket and filename. Note the use of an asterisk wildcard can handle multiple files. If we wanted to read instead from a Cloud Pub/Sub topic you would still use beam.io but instead it's ReadStringsFromPubSub and you'd have to pass in the topic name. What about if you wanted to read in data that's already in BigQuery? Here's how that would look. You'd prepare your SQL query and specify BigQuery as your input source and then pass in the query and source as a read function to Dataflow. That's just a few of the data sources Cloud Dataflow can read from. But now what about writing to sinks? Take the BigQuery example but as a data sink this time. With Dataflow you can write to a BigQuery table as you see here. First, you establish the reference to the BigQuery table with what BigQuery expects, your project ID, data set ID and table name. Then you use beam.io.WriteToBigQuery as a sink to your pipeline. Note that we are using the normal BigQuery options here for rate disposition. Here we're truncating the table if it exists, meaning to drop data rows. If the table doesn't exist we can create it if needed. Naturally, this is a batch pipeline if we're truncating the table with each load. You can also create a P collection in memory without reading from a particular source. Why might you do this? If you have a small data set like a lookup table or a hard-coded list, you could create the P collection yourself as you see here. Then we can call a pipeline step on this new P collection just as if we sourced it from somewhere else.

## 5. Transforming data with PTransforms

* Now that we have looked at how to get the data in, let's look at how we transform each data element in the PCollection with PTransforms. The first step of any map produced process is the map phase where you're doing something in parallel. In the word length example, there is one word length output for each word input. So the word dog would map to three for length. In the bottom graph example the function my_grep returns each instance of the term it's searching for in the line. There may be multiple instances of the term in a single line a one-to-many relationship. In this case, you may want my-grep to return the next instance each time it's called, which is why the function has been implemented with a generator using yields. The yield commands has the effect of preserving the seed of the function so that the next time it's called, it can continue from where it left off. FlatMap has the effect iterating over one-to-many relationship. The map example returns a key value pair. In Python, this is simply a two-topple for each word. The FlatMap example yields the line only for lines that contain the search term. ParDo or parallel do is a common intermediate step in a pipeline. You might use it to extract certain fields from a set of raw input records or convert raw input into a different format. You might also use ParDo to convert process data into an output format like table rows for big query or strings for printing. You can use ParDo to filter a dataset. So consider each elements in a PCollection, and either output that elements to a new collection or discard it or. Or you can use it for formatting or type converting each elements in a dataset. If your input PCollection contains elements that are of a different type or format that you want. You can use pardho to perform a conversion of each elements and output the results to a new PCollection. You can use it for extracting parts of each element in a dataset as well. If you have a PCollection of records with multiple fields, for example, you can use a ParDo to parse out just the fields you want to consider into a new PCollection. And you can use ParDo for performing computations on each elements in a dataset. You can use ParDo to perform simple or complex computations on every elements or certain elements of a PCollection and output the results as a new PCollection. When you apply a ParDo transform, you need to provide user code in the form of a DoFn object. Where do function is a beam SDK class that defines a distributed processing function. Your do function code must be fully serializable, item potent and thread safe. In this example, we're just counting the number of words in a line, and returning the length of the line. Transformations are always going to work on one element at a time here. Here we have an example from Python which can return multiple variables. In this example, we have below and above some cut off in our data elements. And return two different types below and above two different variables by referencing these properties of the results.

## 6. Aggregating with GroupByKey and Combine

* What do you do after the map phase. The unnamed phase is the shuffle phase where you group together like keys. This works on a PCollection of key-value pairs or two elements tuples. Groups by common key and returns a single key-value pair where the value is actually a group of values. The idea here is that we want to find all the zip codes associated with the city. For example, New York is a city and it may have 10001 and 10002 zip codes. You could first create a key-value pair and a ParDo and then group by the key. The resulting key-value pairs are simply two tuples. We do have to be aware of data skew on we're doing this. When the same example is scaled up in the presence of skewed data, the situation becomes much worse. Let's say that you're doing your GroupByKey, but your group has 1 million items in it. A million is not too big of a deal on modern hardware, but a billion you're forcing all of those elements to go to a single work group to be counted. This could definitely run into some issues on the network. This is the same performance concern when doing high cardinality group by queries on billions of records in bigquery. In this example, there are a million X values and only a thousand Y values. GroupByKey will group all of the X values on one worker. The worker will take much longer to do its processing on the million values than the other worker, which only has a thousand values to process. Of course, you're paying for the worker that sits idle waiting for the other worker to complete. Data flow is designed to avoid an efficiencies by keeping the data balance. You can help by designing your application to divide work into aggregation steps and subsequent steps and to avoid grouping or to push grouping towards the end of the processing pipeline. CoGroupByKey is very similar. It groups results across several PCollections by key. For example, input KV ends KW output K with an iteration of V and an iteration of W. CoGroupByKey performs a relational join of two or more key value PCollections that have the same key type. Now, we can move to the reduce phase. How do we calculate totals or averages or other aggregations on our PCollections? Combined is used to combine collections of elements or values in your data. Combine has variants that work on entire PCollections and some that combined the values for each key and PCollections of key-value pairs. CombineGloballlyfn reduces a PCollection to a single value by applying the FN or the function. CombinePerKey is similar to GroupByKey, but combines the values by a combined function or a callable that takes an iterable such as sum or max. When you apply a combine transform, you must provide the function that contains the logic for combining the elements or values. There are pre-built combined functions for common numeric combination operations such as sum, min, and max. Simple combine operations such as sums can usually be implemented as a simple function. More complex combination operations might require you to create a subclass of a combine function that has an accumulation type distinct from the input and or output site. The combining function should be commutative and associative, as the function is not necessarily invoked exactly once on all values within a given key. Because the input data including the value collection may be distributed across multiple workers, the combining function might be called multiple times to perform multiple combining on subsets of the value collection. For more complex combined functions, you can define a subclass of combine function. You should use the combine function if the combine function requires a more sophisticated accumulator. Must perform additional pre or post processing. Might change the output type or takes the key into accounts. A general combining operation consists of four operations. When you create a subclass of combine function, you must provide four operations by overriding the corresponding methods. create_accumulator creates a new local accumulator. In the example case taking a mean average, a local accumulator tracks the running sum of values. The numerator value for our final average division and the number of values sum so far the denominator value. And may be called any number of times in a distributed fashion. add_input adds an input element to an accumulator returning the accumulator value. In our example, it would update the sum and increment the count, and may also be invoked in parallel. merge_accumulators merge several accumulators into a single accumulator. This is how data in multiple accumulators is combines before the final calculation. In the case of the mean average computation, the accumulators representing each portion of the division are merged together. It may be called again on its output any number of times. extract_output performs the final computation. In the case of computing the mean average, this means dividing the combine sum of all of the values by the number of values sumed. It's called once on the final merged accumulator. Combine is orders of magnitude faster than GroupByKey because cloud data flow knows how to parallelize a combine step. The way that GroupByKey works, data flow can use no more than one worker per key. In this example, GroupByKey causes all of the values to be shuffled, so they're all transmitted over the network. And then there is one worker for the X key and one worker for the Y key. Combine allows data flow to distribute a key to multiple workers and process it in parallel. In this example, CombineByKey first aggregates values and then processes the aggregates with multiple workers. Also, only six aggregate values need to be passed over the network. Combine is a Java interface that tells data flow that the combine operation, like count, is both commutative and associative. This allows data flow to chard within a key versus having to group each key first. As a developer, you can create your own custom combined class for any operation that has commutative and associative properties. Flatten works a lot like a SQL UNION. It's a beam transform for PCollection objects that store the same data type. Flatten merges multiple PCollection objects into a single logical PCollection. Partition is also a beam transform for PCollection objects that store the same data type. Partition splits a single PCollection into a fixed number of smaller collections. You might use partition if, for example, you wanted to calculate percentages or quartiles and the top quartile has different processing than all the others.

## 7. Side Inputs and Windows of data

* In addition to the main input PCollection, you can provide additional inputs to a ParDo transform in the form of side inputs. A side input is an additional input that your do function can access each time a process is an elements in the input PCollection. When you specify a side input, you create a view of some other data that can be read from within the ParDo transforms Do function while processing each element. Side inputs are useful if your ParDo needs to inject additional data when processing each elements in the input PCollection. But the additional data needs to be determined at run time and not hard-coded. Such values might be determined by the input data or depend on a different branch of your pipeline. Here's how site inputs work. And here's how it looks in Python. This set of steps is actually a subgraph of our overall graph. It begins with words that run through the map function to get the length and then combine globally to compute the total lengths across the whole data set. So if we were trying to figure out if any given word is shorter or longer than the average word length, first we need to compute the average word length using these steps. But then this whole branch can be fed into this method. That's what creates the view which is static and then becomes available to all the worker nodes for later use. That is a side input you see here. Before we go to the next lab a few notes about additional capabilities. Many transforms have two parts. One occurs item at a time until all items processed and another occurs after the last item is processed. One of the easiest analogies is the arithmetic mean. You add up the value of each element and keep counts. This is the accumulation step. After you've processed all of the elements, you have a total of all of the values read and a count of the number of values read. The last thing to do is to divide the total by the counts. This is fine so long as you know you have read the last item. But if you have an unbounded data set, there is no predetermined end. So you just keep adding and never break out of the loop to perform the division. The global window is not very useful for an unbounded PCollection, meaning streaming data. The timing associated with the elements and an unbounded PCollection is usually important to processing the data. An unbounded PCollection has no defined end our last element, so it can never perform the completion step. This is particularly important for group by key and combined, which perform the shuffle after end. The discussion about unbounded PCollections and Windows will be continued in the course on processing streaming data. The global window is a default and here you see how you can set it with beam.WindowInto(window.GlobalWindows). So we're streaming pipelines out of luck if they can't use the global window. No, you can use time-based Windows, which can be useful for processing data that comes in streaming at different times. We'll cover this in detail in the streaming course. For batch inputs, you can Group by time as well. You can explicitly admit a timestamp in your pipeline instead of standard output. In this example, an offline access log is being read, and the date time stamp is extracted and used for windowing. Here we're using Windows to aggregate our batch data by time. Subsequent groups, aggregations, and so forth are computed only within the time Window. This example here uses a sliding window, as you can see with beam.WindowInto(beam,window.SlidingWindows (60, 30)), which means capture 60 seconds worth of data, but start a new window every 30 seconds. So for example, say you had all of your sales records and you wanted to compute sales by day. You'd just extract that timestamp field that represents the timestamp. Then you would create fixed windows with a one-day duration and data flow automatically will compute the sum over each window to compute those totals. The main thing to remember here is that you can do this in batch. 

## 8. Creating and re-using Pipeline Templates

* Next, we'll look at Dataflow Templates where you as a data engineer can create new templates for your team to leverage. You can also start from some of Google's preexisting templates which we'll cover as well. Cloud Dataflow templates allow users who don't have any coding capability to execute their Dataflow job. It enables the rapid deployment of standard types of data transformation jobs, removing the need to develop the pipeline code, and removing the need to consider the management of components dependencies in the pipeline code. In the traditional workflow, the developer creates the pipeline in the development environment using the Dataflow SDK and Java or Python, and there are dependencies to the original language and SDK files. Whenever a job is submitted, it is reprocessed entirely or recompiled, there's no separation of developers from users. So the users basically have to be developers or have to have the same access and resources as developers. Dataflow Templates enable a new development an execution workflow. The templates help separate the development activities and the developers from the execution activities and the users. The user environment no longer has dependencies back to the development environment. The need for recompilation to run a job is limited. The new approach facilitates the scheduling of batch jobs and opens up more ways for users to submit jobs and more opportunities for automation. App developers, DB admins, analysts, and data scientists will use templates as a solution. You can also run them using the command-line tool, or REST API as you see here. Simply specify the GCS location of your template that you already have or you can use the Google-provided templates. I'll provide the link to this documentation page where you can see all the templates publicly available in GitHub. After you create and stage you're Cloud Dataflow template, execute the template with the Google Cloud Platform Console, Rest API, or Gcloud command-line tool. You can deploy Cloud Dataflow template jobs from many environments including App Engine standard environment, Cloud Functions, and other constraint environments. What if you wanted to create your own template? To create your own template, you'll add your own Value Providers. This is what parses the command line or optional arguments to your template, and that is how users can specify optional arguments. Once a template file is created, you call it from an API. You might not have considered this before but values like user options and input file are compiled into your job. They aren't just parameters, they are compile-time parameters. To make these values available to non-developer users, they have to be converted to runtime parameters. These works through the Value Provider Interface so that your users can set these values when the template is submitted. Value Provider can be used in IO, transformations, and your functions, and there are static and a subversions of Value Provider for more complex cases. This is a Java example for creating your own template. Note that Value Providers are passed down throughout the whole pipeline construction phase. Now sometimes we need to transform a value from what the user passes at runtime, to what a source or sink expects to consume. Nested Value Providers meet this need. Each template has associated metadata with it upon creation. This will help your downstream users know what your template is doing and what parameters it expects. The metadata file is located in the same directory as your template and simply has the underscore metadata suffix to the name.

## 9. Cloud Dataflow SQL pipelines

* Now let's talk about Dataflow SQL. Cloud Dataflow SQL integrates with Apache Beam SQL and supports a variant of the ZetaSQL query syntax. You can use ZetaSQL streaming extensions to define your streaming data parallel processing pipelines. Use your existing SQL skills to develop and run streaming pipelines from the BigQuery web UI. You don't need to set up an SDK environments or know how to program in Java or Python. Join streams such as Cloud, Pub/Sub with snapshotted data sets such as BigQuery tables. Query your streams at static datasets with SQL by associating schemas with objects such as tables, files, and Cloud Pub/Sub topics. Write your results into BigQuery tables for analysis and dashboarding. Selecting Cloud Dataflow as the execution engine for SQL statements using the BigQuery web UI is currently available only as an alpha release and you should not use it in production. You've made it to the end of the module, so let's do a quick recap. Earlier in the course, you learned how to do batch processing of your Hadoop and Spark jobs using Cloud Dataproc. This is an ideal percepts for the Cloud for existing jobs, simply run them on Dataproc and they just work. You learned the Cloud Dataflow takes a lot of the cluster resizing and other management tasks and automate some for you as it choose serverless products. Use Cloud Dataflow if you're writing new pipelines or if you're ready to rewrite and migrate your Hadoop jobs to faster processing with Apache Beam on Dataflow. You then saw how to build pipelines using Apache Beam which is open source. For the pipelines to work, we created inputs with a beam.io syntax and walks through how you can read CSV files from GCS, streaming message queues from Pub/Sub, and structure data already living in BigQuery. We then looked at some key considerations when designing your pipeline. Recall that you should consider using Combine when you can instead of GroupByKey especially if your data is heavily skewed. This will prevent a single worker from being a bottleneck if you have a high cardinality dataset. To do the actual transformations, you practice writing PTransforms in your labs. Remember that the P in PTransforms and PCollections means parallel. Recall that the PCollection itself is immutable. Data is never process in place, a new PCollection is always created and that the individual elements of a PCollection are massively distributed over many workers to perform the parallel transform. This is a whole map part of MapReduce. For the reduced part of MapReduce, we looked at aggregation functions like GroupByKey and Combine. Keep in mind you can have multiple parallel parts of your pipeline Combine into a single PTransform like in aggregation. The pipeline does not have to execute in serial unless you've set it up that way with dependencies. After that you practice with side inputs in your lab and how to create Windows of data even for batch datasets. Lastly, you saw how to create and save new Dataflow templates for your team to use and where you can see Google's pre-made templates in our public GitHub. We ended our discussion with Dataflow's new ability to run SQL on your pipelines. That's it for batch data processing with Dataflow, keep progressing in the course content where we'll cover streaming data pipelines.

## QuizNotes

* Which of the following statements are true?
	* Dataflow executes Apache Beam pipelines
	* Dataflow transforms support both batch and streaming pipelines
* Match each of the Dataflow terms with what they do in the life of a dataflow job:
	* Sink: Output endpoint for your pipeline
	* Transform: A data processing operation or step in your pipeline
	* PCollection: A set of data in your pipeline