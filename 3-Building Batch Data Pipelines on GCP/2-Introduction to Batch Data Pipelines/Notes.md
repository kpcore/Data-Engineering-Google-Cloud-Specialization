## 1. EL, ELT, ETL

* Welcome to the course on building batch pipelines in Google Cloud. We'll focus on streaming pipelines in the next course. What are batch pipelines? These are pipelines that process a bounded amounts of data and then exit. For example, you might have a batch pipeline that runs once a day and takes all of the credit and debit and money transfer transactions over that day, balances the books and writes out the reconciled data to the data warehouse. In this module, we'll review the various decisions that you have to make in order to properly build out a pipeline such as that. We'll do a quick refresher on EL, ELT and ETL including how to decide when to use each methodology. We'll discuss how the selection depends on the types of transformations that you need and how these in turn hinge on data quality considerations. We'll look at how to build pipelines directory in BigQuery. And finally, we'll review the circumstances in which EL and ELT aren't enough and when you might want to use ETL instead. Since this was covered in a previous course, let's do a very quick recap. EL is extract and load. This refers to when data can be imported as is into a system. Examples include importing data from a database where the source and the target have the same schema. ELT allows raw data to be loaded directly into the target and transformed wherever it is needed. For example, you might provide access to the raw data through a view that determines whether a user wants to see all transactions or only reconciled ones. When the amount of transformation you need is significant, you might want to bring in the heavy machinery. That's ETL. Extract, transform, and load is a data integration process in which transformation takes place in an intermediate surface before it's loaded into the target. For example, the data might be transformed in Cloud Dataflow before being loaded into BigQuery. When would you use EL? One case when you'd use EL is if the data are already clean and corrects. If you aren't doing any streaming, not doing any kind of aggregation, then it makes sense to just load the data directly. Use EL for batch loading historical data or do schedule loads of log files. Perhaps you have the log files in Google Cloud Storage. You could extract it from Cloud Storage and load it into BigQuery using a simple REST API call. You can also trigger this pipeline from Cloud Composer, Cloud Functions or scheduled queries. But let me emphasize only use EL if the data are already clean and correct. Well, what about ELT? When would you use that? ELT starts with EL, so the loading is the same and could work the same way. File hits cloud storage, function invokes BigQuery load and a table is appended to. The big difference is what happens next. That's the T, which stands for transform. So if you got to transform the data and there are all sorts of ways that this can be done. For example, the table might be stored in a private data set and everyone accesses the data through a view which imposes data integrity checks or performs calculations on the fly. Or maybe you have a job that materializes the results of a SQL query into a destination table. This way everybody accesses the resulting table with the transformed data instead of the table with the raw data. One common case is when you don't know what kinds of transformations are needed to make the data usable. For example, let's say someone runs an image through the Cloud Vision API, which outputs a long JSON message with all kinds of data about things in the image. Things like text in the image, whether there's a landmark or a logo or what type of objects are in the image. Since you often don't know what of this data an analyst will need in the future, you can store the raw JSON as is. Later, if someone wants to count the number of times a specific company's logo is in this set of images, they can extract logos from the JSON and then count them. Of course, this only works if the transformation that's needed can be expressed in SQL. BigQuery SQL has support for JSON parsing. So in this case, we're good to go.

## 2. Quality considerations

* We're going to come back to ETL in a bit. But now that we've looked at EL and ELT, let's look at some of the transformations you might want to do and how they can be done in BigQuery. To keep things contained let's assume that all of our data processing needs revolve around quality improvements. What are some of the quality-related reasons that we might want to process data? Here are five dimensions of quality: validity, accuracy, completeness, consistency, and uniformity. Note that each dimension is independent. For example, data can be complete without being consistent. It can be valid without being uniform. There's one thing to seek each of these five badges for your data to have objectively good data quality. However, it's another thing when poor data quality interferes with data analysis and lead to incorrect business decisions. So it's well worth spending the time energy and resources detecting and resolving quality issues so they don't negatively affect a business outcome. In the next video, we'll explore methods of detecting each of these issues in data. We're also going to learn how you might address the dimensions of quality using BigQuery and SQL. ELT and BigQuery can often help fix many data quality issues. Here's an example. Imagine you plan to analyze data, but there are duplicate records making it seem like one event is more common when in fact, it's just a data quality issue. You can't derive insights from the data until the duplicates are removed. So do you need a transformation steps to remove the duplicate data before you store it? Maybe but a simpler solution exists to count unique records, and you can simply use count distinct in BigQuery. Similarly, BigQuery is able to help resolve other issues.

## 3. How to carry out operations in BigQuery

* In this section, we'll take a look at exactly how BigQuery can help with some of those data quality issues we just described. Let's start with validity, what do we mean by invalid? It can mean things like corrupted data maybe data that is missing a timestamp. It's really anything that doesn't conform to your business rules. We can use views to filter out values that have quality issues. For example, remove quantity is less than zero using a WHERE clause or after you do a groupBy you can discard groups, whose total number of records is less than 10 using the HAVING clause. Think carefully about how you use treat NULLS and blanks. A NULL is the absence of data, a blank is an empty string. Consider if you're trying to filter out one or both of them. You can easily count non-NULL values using count IF, and use the IF statement to avoid using specific values in computations. Data validity is a good use case for using a Google Sheet as an external table inquiry because if you have some other group that defines the valid values for any given lookup you can join that right into your Query. Next let's look at consistency, consistency is anytime your computations from Data are incorrect. For example if you're calculating sum totals and storing them in a separate table than the detailed data. You could have consistency issues upstream of the transaction. So how can you deal with consistency issues, consistency problems are often due to duplicates. You expect that something is unique and it isn't so many things like totals are wrong. Counts and Count Distinct can help us discover any issues we might have. Similarly,, if you do a groupBy and any group contains more than one row. Then you know you have two or more occurrences of that value. Another reason that you might have consistency problems is if extra characters get tacked on to fields. You string functions like parse state, sub-string are replaced to clean such data. Accuracy is another dimension of quality, accuracy can also be a lookup issue if some data doesn't conform to some objective true value. For accuracy, test a data against known good values. For example if you have an order you could compute the subtotal from quantity ordered and item price and make sure that the math checks out. Similarly, you can check if a value that is being inserted belongs to a canonical list of acceptable values. You can do this with a SQL in clause with a subquery or a join. For completeness, identify any missing values and either filter it out or replace it by something reasonable. If the missing value is NULL, SQL provides functions like NULLIF, COUNTIF and COALESCE to filter them out of the calculations. You might be able to do a union from another source to fill out missing months of data. The automatic process of detecting Data drops and requesting data items to fill in the gaps is called backfilling. It's a feature of some data transfer services. When loading data, verify file integrity with checksum values like hash and MD5. In the fifth dimension uniformity, this one is usually an issue that isn't in the data itself. What happens if you're storing some value in centimeters and suddenly you start getting the value in millimeters. Your data Warehouse will end up with a non-uniform data and you've got to safeguard against this. Use SQL: CAST to avoid issues with datatypes changing within a table, Use the SQL: FORMAT function to clearly indicate units and in general document them very clearly. If you take anything away from this section, it's the fact that BigQuery SQL is very powerful and readily available for you to take advantage of.

## 4. Shortcomings

* In the previous section, we showed you that you don't always need ETL. Instead, you can use SQL in an ELT pipeline to safeguard against quality issues or perform transformations. However, there are situations where ELT won't be enough. In that case, ETL might be what you need to do. Let's dig into some of those situations. The first example translating Spanish and excuse my accent, [inaudible] to English, I like this technical course, requires an external API. It can't be done in SQL. The second example looking at a stream of customer actions over a time window is complex. You can do it with windowed aggregations, but it's far simpler with logic. So if the transformations cannot be expressed in SQL or are too complex to do in SQL, you might want to transform the data before loading it into BigQuery. The reference architecture for GCP suggests Cloud Dataflow is an ETL tool. We recommend that you build ETL pipelines in Dataflow and learn the data in BigQuery. The architecture looks like this. Extract data from Pub/Sub, Google Cloud Storage, Cloud Spanner, Cloud SQL, or other sources. Transform the data using Cloud Dataflow and have the Dataflow pipeline write to BigQuery. When would you do this? When the raw data needs to be quality-controlled, transformed, or enriched before loading into BigQuery and the transforms are too difficult to do in SQL. When the Data loading has to happen continuously, for example, if the use case requires streaming. Dataflow support streaming, which we'll look at in more detail in the next course. When you want to integrate with CI/CD, continuous integration, continuous delivery systems and perform unit testing on all components. It's easy to schedule the launch of a Dataflow pipeline. Dataflow isn't the only option you have on GCP if you want to do ETL. In this course, we'll look at several data processing and transformation services that GCP provides; Cloud Dataflow, Cloud Dataproc, and Cloud Data Fusion. Well, it makes sense to use BigQuery for simpler ELT transformations and it's easier to get set up. It shouldn't be your only ETL tool. Cloud Dataproc and Cloud Dataflow can be used for more complex ETL pipelines. Cloud Dataproc is based on Apache Hadoop and require significant Hadoop expertise to leverage directly. Cloud Data Fusion provides a simple to use graphical interface to build ETL pipelines that can then be easily deployed at scale to Cloud Dataproc clusters. Cloud Dataflow is a fully-managed serverless data processing service based on Apache Beam that supports both batch and streaming data processing pipelines. While significant Apache Beam expertise is desirable in order to leverage the full power of Cloud Dataflow, Google also provides QuickStart templates for Cloud Dataflow to allow you to rapidly deploy a number of useful data pipelines. You can use any of these three products to carry out data transformation and then store the data in a data lake or a data warehouse to support advanced analytics.

## 5. ETL to solve data quality issues

* So now, let's look at using ETL to solve data quality issues. Unless you have specific needs, we recommend that you use Dataflow and BigQuery. What could those needs be? First, latency and throughput. BigQuery queries are subject to a latency on the order of a few 100 milliseconds, less if you're leveraging BI Engine, and you can stream on the order of a million rows per second into a BigQuery table. If your latency and throughput considerations are more stringent, then Cloud Bigtable might be the better sync for your data processing pipeline. Or perhaps you want to reuse Spark pipelines. If you already have a significant investment in Hadoop and Spark, you might be more productive in a familiar technology. Use Spark if that's what you really know well. Lastly, if you have a need for visual pipeline building. Dataflow requires you to code data pipelines in Java or Python. If you want to have data analysts and non-technical users create data pipelines use Cloud Data Fusion. They can drag and drop and visually build pipelines. We'll look at the last two options briefly now, and in greater detail in the remainder of this course. Cloud Dataproc is a managed service for batch processing, querying, streaming, and machine learning. It provides a managed service for Hadoop workloads and is quite cost-effective. About $0.01 more than the cost of running it bare-metal, but eliminating all the typical Hadoop maintenance activities. It also has a few cool features like auto-scaling, and out of the box integration with GCP products like BigQuery. Some benefits, it's fast and scalable, open-source, fully-managed, it has versioning, it's integrated with the GCP and open source ecosystems, and very cost-effective. Cloud Data Fusion is a fully-managed, cloud native, enterprise data integration service for quickly building and managing data pipelines. You can use it to populate a data warehouse, but you can also use it for transformations, cleanup, and ensuring data consistency. Users who can be part of the business can build visual pipelines to address business imperatives like regulatory compliance, without having to wait for an IT team to code up a Dataflow pipeline. Data Fusion also has an API to code against. IT folks can use it to script and automate. Regardless of which ETL you use, Dataflow, Dataproc or Data Fusion, there are some crucial aspects to keep in mind. First, maintaining data lineage is important. What do we mean by lineage? Things like where the data came from, what processes it has been through, and what condition is in? It also tells you what kinds of uses a data is suited for as well as the current condition of the data, and the processes it might need to undergo to be suitable for an intended use. If you find the data gives odd results, you can check the lineage to find out if there's a cause that can be corrected. Lineage also helps with trust and regulatory compliance. The other cost fitting concern is that you need to keep metadata around. You need a way to track the lineage of data in your organization for discovery and identification of suitability for users. On Google Cloud, Cloud Data Catalog provides discoverability, but you have to do your bit by adding labels. A label is a key value pair that helps you organize your resources. In BigQuery you can attach labels to datasets, tables, and views. Labels are useful for managing complex resources because you can filter them based on their labels. Labels are a first step toward a data catalog. Among the things that labels help with is Cloud billing. If you attach labels to Compute Engine instances, to buckets, and to Dataflow pipelines, information about those labels will be forwarded to the billing system. This allows you ways of break down your billing charges by label and gives you a fine-grain look at your Cloud bill. Think of Data Catalog as a metadata as a service. It provides metadata management services for cataloging data assets via custom APIs and the UI, thereby providing a unified view of data wherever it is. It supports schematized tags like enum, bool, or datetime and not just simple text tags, providing organizations reach an organized business metadata. It's serverless and requires no infrastructure to set up or manage. Data Catalog empowers users to annotate business metadata in a collaborative manner and provides a foundation for data governance. So what did you learn in this module? First, you got a quick refresher on when to use EL and ELT. Then you learned about the power of BigQuery SQL to solve many data quality issues and perform transformations. Finally, we discussed the use of ETL for circumstances where EL or ELT might not suffice. Tune in to the following modules to learn more about batch and streaming data pipelines.

## QuizNotes

* Which of the following is the ideal use case for Extract and Load (EL)
	* Scheduled periodic loads of log files (e.g. once a day)