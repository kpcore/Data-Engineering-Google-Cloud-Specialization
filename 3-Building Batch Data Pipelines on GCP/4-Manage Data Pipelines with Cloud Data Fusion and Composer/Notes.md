## 1. Introduction

* In this module, we will discuss how to manage data pipelines with Cloud Data Fusion and Cloud Composer. Cloud Data Fusion provides a graphical user interface and APIs that increase time efficiency and reduce complexity. It equips business users, developers, and data scientists to quickly and easily build, deploy, and manage data integration pipelines. Cloud Data Fusion is essentially a graphical no code tool to build data pipelines. Data Fusion is used by developers, data scientists, and business analysts alike. For developers, Data Fusion allows you to cleanse, match, remove duplicates, blend, transform, partition, transfer, standardize, automate, and monitor data. Data scientists can use Cloud Data Fusion to visually build integration pipelines, test, debug, and deploy applications. Business analysts can run Cloud Data Fusion at scale on GCP, operationalized pipelines, and inspect rich integration metadata. Behind the scenes, Cloud Data Fusion creates ephemeral criminal environments to run pipelines. In the beta release, Data Fusion supports Cloud Dataproc as an execution environment where you can choose to run pipelines such as MapReduce, Spark, or Spark Streaming programs. Data Fusion provisions an ephemeral Cloud Dataproc cluster in your customer project at the beginning of a pipeline run. Executes the pipeline using MapReduce or Spark in the cluster, and then tears the cluster down after the pipeline execution is complete. Alternatively, if you manage your Cloud Dataproc clusters in controlled environments through technologies like terraform. You can also configure Data Fusion not to provision clusters. In such environments, you can run pipelines against existing Cloud Dataproc clusters. You can create multiple instances in a single project and can specify a GCP region to create instances. Based on requirements and cost constraints, you can create a basic or an enterprise instance. Each instance contains a unique independent Data Fusion deployment that contains a set of services which are responsible for pipeline life-cycle management, orchestration, coordination, and metadata management. These services are run using long running resources and a tenant project. Note, although each instance creates long-running resources, you are only charged for pipeline execution beyond upfront cost. A long running but idle instance does not incur additional charges over time. You only incur charges when you run pipelines to process data using the instance. Data Fusion creates instances on a GKE cluster inside a tenant project. You can find more details about the resources used by an instance in architecture components. You can create and manage Data Fusion instances using the GCP console UI, by clicking the Data Fusion link in the big data section. Let's take a closer look at how Data Fusion is integrated with GCP. At the time of this recording, Data Fusion executes your pipelines on Cloud Dataproc UI instance we just saw. There was future support coming for executing Cloud data-flow in the future. Inside of a Data Fusion instance that is booted up on a Dataproc VM, are these five core products and services listed here. Data Fusion runs in a containerized environment on GKE, with persistent disks and long-term data storage on Cloud storage. To manage user and pipeline data, is backed by a Cloud SQL database, and uses the key management service. All of that said, you will likely never interact with any of these underlying services as a goal of Data Fusion is to abstract them away from you, so you can focus your time on exploring data sets and building beautiful pipelines with no code and in a UI. Through the rest of this module, we'll show you tips and tricks for working in the Data Fusion UI as you will practice in your lab. At a high level, Data Fusion provides you with a graphical user interface to build data pipelines with no code. You can use existing templates, connectors to GCP, and other Cloud services providers and an entire library of transformations to help you get your data in the format and quality you want. Also, you can test and debug the pipeline and follow along with each node as it receives and processes data. As you will see in the next section, you can tag pipelines to help organize them more efficiently for your team, and you can use the unified search functionality to quickly find field values or other keywords across your pipelines and schemas. Lastly, we will talk about how Data Fusion tracks the lineage of transformations that happen before and after any given field on your dataset. One of the advantages of Cloud Data Fusion is that it's extensible. This includes the ability to templatize pipelines, create conditional triggers, and manage and templatize plugins. There is a UI widget plug-in as well as custom provisioners, custom compute profiles, and the ability to integrate to hub. The two major user interface components we will focus our attention on in this course, are the wrangler UI for exploring data sets visually, and building pipelines with no code, and the data pipeline UI for drawing pipelines right on to a Canvas. You can choose from existing templates for common data processing tasks like GCS, to BigQuery. There are other features of Cloud Data Fusion that you should be aware of too. There's an integrated rules engine where business users can program in their pre-defined checks and transformations, and store them in a single place. Then data engineers can call these rules as part of a rule book or pipeline later. We mentioned data lineage as part of field metadata earlier. You can use the metadata aggregator to access the lineage of each field in a single UI and analyze other rich metadata about your pipelines and schemas as well. For example, you can create and share a data dictionary for your schemas directly within the tool.

## 2. Components of Data Fusion

* Managing your pipelines is easiest when you have the right tools, in this module, we'll take a high-level look. At the Cloud Data Fusion UI as you saw in the component overview, here are some of the key user interface elements that you will encounter when using Cloud Data Fusion. Let's look at each of them in turn under control center. There is a section for applications, artifacts and a data set, here you could have multiple pipelines associated with a particular application. The control center gives you the ability to see everything at a glance and search for what you need, whether it's particular dataset, pipeline or other artifact like a data dictionary, for example. Under the pipeline section you have a Developer Studio. You can preview, export, schedule a job or a project. You also have a connector and function palette and a navigation section as well. You'll be spending a lot of your time here. So, we will explain this page in more detail soon, under the Wrangler section. You have connections, transforms, data quality, insights and functions under the integration metadata section. You can search, add tags and properties and see the data lineage for field and data, the Hub allows you to see all the available plugins, sample use cases and pre-build pipelines. Entities include, the ability to create pipelines, upload an application, plug-in, driver library and directives. There are two components in administration, management and configuration under management. You have services and metrics, under configuration. You have namespace, compute profiles, preferences, system artifacts and the rest client. Next we'll focus on the two pages where you will be spending most of your time building pipelines and wrangling data.

## 3. Building a Pipeline

* Now that we've looked at the components in the UI, we will discuss the process of building a data pipeline. A pipeline is represented visually as a series of stages arranged in a graph. These graphs are called DAGs or directed acyclic graphs because they flow from one direction to another and they can not feed into themselves. Acyclic simply means not a circle. Each stage is a node and as you can see here, it can be of a different type. You may start with a node that pulls data from Google Cloud storage, then passes it on to a node that passes a CSV. The next node takes multiple nodes, has an input, and joins them together before passing the join data to two separate data sync nodes. As you saw in our previous example, you can have multiple nodes for out from a single parent node. This is useful because you may want to kick off another data processing workstream that should not be blocked by any processing on a separate series of nodes. Naturally, you can combine data from two or more nodes into a single output in a sync as you saw before. In data fusion, the studio is your user interface where you author and create these new pipelines. I'll highlight some of the major features that you will explore later in your lab. First, the area where you create these nodes and chain them together in your pipeline, that's your canvas. If you have many nodes in a pipeline, the canvas can get visually cluttered, so use the mini map to help navigate around a huge pipeline quickly. You can interact with the canvas and add objects by using the Canvas Control Panel. When you're ready to save and run the entire pipeline, you can do so with the pipeline actions toolbar at the top. Don't forget to give your pipeline a name and description, as well as make use of the many preexisting templates and plugins, so you don't have to write your pipeline from scratch. Here, you see we've used a template or data pipeline batch which gives us the three nodes you see here to move data from a GCS file, process it in a wrangler, and output it to BigQuery. You should make use of preview mode before you deploy and run your pipeline in production to ensure everything you run will run properly. While a pipeline is in preview, you can click on each node and see any sample data or errors that you will need to correct before deploying. After deployment, you can monitor the health of your pipeline and collect key summary stats of each execution. Here, we are ingesting data from Twitter and Google Cloud platform, and passing each tweet before loading them into a variety of data syncs.
Start transcript at 3 minutes 14 seconds3:14
If you have multiple pipelines, I recommend you make liberal use of the tags feature to help you quickly find and organize each pipeline for your organization. As you see here, you can view the start time, the duration of the pipeline run, and the overall summary across runs for each pipeline. Again, you can quickly see the data throughput at each node in the pipeline simply by interacting with the node. One last thing you'll notice is the compute profile used in the Cloud. Currently at the time of recording, Cloud Data Fusion supports running on Cloud Dataproc, but Cloud Data Source support is on the road map. Remember, clicking on a node gives you detail on the inputs, outputs, and errors for that given node. Here, we are integrating with the Cloud speech to text API to process audio files into searchable text. You can track the individual health of each node and get useful metrics like records out per second, average processing time, and max processing time, which can alert you to any anomalies in your pipeline. You can set your pipelines to run automatically at certain intervals. If your pipeline normally takes a long time to process the entire dataset, you can also specify a maximum number of concurrent runs to help avoid processing data unnecessarily. Keep in mind that Cloud Data Fusion is designed for batch data pipelines. We will dive into streaming data pipelines in future modules. One of the big features of Cloud Data Fusion is the ability to attract the lineage of a given field value. Let's take this example of a campaign field for double-click dataset and track every transform operation that happened before and after this field. Here, you can see the lineage of operations that are applied to the campaign field between the campaign dataset and the double-click dataset. Note the time this field was last changed by a pipeline run and each of the input fields and descriptions that interacted with the field as part of processing it between datasets. Imagine the use cases if you have inherited a set of analytical reports and you want to walk back upstream all of the logic that went into a certain field. Well now, you can.

## 4. Exploring Data using Wrangler

* We've discussed the core components, tools, and processes of building data pipelines. Now, we'll look at using Wrangler to explore the data set. So far in the course, we have focused on building new pipelines for our data sets. That presumes we know what the data is and what transformations need to be made already. Oftentimes, a new data set still needs to be explored and analyzed for insights. The Wrangler UI is the Cloud Data Fusion environment for exploring new data sets visually for insights. Here, you can inspect the data set and build a series of transformation steps called directives to sticks together a pipeline. Here's what the Wrangler UI looks like. Starting from the left, you have your connections to existing data sets. Here, you can add new connections to a variety of data sources like Google Cloud Storage, Google BigQuery, or even other Cloud providers. Once you specify your connection, you can browse all of the files or tables in that source. Here, you see a GCS bucket of demo data sets and all the CSV files of customer complaints. Once you've found an example data set like customers.csv here, you can explore the rows and columns visually and view sample insights. As you explore the data, you might want to create new calculated fields, drop columns, filter rows, or otherwise wrangle the data. You can do so using the Wrangler UI by adding new directives to form a data transformation recipe. When you're happy with your transformations, you can create a pipeline that you can then run at regular intervals.

## 5. Orchestrating work between GCP services with Cloud Composer

* The next big task for managing data pipelines is to orchestrate the work across multiple GCP services. For example, if you had three Data Fusion Pipelines and two ML models that you want it to run in a certain order, you need an orchestration engine. In this module, we'll look at using Cloud Composer to help out with task like that. Cloud Composer will command the GCP services that we need to run. But Cloud Composer is simply a serverless environment in which an open-source workflow tool runs. That workflow tool is called Apache Airflow, which is an open-source orchestration engine. The heart of any workflow, and by the way, I use workflow pipeline and DAG pretty interchangeably, is to DAG. As you saw with Data Fusion, you're also building DAGs with Apache Airflow as you see here. The stuff that's happening in this particular dagger, four tasks that update our training data, export it, we train our model, and we deploy it. I purposely, I'm being ambiguous by using the word stuff because you can tell your DAG to pretty much do anything you need it to do. Here it's sending tasks to BigQuery, GCS, and Cloud AI platform, but yours could orchestrate among four completely different services.

## 6. Apache Airflow Environment

* Let's start by previewing the actual Cloud Composer environment. Once you use the command line or GCP web UI to launch a Cloud Composer instance, you'll be met with a screen like this. Keep in mind that you can have multiple Cloud Composer environments and with each environment, you can have a separate Apache Airflow Instance, which could have zero to many DAGs. An important note here is that sometimes you'll be required to edit and environment variables for your workflows like specifying your specific GCP project account. Normally, you will not need to do that at the Cloud Composer level but on the actual Apache Airflow Instance level, which we will show you in the next demo. To access the airflow Admin UI where you can monitor and interact with your workflows, you'll click on the link underneath Airflow webserver. The second box you see is the DAGs filter which is where the code of your actual workflows will be stored. The DAGs folder for each Airflow Instance is simply a GCS bucket that is automatically created for when you launch your Cloud Composer Instance. Here is where you upload your DAG files and bring your first workflow to life in Airflow.

## 7. DAGs and Operators

* Now that you're familiar with basic environment setup, it's time to discuss your primary artifact which is your DAG and the operators you're using to call whichever services you want to send task to. First, airflow workflows are written in Python. You'll have one python file for each tag. For example, here we have simple_load_dag.py in our dag folder GCS bucket where you can see a preview of what the DAG file looks like. Don't worry about reading the code, we'll go into that later. It's sufficient enough for now to just know that there are a series of user-created tasks and each DAG file that invoke predefined operators like this task, which uses the data flow Python operator and is given the task ID of process delimited and push. We'll go over creating a DAG file and its components a little later. Once you've uploaded the Python file to the DAG folder, you can navigate back to the airflow web server. And under DAGs directed graph with nose and edges. You remember that the Python code that defined a task we called process delimited and push is now a node in our graph here. Let's explore a bit more of the airflow web UI. You can see that this particular workflow is called GcsToBigQueryTriggered and it has three tasks when it runs. One, process delimited and push from that Python file you saw earlier invokes a data flow job to read in a new CSV file from a GCS bucket processes it and writes the output to be query. Two, success move to completion which moves the CSV file from an input GCS bucket to a process or completed GCS bucket for archiving. Three, if the pipeline fails part way, the file is moved to the completion bucket, but tagged has failure. This is an example of a DAG, which isn't strictly sequential. There, a decision is made to run one node or a different one based on the outcome of the parent node. But regardless of the size and the shape of your workflow DAG, one common thread for all workflows is the common operators used. It's a dagger itself is how to run the workflow. First, do step one, then either move to step two or three. The operators specified what actually gets done as part of the task. In this simple example, we're calling on the data flow python operator and the general Python operator. Those are by no means the only operator. So let's pause here and look at all the operators at our disposal to achieve our goal of automatic retraining, and deployment of our ml model. Airflow has many operators where you can invoke the tasks you want to complete. Operators are usually atomic and a task, which means generally you only see one operator per task. I've taken this list directly from the Apache airflow docks of all services that airflow can orchestrate to. Let's take a look at the ones that your ml engineering team we care about the most. As you might have guessed, we'll certainly be making use of the VBgQuery operators since our workflows live and die by the data that is fed into them through GCS and BigQuery. Here's a list of the specific operators that we can invoke in a task to call on the BbigQuery service for querying and other data related tasks. You will be mainly working with the first three in this course, but I encourage you to skim the resource link on all the operators, so that you can get a feel for what is possible. Once we have our training data and a good place, the next logical step in our workflow is to retrain and redeploy our model. In the same DAG, after the BigQuery operators are complete we can make a service call through a cloud. Ml engine operator to kick off a new training job and manage our model. Like incrementing the version for example note that although ml engine in name has become AI platform at the time of this recording the airflow integration still use the NL engine name. You might have noticed that your are flowed. I can have operators that send tasks out to other Cloud providers. This is great for hybrid workflows where you have components across multiple Cloud platforms or even on-premise. Apache airflow is open source and continually adds more operators to other services. So be sure to check out the list in the documentation so that if you're waiting for a new service, it may have already been added here. You see four tasks T1, T2 T3 and T4 and for operators corresponding to for Google Cloud platform services. The names should look familiar and you can probably start to guess what this pipeline does at a high level just by reading them in order. The first two are concerned with getting fresh model data from a BigQuery data set and into GCS for consumption by our in male model later in the pipeline. In the lab, you're going to work on later. The data set will be the one you are already familiar with. The Google Analytics news articles sample data set. Let's see the parameters the BigQuery operator takes. The BigQuery operator allows you to specify a SQL query to run against a big query data set in this quick example. We're passing in a query which returns the top 100 most popular stack overflow posts from the BigQuery public data set for a specified date range there in the where clause. Notice anything different about the filters in the where clause? Yes, they are parameters. In this case, for a Mac state and mandate. You can parameterize pieces of the SQL statement like what we did here to only return posts for January 2018 with min query date and max query date. What's really neat is you can even make the parameters dynamic instead of the static ones shown here. And have them be based on the deck schedule date like micros.ds_addunderscore add(ds,-7) which is a week before the DAG scheduled one date. The next two operators handle retraining the model by submitting a job to cloud machine learning engine and then deploying the updated model to app engine. The ML Engine training operator is how cloud composer interacts with cloud machine learning engine. And thus, gives you the ability to periodically schedule new jobs to be sent as part of your automated workflow. Take a look through the parameters you provide to the operator. First, we specify an ID for the tasks that we are running and then the project we want to run it on. For our lab, this will be the same project that is also managing the airflow instance and everything else. Then you have the option of creating your own job ID, which usually involves a concatenated timestamp or similar unique identifier. After that is packed to the actual model code, which will given as IP file which is created by a shell script. After training is run in our lab, then the actual Python module name within your code is used to run within the ml engine training job after installing the package and this example we called it trainer dot task. Next is a series of arguments for training, which we store as an array called training_arcs which includes the location of the job directory where the training files are where the output directory is and what data were using. The next three region, scale_tear and master_type are GCP infrastructure parameters for where you want the job to be read and with what specialty hardware like GPUs or TV use if needed. Lastly, once your model is retrained and ready to be delivered as an API endpoint for serving, we need to redeploy our app engine project with the latest model. Here the parameters are simple since we're just redeploying our existing app engine instance associated with our project and that's it. Well, almost done the order in which we want. These operators to run is defined by the dependencies. In our example, t2 our task to won't run until t1 has completed. This is what gives our graph the dependencies of workflow. Now, you could build some branching of multiple child nodes per upstream parent. But if you do don't forget to comment your code, so you know where one branch begins where it's going and what tasks are involved. As a tip, after you load your DAG file into the decks folder, you can see the visualization of your DAG on the airflow UI has a directed graph or get chart or a list. Reviewing the visual representation of the order tasks will help you confirm that your tasks are ordered properly.

## 8. Workflow scheduling

* Now that you're familiar with the Cloud Composer and Apache Airflow environments and the basics of building a list of tasks for GCP services in your DAG, it's time to discuss a really important topic, workflow scheduling. There are two different ways your workflow can be run without manually clicking one DAG. The most common is a set schedule or periodic run of a workflow, such as once a day at 6 AM or weekly on Saturdays. The second is trigger based. You might choose this if you need your workflow to run whenever new CSV data files are loaded into a GCS bucket, or if new data comes in from a pub subtopic you've subscribed to. To view the schedules for your DAGs, you first launch the Airflow web server from within Cloud Composer. Then navigate to the DAGs tab to view the existing workflows that you have Python drag files for. Here we have two DAGs. The bottom one, composer_sample_simple_greeting has a daily schedule, but do you see anything odd? How will the first DAG in this example ever get run when it's missing a schedule, ie schedule equals none? The answer is it's not on a set schedule at all. It's event-driven. The driver of when this workflow runs is a cloud function that we create. In the next lesson, we'll actually create our own cloud function that watches a GCS bucket for new CSV files. If you want a regular schedule, you can specify the schedule_interval in your DAG code similar to what's shown here. Be aware that you cannot edit a schedule by clicking on a one-day schedule object in the UI. Doing so takes you to the history of all the runs for that workflow. As you saw earlier, there are two general patterns for ETL workflows. They can be event-triggered or push pattern, such as, if you push a new file to GCS and a workflow kicks off. Alternatively, they can be pull patterns, such as where or at a set time. Airflow looks in your cloud storage bucket folder and utilizes the content it finds there for its workflow run. We can use cloud functions to create our event-driven or push architecture workflow. Triggering on events within a cloud storage bucket is one method. But you can also trigger based on HTTP requests, pub, sub, Firestore, Firebase, and more. Generally, push technology is great when you want to distribute transactions as they happen. Stock tickers and other types of financial institution transactions are very important when it comes to push technology. How about disasters and notification, again, important. For ML workflows where your upstream data doesn't arrive at a regular pace, such as getting all the transactions at the end of each day, consider experimenting with the push architecture. Your final lab since it's based on regular Google Analytics news article data, will be a pull architecture. But there is an optional lab for you to get practice with cloud functions and event-driven workflows. Let's take a look. For our example, let's assume we have a CSV file or set of files loaded to Google Cloud Storage. Thus, we'll choose a Cloud Storage trigger for our function. Next, we specify an event type, Finalize Create new files, and a bucket to watch. As part of the cloud function, we need to create the actual function we want called in JavaScript. The good news is most of this code for triggering Airflow DAGs in a function is all boilerplate for you to copy from as a starting point. Here, we are simply specifying a name for our function. We'll call it triggerDag. Next, we tell it where your Airflow environment is to be triggered and reference which DAG in that Airflow environment is used. In this case, it's looking for one called GcsToBigQueryTriggered. Keep in mind, you can have multiple workflows or DAGs in a single Airflow environment. So be sure you specify the correct DAG_NAME to trigger. Next, we have a few constants which construct the Airflow URL. We'll use this in the post request that we'll trigger. They also define who's making the request and what the body of the request is. Lastly, the triggerDag function makes the actual request against the Airflow server to kick off a workflow DAG. Once you've got the cloud function code ready in your index.js file, and the metadata about the function in your package.json, which contains codependency and versioning information, you will need to specify which function you actually want executed. In this case, we created one called triggerDag, so we just copy that down. Something to be aware of is this, the function to execute box is case sensitive. For example, all capital letters DAG is different than capital D followed by lowercase a and g, and there you have it. Your cloud function has been created, and is actively watching your GCS bucket for file uploads. But how can you be sure anything is working as intended? Well for that, check out the next topic on monitoring and logging.

## 9. Monitoring and Logging

* By this point, we've got our environment setup with our DAGs running in a predefined schedule, or with triggered events. The last topic we'll cover before you practice what you've learned in your labs, is how to monitor and troubleshoot your Cloud Functions and Airflow Workflows. One of the most common reasons you'll want to investigate the historical runs of your DAGs is in the event that your workflows simply stops working. Note that you can have it auto retry for a set number of attempts in case it's such transient bug, but sometimes you just can't get your workflow to run at all in the beginning. In the DAG runs, you can monitor when you're Pipelines run and in what state, like success, running, or failure. The quickest way to get to this page is clicking on the schedule for any of your DAGs from the main DAG's page. Here, we have five successful runs over five days for this DAG. So this one seems to be running just fine. Back on the main page for DAGs, we see some red. We know it indicates trouble where some of our recent DAG runs, but what exactly does this mean? Which of these two pipelines is healthier? Speaking of DAG runs, you'll note the three circles below, which indicates how many runs passed, are currently active, or have failed. It certainly doesn't look good for 268 runs failed and zero passed for this first DAG. Let's see what happened. We click on the name of the DAG to get to the visual representation. It looks like the first task is succeeding, judging by the green border. But the next task, success-move-to-completion is failing. Note that the lighter pink color for the failure-move-to-completion node means that that node was skipped. So reading into this a bit, the CSV file was correctly processed by Dataflow in the first task, but there is some issue moving the CSV file to a different GCS bucket as part of task 2. To troubleshoot, click on the node of a particular task and then click "Logs." Here, you will find the logs for that Airflow run. This was a pretty simple error, where it was trying to copy a file from an input bucket to an output bucket, and the output bucket did not exist or was named poorly. Another tool in your toolkit for diagnosing Airflow failures is the general GCP logs. Since Airflow launches other GCP services through tasks, you can see and filter for errors for those services and Stackdriver as you would debugging any other normal application. Here, I filtered for Dataflow step errors to troubleshoot why my workflow is failing. It turns out that I had not changed the name of the output bucket for the CSV file. So after the file was processed by Dataflow as part of step 1, it dumped the completed file back into the input bucket which triggered another Dataflow job for processing and so on. You might be wondering, "If there's an error with my Cloud Function, would my Airflow instance have been triggered or issued any logs at all, since it was unaware we were trying to trigger it?" If you're using Cloud Functions, be sure to check the normal GCP logs for errors, and warnings in addition to your Airflow logs. In this example, each time I uploaded a CSV file to my GCS bucket hoping to trigger a Cloud Function, my DAG received an error, "Expected to find function triggered DAG." Remember way back when I said Cloud Functions were case-sensitive? Looking for a function with capital DAG does not exist if its capital D, lowercase A and G. So be sure to be mindful when setting up your Cloud Functions for the first time.

## QuizNotes

* Cloud Data Fusion is the ideal solution when you need
	* to build visual pipelines