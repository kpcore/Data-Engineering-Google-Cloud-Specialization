## 1. Introduction to Cloud Pub/Sub

* Now that we have a good understanding of the process of Streaming Data, let's dive into Cloud Pub/Sub to see how it works. As we begin this topic, make sure to keep an open mind to new ways of doing things. Cloud Pub/Sub does Streaming differently from probably anything you've used in the past. It may be a different model from what you've seen before. Cloud Pub/Sub provides a fully managed data ingestion and distribution system. It can be used for many purposes. In the previous module, we mentioned using Cloud Pub/Sub to ingest and distribute incoming Streaming events. Cloud Pub/Sub provides an asynchronous messaging bus, which can hold these events until they are consumed by respective services for further processing. You can use Cloud Pub/Sub to connect applications within Google Cloud, and with applications on-premise, or in other Clouds to create Hybrid Data Engineering solutions with Cloud Pub/Sub, the applications do not need to be online and available all the time. The parts don't need to know how to communicate to each other, just to Cloud Pub/Sub, which can help simplify system design. A good example is the email scenario, where sender and receiver do not need to be available at the same time, but the message still goes through, and the receiver ultimately we'll get to consume it when it's ready to receive it. Cloud Pub/Sub is a ready to use service with nothing to install, and Cloud Pub Sub Client Libraries are available in C-sharp, Go, Java, Node.js, Python and Ruby, to help you write your code. These wrap REST API calls which can be made in any language. Cloud Pub/Sub has qualities that make it desirable when we consider Streaming solutions. Firstly, Cloud Pub/Sub is highly available. Cloud Pub/Sub servers run in most GCP regions around the world, and this allows the service to offer fast global Data Access. Messages are stored in multiple locations for availability and durability. But Cloud Pub/Sub does more to ensure durability in messages. By default, it will save your messages for up to seven days. In the event your systems are down and not able to process them at the time, you may retrieve messages at a later time, and continue with your processing. Finally, Cloud Pub/Sub is highly scalable. One of the use cases for Pub/Sub early on at Google was to be able to distribute the search engine and index around the world, processing internally about 100 million messages per second across the entire infrastructure. Currently, Google indexes the web anywhere from every two weeks, which is the slowest, to more than once per hour, for example, on really popular new sites. So on average, Google is indexing the web up to three times a day. Pub/Sub being a serverless service provides necessary infrastructure resources automatically to provide this kind of scalability. Cloud Pub/Sub is a HIPAA compliant service, offering fine grained access controls, and end to end encryption. Messages are encrypted in transit and at rest. These features make Cloud Pub/Sub an ideal solution for ingesting incoming Streaming Data, but we also need resilience. What happens if your systems get overloaded with large volumes of transactions, like on Black Friday? What you really need is some sort of buffer or backlog so that you can FIM messages only as fast as the systems are able to process them. Pub/Sub has this as a built-in capability. This relieves you of having to size the application to handle the highest traffic spike, plus some additional capacity as a safety buffer. This is not only wasteful of resources, which must be retained at top capacity, even when not being used, but it provides a recipe for a distributed denial of service attack, by creating an upper limit at which the application will cease to behave normally, and will exhibit non-deterministic behavior. Instead, you can use Cloud Pub/Sub as an intermediary, receiving and holding data until the application has resources to handle it. Either through processing the backlog of work, or by auto-scaling to meet the demand. As we dive deeper into Pub/Sub in the following slides, this will become more obvious. Now that we've given you a high-level overview, let's see how Pub/Sub works. The model is simple. The story of Cloud Pub/Sub is the story of two data structures, the topic and the subscription. Both the topic and the subscription are abstractions which exists in the Pub/Sub framework, independently of any workers, subscribers, or anything else. The Cloud Pub/Sub Client that creates the topic is called the publisher, and the Cloud Pub/Sub Client that creates the subscription is called the subscriber. Publisher will publish events and messages into a topic to be distributed and used for further processing. To receive messages published to a topic, you must create a subscription to that topic. In this example, the subscription is subscribed to the topic. Only messages published to the topic after the subscription is created are available to subscriber applications. The subscription connects the topic to a subscriber application that receives and processes messages published to the topic. A topic can have multiple subscriptions, but a given subscription belongs to a single topic. It helps to think of it as an enterprise messaging bus. Let's use the following example. Here, you see there is an HR topic that relates to new hire events. For example, a new person joins your company, and this notification should allow other applications that need to be notified about a new user joining to subscribe and get that message. What applications could tell you that a new person joined? Well, one example is the company directory. This is a client of the subscription, also called a subscriber. However, Cloud Pub/Sub is not limited to one subscriber or one subscription. Here, there are multiple subscriptions, and multiple subscribers. Maybe the facilities system needs to know about the new employee for badging, and the accounting and provisioning system needs to know for payroll. Each subscription guarantees delivery of the message to the service. These subscriber clients are decoupled from one another, and isolated from the publisher. In fact, we will see later that the HR system could go offline after it has sent its message to the HR topic, and the message will still be delivered to the subscribers. These examples show one subscription and one subscriber, but you can actually have more subscribers for a single subscription. In this example, the badge activation system requires a human being to activate the badge. There are multiple workers, but not all of them are available all the time. Cloud Pub/Sub makes the message available to all of them, but only one person needs to fetch the message and handle it. This is called a pull subscription. This is also what we call a push subscription, and we'll see more about this later. Now, say a new contractor arrives, instead of entering through the HR system, they go through the vendor office. The same kinds of actions need to occur for this worker. They need to be listed in the company directory. The facilities team needs to assign them a desk. Account provisioning needs to set their corporate identity and their accounts. The badge activation system needs to print and activate their contractor badge. A message can be published by the vendor office to the HR topic. The vendor office and the HR system are entirely decoupled from one another, but can make use of the same company services. The way that Pub/Sub works makes coupling possible. In the next section, you will learn more about the different patterns or flows of Pub/Sub messaging.

## 2. Cloud Pub/Sub Push vs Pull

* From the HR analogy previously, we saw how Pub/Sub works and it's used in decoupling systems. Let's now discuss the technical details associated. We'll start by understanding the distribution of messages and different patterns. Note that the different colors for messages represent different messages. The first pattern is just a basic straight through flow, where one publisher publishes messages into a topic, which then get consumed by the one subscriber through the one subscription. The second pattern is fan in or load balancing. Multiple of publishers can publish the same topic, and multiple subscribers can pull from the same subscription, leveraging parallel processing. What you see here are two different publishers sending three different messages all on the same topic. That means the subscription will get all three messages. The third pattern is fan out, where you have many use cases for the same piece of data, and all data is sent to many different subscribers. As you can see here, we have two subscriptions. So both are going to get the messages, both the red message and the blue message. In the HR example we saw, the same new hire event was used by multiple subscriber applications. Now that you are familiar with publish and subscribe patterns, let's see how subscribers get the messages delivered through subscription. Cloud Pub/Sub subscription allows for both push and pull delivery. In the pull model, your clients are subscribers and will be periodically calling for messages, and Pub/Sub will just be delivering the messages since the last call. In the pull model, you have to acknowledge the message as a separate step. So what you see here is we initially make the call to the subscribers, it pulls the messages, it gets a message back, and then separately it acknowledges that message. The reason for this is because the pull queues are often used to implement some kind of queuing system for work to be done. So you don't want to acknowledge the message until you firmly have the message and have done the processing on it. Otherwise, you might lose the message if the system goes down. Therefore, we generally recommend you wait to acknowledge until after you have received it. In the pull model, the messages are stored for up to seven days. In push delivery, Cloud Pub/Sub initiates requests your subscriber application, to deliver messages. The Cloud Pub/Sub servers sends each message as an HTTPS request to the subscriber application at a preconfigured endpoint. In the push scenario, you just respond with status 200 Okay for the HTTP call, and that tells Pub/Sub the message delivery was successful. Push delivery is ideal when multiple topics must be processed by the same webhook for example. The way the acknowledgments work is to ensure every message gets delivered at least once. What happens is when you acknowledge a message, you acknowledge on a per subscription basis. So if you have two subscriptions, say you have one acknowledged and the other one not acknowledged. Then the one that was acknowledged will continue to get subsequent messages while Pub/Sub will continue to try to redeliver the unacknowledged message up to seven days to the other one until it is acknowledged. Now there is a replay mechanism as well that you can rewind and go back in time and have it replay messages, but in any case, you will always be able to go back seven days. You can also set the acknowledgment deadline and do that on a per subscription basis. So if you know that on average it takes you 15 seconds to process a message in your work queue, then you might set your acknowledgment deadline to 20 seconds, this will ensure it doesn't try to redeliver the messages. Subscribers can work as an individual or as a group. If we have just one subscriber, it is going to get every message delivered through that subscription. However, you can set up worker pools by having multiple subscribers sharing the subscription. In the case of a push subscription, you only have one web endpoint. So you will only have one subscriber typically. But that one subscriber could be a Standard App or a Cloud Run Container Image which autoscales. So it is one web endpoint but it can have autoscale workers behind the scenes, and that's typically a pretty good pattern.

## 3. Publishing with Pub/Sub code

* With the theoretical background covered in the previous section, let's now shift our focus on actual implementation. How are things set up in PubSub? Let's look at a little bit of code now. This example is using the client library for PubSub. If we want to publish the message, we first create the topic. Then, we can publish the topic on the command line. More commonly, it will be done in code. Here, we get a publisher client to first create topic and then publish the message. Notice the b right here in front of my-first message. This is because PubSub just sends raw bytes. This means that you aren't restricted to just texts. You can send other data like images if you wanted to. The limit is 10 megabytes. There are also extra attributes that you can include in messages. In this example, you see author equals dylan. PubSub will keep track of those attributes to allow your downstream systems to get metadata about your messages without having to put it all in the message and pass it out. So instead of serializing and deserializing, it will just keep track of those key value pairs. Some of them have special meaning. We will see some of those meanings shortly. Now, once messages start flowing in, you'll want to retrieve them. For that, you will need to first create a subscription. In the next few slides, we will see how to set up a poll subscription. Now, there are two variants. You can set up either an asynchronous pull or a synchronous pull. Using asynchronous pulling provides higher throughput in your application by not requiring your application to block for new messages. Messages can be received in your application using a long-running message listener and acknowledge one message at a time as shown in the example here. Let's take a look at the code for setting up asynchronous pull. We'll start by selecting the topic. Next, we'll name the subscription. Finally, we will define a call back. Sometimes though, you may want to use synchronous pulling. For example, when you need a precise cap on the number of messages retrieved by client at any given time. Let's see how we set up synchronous pulling. You can pull messages from the command line. You'll get to try this in the lab later. By default, it will just churn one message, the latest message. But there is a limit option you can set. Maybe you want 10 messages at a time. You can try that later on in the lab. The example also shows how to do this in Python code. Here, the subscriber pulls a specific number of messages which is two in this example. In general, asynchronous pull is preferable for latency sensitive applications. Can we control the latency on the publish side of things? By default, the publisher will batch publish messages. This prevents the overhead of the call for individual messages on the publisher side. This allows the publisher to wait and send 10 or 50 at a time. This increases efficiency. However, if you are waiting for 50 messages, this means that the first one now has some latency associated with it. So it's a trade-off in your system. What do you want to optimize? In any case, even if you batch publish, they'll still get delivered one at a time to your subscribers. We will practice this technique later on in the lab. Here's how you would control the batch publish setting in Python code.

## 4. Summary

* When working with Pub/Sub there are few things to keep in mind. First, data may be delivered out of order, but that doesn't mean you have to process the data that way. You could write an application that handles out of order and replicated messages. This is different from a true queuing system. In general, they will be delivered in order but you can't rely on that with Pub/Sub. This is one of the compromises made for scalability especially since it's a global service. We have a mesh network, so a message might take another route, and if it happens to be a slower route, you could have an earlier message arriving later. For example, you wouldn't use this to implement a chat application because it would be awkward when messages arrive out of order. Therefore, we will handle ordering using other techniques. Finally, we need to be ready for duplication. Now you can use Dataflow in conjunction with Pub/Sub to solve some of the problems we just discussed. Dataflow will de-duplicate messages based on the message ID because in Pub/Sub, if a message is delivered twice, it will have the same ID in both cases. BigQuery can also be used for this purpose but has limited capabilities. Data flow will not be able to order in the sense of providing exact sequential order of when messages were published. However, it will help us deal with late data. Using Pub/Sub and Dataflow together allows us to get a scale at that wouldn't be possible otherwise. In the next section, we will look at Dataflow's streaming capabilities in greater detail.

## QuizNotes

* Which of the following about Cloud Pub/Sub is NOT true?
	* Pub/Sub stores your messages indefinitely until you request it
* Cloud Pub/Sub guarantees that messages delivered are in the order they were received
	* False
* Which of the following about Cloud Pub/Sub topics and subscriptions are true?
	* 1 or more publisher(s) can write to the same topic
	* 1 or more subscriber(s) can request from the same subscription
* Which of the following delivery methods is ideal for subscribers needing close to real time performance?
	* Push delivery