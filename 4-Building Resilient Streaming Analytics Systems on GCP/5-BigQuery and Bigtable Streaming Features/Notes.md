## 1. Streaming into BigQuery and Visualizing Results

* In this module, you will learn about Streaming Analysis and the throughput constraints associated. These analysis systems are majorly responsible for analyzing data in real time and help getting timely business decisions done. We're going to talk about BigQuery and Bigtable in this module which helps us in making timely decisions. So let's get started. So we looked at BigQuery previously and we know that it can do streaming and we looked at Dataflow SQL. Now what we are looking at is different. This is actually streaming data into BigQuery in real-time using BigQuery API. This will enable us to query tables at the same time we are streaming data into those tables. Then, we can see the latest arriving data in seconds. You can do this at a very high rate but there is [inaudible] on this slide. BigQuery streaming gives you something called insert ID. Streaming is not a load job, there is a separate BigQuery method called streaming inserts. It allows you to insert one item at a time into the table. We first create a table in which we wish to store the streaming data. The tables can be created by using template tables or we can provide schema specifically. The data enters the system and gets buffered in streaming buffers for a brief time until it can be inserted into the table. Usually, the data is made available in seconds. You may be wondering what is the rate for streaming inserts. Well, as you can see on the screen, 100k rows per second per table is the rate for inserts. Since streaming data is unbounded, you need to consider the streaming quotas. There is both a daily limit and a concurrent rate limit. Please check the pricing documentation for BigQuery streaming. When should you ingest a stream of data rather than use a batch approach to load data? When the immediate availability of data is a solution requirement, you can do that. One major reason in most cases, loading batch data is not charged, Streaming Data is charged, so use batch loading or repeated batch loading rather than streaming unless there is a requirement of the application. Well, that is really good explanation about how streaming inserts work. So if you visualize the code for the same, how will it look like? Here is an example of the code used to insert streaming data into a BigQuery table. So now you know how to stream data into BigQuery. The next steps are to run SQL queries to analyze this data which you have already done so in Course 1. Query results are typically helpful but if we could visualize them, that would be awesome. One such feature for visual exploration is trending now and it's part of BigQuery. Let's see how can we use that. Now, after you execute a query in BigQuery, you can use the exploring Data Studio feature to immediately start creating visuals as a part of a dashboard. Here is an example of an actual Data Studio report. You don't have to understand the numbers but just on the surface, what is the report trying to do? In this case, it was the ad performance for the 2016 Olympic Games broadcasted on NBC. The role of your report should be to highlight key insights that your audience cares about. Do not show them everything, focus and filter the attention to just what data they need to see. Telling a good story with your data on a dashboard is critical because your users likely aren't going to care about a missing pipeline that you just built if the output is a dashboard that is hard to use. Here is a quick walkthrough of Data studio UI. Keep in mind that team is continually updating the product. So refer to their documentation and examples for additional practice. The homepage shows the dashboards and the data sources you have access to. It is an important distinction. Connected data sources can feed into dashboards, but just because someone has access to your dashboard doesn't mean they have permission to view the data presented because that could be controlled in BigQuery or your GCP project. Anyway, there are two ways to create a new report from scratch. First is from the template panels on the top, and the second is from the button in the lower right. The first thing you need to do it's tell Data Studio where your data is coming from, that is known as the Data Source. A Data Studio report can have any number of data sources but we will just start with one. That Data Source picker shows all the data sources you have access to. Note that you can have any or all of these data sources in a single Data Studio report. Since Data Studio reports and datasets can be shared, you should be aware of the ramifications of adding a data source. When you add a data source to a report, other people who can view the report can potentially see all the data in that Data Source if you share that source with them. Anyone who can edit the report can use all the fields from any added data sources to create new charts with them. You can click "Add to Report" to connect the data source and then you're ready to start visualizing.

## 2. High-Throughput Streaming with Cloud Bigtable

* So far, we looked at how to do queries on data even if it's streaming in using BigQuery, and displaying the data using Data Studio. BigQuery is a very good general purpose solution, something that would work in most cases that you are worried about. But every once in a while, you will come across a situation where the latency of BigQuery is going to be problematic. In BigQuery, the data that's streaming in is available in a matter of seconds, and sometimes you will want lower latency than that. You will want your information to be available in a matter of milliseconds, for example, or microseconds. You may also have to run into issues where the throughput of BigQuery may not be enough and you may want to deal with a higher throughput. So what we will be looking at this in this final chapter is how to handle such throughput or latency requirements when BigQueries is not enough. Where do you go? So we will talk about Bigtable. We will look at how to design for Bigtable , specifically how to design schemas, how to design the row key from Bigtable. We will look at how to ingest data into Bigtable. Bigtable is for high-performance applications. To use Cloud Bigtable effectively, you have to know a lot about your data and how it will be queried upfront. A lot of optimizations happen before you load data into Cloud Bigtable. Cloud Bigtable is ideal for applications that need very high throughput and scalability for non-structured key-value data, where each value is typically no larger than 10 megabytes. Not good for highly structured data, transaction data, small data, less than one terabytes and anything requiring SQL queries and SQL-like joins. Here are few examples of data engineering requirements that have been solved using Cloud Bigtable. Machine learning algorithms frequently have many or all of these requirements. Applications that uses marketing data, such as purchase histories or customer preferences. Applications that use financial data, such as transaction histories, stock prices, or currency exchange rates. Internet of Things, IoT data, such as usage reports from meters, sensors, or devices. Time series data, such as resource consumption like CPU and memory usage over a period of time for multiple servers. The most common use of Cloud Bigtable is to productionize a real-time look as part of an application, where speed and efficiency are designed beyond that of other databases. The Cloud Bigtable use case was originally for a keyword search. Today, it is a solution for things like a transit system or an employ badge with a unique ID that uses a lookup table to find that data and triggers an event. Cloud Bigtable stores data in a file system called colossus. Colossus also contains data structures like tablets that are used to identify and manage the data, and metadata about the tablets is what it is stored on the VMs in Bigtable cluster itself. This design provides amazing qualities to Cloud Bigtable. It has three level of operation. It can manipulate the actual data, it can manipulate the tablets that point to and describe the data, or it can manipulate the metadata that points to that tablets. Rebalancing tablets from one node to another is very fast, because all the pointers are updated. Cloud Bigtable is a learning system. It detects hotspots, where a lot of activity is going through a single tablet and splits the table into two. It can also rebalance the processing by moving the pointer to a tablet to a different VM in the cluster. So it's best used cases with Big Data above 300 gigs and very fast access, but constant use over a longer period of time. This gives Cloud Bigtable a chance to learn about the traffic pattern and rebalance the tablets and the processing. We know that Bigtable is a cluster-based implementation, that means there are multiple machines which host Bigtable service and offer processing capabilities. What if one of the machine or node fails in this cluster? Will data lost in this case? What will happen to processing? Answer to all these question is, Bigtable is a managed service and is fault tolerant by default. In case of a node's failure, only pointers to the actual data will be lost. Actual data will be available in colossus. So a new replacement node will come up and it will be provided with a fresh copy of pointers or metadata which was handled by faulty node. Colossus maintains three replicas by default to provide durability. With that brief overview, let us dive deeper and see how is data stored in Bigtable. Cloud Bigtable stores data in tables. To begin with, it is just a table with rows and columns. However, unlike other table-based data systems like spreadsheets or SQL databases, Cloud Bigtable has only one index, that index is called as row key. There are no alternate indexes or secondary indexes, and when data is entered, it is organized lexicographically by the row key. The design principle of Cloud Bigtable is speed through simplification. If you take a traditional table and simplify the controls and operations, you allow yourself to perform on it. Then you can optimize for those specific tasks. Simplify the operations and when you do not have to account for variations, you can make those that remain very fast. In Cloud Bigtable, the first thing we must abandon in our design is SQL, that is a standard of all the operations are database can perform. To speed things up, we will drop most of them and build up from a minimal set of operations. That is why Cloud Bigtable is called a NoSQL database. But all the data and not all the queries are good use cases for the efficiency that the Cloud Bigtable service offers. But when it is a good match, Cloud Bigtable is so consistently fast that it is magical. The green items are the results you want to produce from the query. In the best case, you are going to scan the row key one time, from top to bottom, and you will find all the data you want to retrieve in adjacent and contiguous rows. You might have to skip some rules, but the query takes a single scan through the index from top down to collect the result set. The second instance is sorting. You are still only looking at the Row Key. In this case, the yellow line contains data that you want, but it is out of order. You can collect the data in a single scan, but the solution set will be disorderly. So you have to take the extra step of sorting the intermediate results to get the final results. Now, think about this. What about the additional sorting operation to do the timing? It introduces a couple of variables. If the solution set is only a few rows, then the sorting operation will be quick. But if the solution set is huge, the sorting will take more time. The size of the solution set becomes a factor in timing. The orderliness of the original data is another factor. If most of the rows already in order, there will be less manipulation required than if there are many rows out of order. The orderliness of the original data becomes a factor in timing. So introducing sorting means that the time it takes to produce the result is much more variable than scanning. The third instance is searching. In this case, one of the columns contains critical data. You cannot tell whether a row is a member of a solution set or not, without examining the data contained in the critical column. The Row Key is no longer sufficient. So now, you are bouncing back and forth between Row Key and column contents. There are many approaches to searching. You can divide it up into multiple steps. Once scan through the Row Keys, and subsequent scans through the columns, and then perhaps, a final sought to get the data in the order you want. It gets more complicated if the conditions of solution, set membership involve a logic. Such as, a value in one column and a value in another column, or a value in one column or a value in another column. However, any algorithm or strategy you use to produce the result is going to be slower and more variable than scanning or sorting. What is the lesson from this exploration? That to get the best performance with the design of the Cloud Bigtable service, you need to get your data in order first, if possible, and you need to select or construct a Row Key that minimizes sorting, and searching, and turns your most common queries into scans. Let us take an example and understand the Bigtable design practices implementation tips. So we are considering the flights data for all the airlines, and all flight routes operated on. Each entry records the occurrence of one flight. The data includes city of origin, and the date and time of departure, and destination city, and date and time of arrival. Each airplane has a maximum capacity, and related to this is the number of passengers that were actually aboard each flight. Finally, there is information about the aircraft itself, including the manufacturer, called the make, the model number, and the current age of the aircraft at the time of the flight. In this example, the Row Key will be defined for the most common use cases. The query is find all the flights originating from the Atlanta airport, and arriving between March 21st and 29th. The airport where the flight originates is the origin field, and the date when the aircraft landed is listed in the arrival field. If you use the arrival field as the Row Key, it will be easy to pull out all the flights between March 21st and 29th, but the airport of origin won't be recognized. So you will be searching through the arrival column to produce the solution set. In the third example, a Row Key has been constructed from information extracted from the origin field and the arrival field, creating a constructed Row Key. Because that data is organized lexicographically by the Row Key, all the Atlanta flights will appear in a group, and sorted by date of arrival. Using this Row Key, you can generate the solution set with only a scan. In this example, the data was transformed than it arrived. So constructing a Row Key during the transformation process is straightforward. Cloud Bigtable also provide column families. By accessing the column family, you can pull some of the data you need without pulling all of the data from the row, or having to search for it and assembling. This makes access more efficient. For example, you may most of the time only need flight information, and do not really care about aircraft details. Say the most common query is for current arrival delays in Atlanta. That will involve averaging flight delays over the last 30 minutes, hence origin, arrival. We want this at the top of the table, hence RTS. You can reverse timestamps by subtracting the timestamp from your programming languages, maximum value for long integers. Such as, you can use Java's java.lang.long.max value. For example, long max Timestamp milliseconds since Epoch. By reversing the timestamp, you can design a Row Key where the most recent event appears at the start of the table, instead of the end. As a result, you can only get the n most recent events simply by retrieving the first n rows of the table. So far, we have discussed adding and querying the data. Now let's see how Bigtable handles removal. When you delete data, the row is marked for deletion, and skipped during subsequent processing. It is not immediately removed. If you want to make a change to data, the new row is appended sequentially to the end of the table, and the previous version is marked for deletion. So both rows exist for a period of time. Periodically, Cloud Bigtable complex the table, removing rows marked for deletion, and reorganizing the data for read and write efficiency. Distributing the write across nodes provides the best write performance. One way to accomplish this is by choosing rows keys that are randomly distributed. However, choosing a Row Key that groups related rows so they're adjacent makes it much more efficient to read multiple rows at one time. In our airline example, if we were collecting weather data from the airport cities, we might construct a key consisting of a hash of the city name, along with a timestamp. The example Row Key shown would enable pulling all the data for Delhi, India as a contiguous range of rows. Whenever there are rows containing multiple column values that are related, it is a good idea to group them into a column family. Some NoSQL databases suffer performance degradation if there are too many column families. Cloud Bigtable can handle up to 100 column families without losing performance. It is much more efficient to retrieve data from one or more column families than retrieving all of the data in a row. There are currently no configuration settings in Cloud Bigtable for compression. However, random data cannot be compressed as efficiently as organized data. Compression works best if identical values are near each other, either in the same row or in adjoining rows. If you arrange your row keys so that rows with identical data are adjacent, the data can be compressed more efficiently. Cloud Bigtable periodically rewrites your table to remove deleted entries, and to reorganize your data so that reads and writes are more efficient. It tries to distribute reads and writes equally across all Cloud Bigtable nodes. In this example, A, B, C, D, E are not data, but rather pointers or references and cache, which is why re-balancing is not time-consuming. We are just moving pointers. Actual data is in tablets in Colossus file system. Based on the learned access patterns, Bigtable re-balances data accordingly, and balances the workload across the nodes. With a well-designed schema, reads and writes should be distributed fairly evenly across an entire table and cluster. However, in some cases, it is inevitable that certain rows will be accessed more frequently than others. In these cases, Cloud Bigtable will re-distribute tablets, so that reads are spread across nodes in the cluster evenly. Note that ensuring an even distribution of reads has taken priority over evenly distributing storage across the cluster.

## 3. Optimizing Cloud Bigtable Performance

* In the next few slides, we will look at how you can further optimize Bigtable performance. There are several factors that can result in slower performance. That table schema is not designed correctly. It's essential to design a schema that allows reads and writes to be evenly distributed across the Cloud Bigtable cluster. Otherwise, individual nodes can get overloaded, that may results in slowing performance. The workload isn't appropriate for Cloud Bigtable testing, with a small amount of data. For example, less than 300 gigs or for a very short period of time. Seconds rather than minutes or hours. Cloud Bigtable won't be able to properly optimize your data. It needs time to learn your access patterns and it needs large enough shards of data to make use of all of the nodes in your cluster. The Cloud Bigtable cluster doesn't have enough nodes. Typically, performance increases linearly with the number of nodes in a cluster. Adding more nodes can therefore improve performance. Use the monitoring tools to check whether a cluster is overloaded. The Cloud Bigtable cluster was scaled up very recently, while nodes are available in your cluster almost immediately. Cloud Bigtable can take up to 20 minutes under load to optimally distribute cluster workload across new nodes. The Cloud Bigtable cluster uses HDD discs. Using HDD disk instead of SSD disks means slower response time and a significantly lower cap on the number of read requests handled per second, 500 QPS for HDD disks versus 10,000 QPS for SSD disk. There are issues with the network connection at work. Network issues can reduce throughput and causes reads and writes to take longer than usual. In particular, you will see issues if your clients are not running in the same zone as your Cloud Bigtable cluster, because different workloads can cause performance to vary. You should perform test with your own workloads to obtain the most accurate benchmarks. This is an example of some of the numbers that are possible in terms of throughput. With 100 nodes, you can handle 1 million queries per second. Throughput scales linearly welled up into the hundreds of nodes. A high-throughput means, more items are processed in a given amount of time. If you have larger rows, then fewer of them will be processed in the same amount of time. In general, smaller rows offers higher throughput and therefore, is better for streaming performance. Cloud Bigtable takes time to process cells within rows. So if there are fewer cells within a row, it would generally provide better performance than most cells. Finally, selecting the right row keys critical. Rows are sorted lexicographically. The goal when optimizing for streaming, is to avoid creating hotspots when writing, which would cause Cloud Bigtable to have a split tablets and adjust loads. To accomplish that, you want the data to be evenly distributed as possible. Reading delays, adding to processing delays leads to response time. Besides high-throughput, you also want to make sure your data is highly available. Replication of Cloud Bigtable enables you to increase the availability and durability of your data by copying it across multiple regions or multiple zones within the same region. You can also isolate workloads by routing different types of requests to different clusters. A simple command like gcloud bigtable clusters creates, lets you create a cluster of Bigtable replicas. If a Cloud Bigtable cluster becomes unresponsive, replication makes it possible for incoming traffic to fail over to another cluster in the same instance. Failovers can be either manual or automatic depending on the app profile an application is using and how the app profile is configured. The ability to create multiple clusters in an instance is valuable for performance. As one can be for writing and the replica clusters exclusively for reading. Bigtable also supports automatic failover for [inaudible]. The generalizations to isolate the right workload increased number of nodes and decrease row size and the cell size will not apply in all cases. In most circumstances, experimentation is the key to define the best solution. A performance estimate is given in the documentation online for write-only workloads. Of course, the purpose of writing data is to eventually read it. So the baseline is an ideal case. At the time of this writing, a 10-node SSD cluster with one kb rows and a write-only workload can process 10,000 rows per second at a six millisecond delays. In this estimate, will be affected by average row size, the balance and the timing of read discrete from writes and other factors. You will want to run performance tests with your actual data and application code. You need to run the test on an at least 300 gigs of data, to get valid results. Also, to get valid results, your tests need to perform enough actions over a long enough period of time to give Cloud Bigtable, the time and conditions necessary to learn usage pattern and perform it's internal optimizations. Key Visualizer is a tool that helps you analyze your Cloud Bigtable usage patterns. It generates visual reports for your tables that break down your usage based on the row keys that you access. Key Visualizer automatically generates hourly and daily scans for every table in your instance, that meets at least one of the following criteria. During the previous 24 hours, the table contain at least 30 gigs of data at some point of time. During the previous 24 hours, the average of all the reads or all the writes was at least 10,000 rows per second. The core of a Key Visualizer scan is a heat map, which shows the value of a metric over time, broken down into contiguous ranges of raw keys. The x-axis of the heat map represents time and the y-axis represents raw keys. If the metric had a low value for a group of raw keys at a point in the time, the metric is cold and it appears in a dark color. A high-value is hot and it appears in a bright color. The highest value appears in white. By Learning such patterns, you may be able to take actions to help keep Bigtable performance optimal.

## QuizNotes

* Which of the following is true for Data Studio ?
	* Data Studio supports data ingest thought multiple connectors.
* Data Studio can issue queries to BigQuery
	* True
* Which of the following are true about Cloud Bigtable?
	* Offers very low-latency in the order of milliseconds
	* Ideal for >1TB data
	* Great for time-series data
* Cloud Bigtable learns access patterns and attempts to distribute reads and storage across nodes evenly
	* True
* Which of the following can help improve performance of Bigtable?
	* Change schema to minimize data skew
	* Clients and Bigtable are in same zone
	* Add more nodes