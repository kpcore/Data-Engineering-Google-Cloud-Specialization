## 1. GIS Functions

* Here's the content that we're going to cover. First, BigQuery natively supports geographic information system functions, or GIS functions and their data types. We'll analyze three key questions that we want to answer and then you use those GIS functions to help us solve and map the results with BigQuer Geo Viz. Next, we'll explore with clauses versus permanent tables for performance and readability. After that is one of the most critical tools in an analysts tool kit, which is the use of analytical window functions, which allow you to break up a given data set and perform SQL only over certain windows of that data. Then, we'll discuss ranking functions and go deep into repeated data with array data types. Lastly, we'll conclude with performance best practices to help govern your data engineering and analysis efforts. I find it's best to start with a robust data set and get inspired to ask difficult questions that we can then build out with advanced SQL functions. Our data set for this module will be over 24 million bike-share trips in downtown London. If you haven't seen these bike docking stations in cities, they're getting pretty popular. Essentially, a person can go to one of 100 bike docks that are in a street corner in the city and rent a bike by the hour. They must bring the bike back to one of the station's to end their trip. The public data set we have consists of two tables, the actual millions of trips the bikes have taken, and the locations of the docking stations around London. Let's first start by asking some fun, thought-provoking questions. Here are three questions that I came up with that I want to know. First, I want to know which pairs of stations rented from and then dropped off at has the fastest riders in kilometers per hour on average. I've some assumptions about people that are late for work that live far away that I'm curious to see if the data supports. Second, if I were running this business, I would want to know which stations have bikes that are always in use. Docked out, docked in, in and out quickly. Versus those bike docks that are getting dusty with unused bikes. That can help the business plan for capacity, and maybe even move bikes midday from those slow stations to really hotly demanded ones. Lastly, I want to manage the overall health of our bike fleet. These are assets. If we don't want to open ourselves up to liability by having our bikes breakdown on a midday ride in busy London, we want to make sure that our bikes get the maintenance that they need, and we prioritize those bikes that need the maintenance first. Sound like fun? Let's tackle that first problem first. Finding the average pace, the fastest one, between a pair of docking stations. We'll make the assumption here, since we don't have GPS data in the ride itself, that the clock begins when the bike is rented from one station and ends at the final station. And for ease, we'll use a straight line distance in kilometers between the stations themselves for distance. It's time to explore how we can make this insight a reality with the data that we have. As I mentioned, we have those two tables, the individual bike rides that are in the cycling higher table, which is shown here. Take a look at the columns, and knowing what we want is the average speed, which is that distance traveled over that time duration, what column or columns would be useful? We could do a timestamp difference between start and end date to get the duration. But lucky for us, we already have a duration field. So let's use that d uration field as the total time the bike was out during that trip. What does 3,180 mean for duration? Well, if you were to click over to the schema tab, you can see that in the description, this is the duration in seconds for the trip. This is why it's critical when you're creating your own data sets to add in column descriptions. So duration is done. What about distance? How can we calculate the distance in kilometers between the starting and ending station? We still need to know how far the bike physically traveled to get to the pace of the bike. The cycles higher table only has those names of the starting and ending stations. Where else might we look? The cycle stations table has not only the name of the station but the actual physical location in latitude and longitude values. Since BigQuery natively supports GIS functions, we can use the function ST_DISTANCE, which will give us the straight line distance between two geographic points. By the way, you'll see a lot of these GIS functions that begin with the letters ST. It simply means spatial type, and BigQuery has a special geographic data type as a backend for all these geographic data, and it's optimized for GIS work like this. More on GIS later. Before we begin writing any SQL, let's first remind ourselves of the overall insight, see what data we have, and then mentally prepare for the query. Our goal is to find a pair of bike stations with the highest average rider pace in kilometers per hour. We analyze the schema, and found duration in seconds, and a lat/long of the starting and ending stations, which will give us that distance. Now, duration and distance will be in two separate tables. So the first part of our query will need to be a join. Then, since the lat and longs are not of a geographic data type by nature, we'll need to convert them into formal geographic points before we can use the power of GIS functions on them. Then, we use ST_DISTANCE, which will give us that straight line distance between the stations in meters. Next, we want the average for all riders on that route. So we'll then use a SQL aggregation function. Lastly, we'll apply any filter that we want to remove any anomalies from the data. This is the first part of our query, to bring the data from two different tables together, bikes and bike stations. In complex queries where I know that I'll likely share my results with others in the form of a table, I often use the STRUCT function to clearly outline what fields came from what tables. It may be overkill for small examples like this, but for super wide schemas, you're talking like 30 plus columns, you're almost guaranteed to see STRUCTs in use. So why not get familiar now? The schema behind Google analytics is a great example of this super wide schema. So by the way, looking at the query, why do we have three tables in the join conditions? Well, each bike starts and stops at a station, so we need to join the station's table twice, because the bikes table has those two column IDs to join on. This is the result from running that previous query. You can see how putting the columns for each table in a STRUCT gives us a nice prefix in the column name. So it's super clear that starting dot longitude is different from ending dot longitude. This process of joining together many tables into a single table is called denormalization. It's often a great way to store data for fast retrieval from BigQuery without having to do those joins each time. It's also highly performant because BigQuery stores data in a columnar format instead of a record format, which means you're not punished for having super wide schemas. Now that you're familiar with STRUCTs, it's time to do the actual GIS functions to convert our lat longs into points, then take that distance between the starting and ending stations. The inner function there, that's the ST geography point, accepts a longitude and latitude float value and converts it into an actual GIS point. We do that twice, and then pass those into the ST_DISTANCE function for the final distance calculation, where we got 661 meters apart from those two stations that you see there. Lastly, we'll probably want to draw these distance lines on a map somewhere so we can use the example ST_MAKELINE to do so in a demo. Now that we're done pre-processing all of our data, it's time to do the actual SQL to calculate the insight that we want. We pull the station names, round the trip distance, convert it from meters to kilometers, count all those trips from that station, and filter by only stations that have only 100 rentals or more with a HAVING clause. Then, we actually do the speed calculation by dividing the distance and duration and taking the average. Lastly, for the insight to make any sense, we'll sort from the highest average speed first in our ORDER BY clause. You'll see that the data is pulled from a staging place noted there in the fROM clause. That is actually a named sub-query, or a WITH clause that we'll cover later. It's essentially the query that you saw before to pre-process the data, which we named temporarily as staging so we can call it later. And here's the actual output of that query. You can see that if you're going from Finlay Street to King Edward Street, you're likely to encounter the fastest commuters on average, with about a 16.6 kilometer per hour average pace, which is about 10 miles an hour. On commuter bike, that's pretty good. What does it say about these stations? Well, maybe the goal of these riders is just get from point A to point B, and no time spent enjoying the sights of London that'll eat up your average pace. Without having GPS data mid-ride on these bikes, we can only speculate at what the actual routes that each rider took was. Well, wouldn't it be easier to see these trips as actual lines on a map of London? With BigQuery Geo Vis, you can do exactly that. Geo Vis is a web-based front-end that you can run BigQuery queries on and visualize the geographic data points. You can even style those points lines or even two polygons with different weights and colors based on your actual data values. Let's do a quick demo of the tool using the results of the fastest commuters query that we just made.

## 2. WITH Clauses vs Permanent Tables

* In our previous example, we discussed how you can analyze a schema and plan out your query in steps. Using a WITH clause is a great way to help break apart a complex query. But you can also create a series of new tables if you wanted to. Let's briefly discuss the pros and cons of each approach. Here is the query that you saw before which pre-process the data, the cycle hire, and cycle stations tables. To call it later in the query like you would call it table, we simply wrap the entire query with a WITH clause, a name, and then some parentheses. WITH clause queries are essentially named sub-queries, which means you could paste the entire query inside of this into a FROM clause in another query, but it'd be super ugly and really hard to read. That's why I recommend naming these queries at the top of your script and then simply calling them later. I commonly edit and add new columns and calculations in my WITH queries which is why I refer to them as a staging area for later. You can have more than one named sub-query by simply chaining them together with a comma. One thing you may ask is why keep the pre-process query in the WITH clause and not stored in a new permanent table? Good question. You absolutely could store the results of the pre-processing query in a table. They'll be much faster to query later, because the pre-processing and joins are done, and you can potentially share that table with others. The cons are that if users only ever needed a subset of that data, like, they're continuously adding WHERE clauses, it may be much faster just to use the WITH clause. Why? Well, because even if your WITH clause query doesn't have a WHERE filter, that a user would specify later to filter their data, BigQuery is smart enough to do what's called predicate push-down and do that filter first as part of the WITH statement even if you didn't specify it. It's like magic. Lastly, I mentioned I'm continuously tweaking the pre-processing steps on that data and I'd like to know what I did on those columns. If you created a permanent table, you'd constantly be dropping and recreating it. There's no clear right or wrong here. Just use your judgment when creating your queries. Generally, I'll start with a lot of WITH clauses and then promote them over time to permanent tables as each use case demands. For this exercise, look at the next query and practice reading the query. It points out the parts in the query that are pulled from WITH clauses or named sub-queries. Don't worry about the lag and partition By Functions as we'll cover those next.

## 3. Analytical window functions

* Next is a critical topic for any Data Engineer and analysts to understand. Which is how to break apart a single data set, into groups or Windows to run SQL over just pieces of it. Let's go back to the questions that we want to answer. For this next inside, we want find this stations with the fastest, and the slowest bike turnover times. As we did before, let's map out our analysis. It's the same dataset as before, but this time we want the timestamp of when each byte came in and out of the docking station. We ultimately want to find out how long that byte sat there after its previous trip ended, before it was checked out again. On the query side, same as before will join the datasets together. Next though, we need to find a way to look backwards and see the previous time a bike_id was returned to that station. Then we simply take the timestamp difference for the idle time to that bike_id at that station. Lastly, we'll get together some aggregate metrics for that station as our final report, and filter the data as needed. We can use a lag function which is one of those navigation functions in SQL. So go back one record in the dataset, and pull the previous value for bike.end_date. In order for the lag to make any sense, we need to use an analytical window function to logically partition our data before doing the lag. Here is a visual of the results. So you can see what the actual lag is doing. The last end date is taken from the previous end date from the record before. By default, lag just looks back one record, but you can look deeper if you want to offset more. The critical part here is the window function that we put with the lag. The over partitioned by bike_id, order by bike.start_date, ensures that we're only taking the lag within all this same bike IDs that is what that partition does, like the bike ID you see here 9,842, and then that sort order of the window of the data is correct, oldest to newest. That allows us to transverse the past rentals accurately. Lastly, this is simple timestamp diff between the current rental start time, and the last rentals end time, which gives us the time in hours that the bike was idle in that dock. Nobody was using it. We can then aggregate with an average and show that result for each station name. Here you can see the hottest station is warmwood. With only a 1.15 hour downtime on average in the station with the bikes just sitting there is a new station that was opened in 2017. Here East South in Queen Elizabeth Olympic Park with over a 116 hours idle on average for those bikes. One thing you could try, is filtering the dataset for only daytime hours or nighttime hours, to see how time of day affects each station. Last up in our discussion of advanced functions are ranking functions as well as the array data type. Our last insight will be the rank which bikes need maintenance based on the most usage metrics that we're going to create. So let's collect some bike stats. For the first part of the query, let's get granular information on each trip taken by each bike by given a rank. Give a one to the first trip ever took, and a two for the second trip etc. Then lets sum common metrics for duration and distance. Finally we will store all these metrics, new convenient structs that we call stats. These are the results from that last query. Here's a challenge for you. What if we wanted to rank each bike against all the other bikes, and we also wanted granular details for every trip that each bike took? How could you write a query. They will return this result here. It's one record for bike 12757 which is ranked number one in distance traveled over 5,000 kilometers. But we also want a repeated value for every trip that that bike took. If you wanted to drill down into that data, the secret to that is the array data type.

## 4. Performance Considerations

* We'll close this module with a recap on BigQuery performance and pricing topics. There are five key areas for performance optimization in BigQuery and they are: Input output, how many bytes were read from disk? Shuffling, how many bytes were passed to the next query processing stage? Grouping, how many bytes were passed through to each group? Materialization, how many bytes are written permanently out to disk? Lastly, Functions in UDFs, how computationally expensive is the query on your CPU? Here's a cheat sheet of best practices that you should follow. Starting from the top. Don't select more data columns then you need, that means avoid Select * at all costs when you can. If you have a super huge data set, consider using approximate aggregation functions instead of regular ones. Next, make liberal use of the WHERE Clause at all times to filter data. Then, don't have an ORDER BY on a width clause or sub-queries or any other sub-queries that you have, only order is the last thing that you do. For Joins, put the larger table on the left if you can, that'll help BigQuery optimize it and how it does its joins. If you forget, BigQuery will likely do those optimizations for you so you might not even see any difference. You can use wildcards in table suffixes to query multiple tables, but try to be as specific as possible as you can with those wildcards. For your GROUP BYs, if you're grouping by the names of every Wikipedia author ever, which means high distinct values or high cardinality, that's a bad practice or an anti-pattern. Stick to a low unique value group bys. Lastly, use partition tables when you can. You'll practice creating these in a lab later. There are two major ways to partition tables in BigQuery. When you first ingest the data as a destination table or as an actual column that you can partition on. The first example shows how you can create a table with ingestion time partitioning by day, and the other two examples, try to promote an already existing column that you have in a data set to be a partition column. The columns must be of type dates, timestamp or newly added as an integer range column. Partitioning can improve query cost and performance by reducing the amount of data that's scanned. Since BigQuery has access to the partitioning metadata, it can avoid partitions of data that it knows will be of zero value to the query asked, like ignoring all data before a certain date filter, as you can see here.

## QuizNotes

* Which of the following practices help optimize BigQuery queries?
	* Put the largest table on the left
	* Filter early and often
	* Use COUNT(DISTINCT) instead of APPROX_COUNT_DISTINCT
	* Avoid using unnecessary columns