## 1. Introduction to BigQuery

* we'll explore in great detail, some of the cool things that you can do with SQL inside of BigQuery. Now I know what some of you may be thinking. SQL, isn't that just used for selecting and returning records for my database? Well, as the title shown here as already probably spoiled it, you can now build machine learning models with SQL too. It's literally my favorite feature in Google Cloud Platform, so I'll spend lots of time walking you through how you can create and analyze the performance of your ML models right within BigQuery where your data lives. But before we jump headlong into ML, we'll first introduce you to BigQuery as a service which allows you to have a petabyte-scale, analytics data warehouse. You'll soon learn that BigQuery is actually two services in one, a fast SQL Query Engine and fully managed data storage. Then, we'll show you some of the other cool built-in features, like using GIS functions on your Geographic data, and how you can even visualize those insights on maps. Next, we'll expand on our course theme of applying MLT or datasets, by looking at how to choose the right model type for your structured data. Lastly, we'll build a custom model using just SQL with BigQuery ML. I'll also show you how I commonly organize my ML projects and some advanced BigQuery ML features that let you do things like see which fields the model thinks are most important in making those predictions. Sound good? Let's get started. BigQuery is designed to be an easy-to-use data warehouse. We can focus on writing SQL statements on small or large datasets without worrying about infrastructure. If you've never written SQL before, I'll also provide resources and labs to get you up to speed as we go. So that's point Number 1, it's serverless. BigQuery's default pricing model is pay as you go. Where you pay for the number of bytes of data that your query processes and any other permanent data that's stored inside of BigQuery. Now there is some magic built-in like automatic caching of query results, so you don't end up paying for the same query returning the same data twice, which is cool. If you want to have a set bill every month instead, you can subscribe to flat tier pricing, where you get a special reserved amount of resources for your dedicated use. Data in BigQuery is encrypted at rest by default. You can also specify the geographic locality of your data if you need to meet things like regulatory requirements. Controlling access to your data can be as granular as specific columns, say any column tag with PII, Personally Identifiable Information or specific rows, like if your marketing team only needs access to see certain rows in one of your tables. BigQuery works in tandem with Cloud IAM to set these roles and permissions at a project level, and then inherited down to the BigQuery level. We'll discuss data access in detail a bit more later. SQL as a language has been around since the 1970s, and just watching the functionality added to the language over time has been awe-inspiring. You can now perform those GIS functions like distances from the lat long points and much more. You data itself, think of your datasets. It most likely has some kind of geographic component like city, state, zip code, latitude, longitude. So now it's high time to unlock those additional insights. Lastly, BigQuery as both a data warehouse and an Advanced Query Engine is foundational for your AI and ML workloads. It's common for data analysts, engineers, and data scientists to use BigQuery to store, transform, and then feed those large datasets directly into your ML models. This is a huge leap over training ML models on just a few small samples of your data locally on your laptop or desktop. You can now train on all the data that you have available. That's big news for ML. That's the elastic data warehouse nature of BigQuery for ML datasets. Beyond that, as you've seen in the demo, you can now write ML models directly in BigQuery using SQL. This is a great start for modeled prototyping, as you can quickly engineer what features to use right where your data lives. So what does a typical data warehouse solution architecture look like? Take a look at the green box. BigQuery is the analytics engine that sits at the end of a data pipeline like Cloud Dataflow. It stores all the incoming data from the left and allows you to do your analysis and your model-building. On the far right, you can see the myriad of visualization another analytical tools that you can connect through BigQuery as a backend data source. For ML engineers, once your dataset is in BigQuery, you can easily call it from your IPython ML notebooks in the cloud with just a few commands. If you're a business intelligence analyst, you can connect out to visualization tools like: Data Studio, Tableau, Looker, QlikView, and more. Lastly, and worth mentioning here, if you have a team of analysts who prefers to work in spreadsheets, you can now query your smaller huge BigQuery datasets directly, all of that data directly from Google Sheets and perform common operations like PivotTables, and more on the entirety of your dataset. So no more limitations of rows inside of sheets. So new feature, I'll send a link so you can check it out. But the key takeaway is that BigQuery is a common sink or staging area for your data analytics workloads. Once your data is there, your data analysts, business intelligence developers, and ML engineers can then be granted access to your data to start creating their very own insights.

## 2. BigQuery: Fast SQL Engine

* It's time to get a little more technical. So how does BigQuery actually work? Well it's actually two services in one as we hinted at earlier. It's both a Fast SQL Query Engine, and also a fully managed storage layer for loading and storing your datasets. Keep in mind that it's a serverless service, meaning that's fully managed. Let's take a look at what Google Cloud is managing for you as part of the BigQuery service. The two services are connected by Google's high-speed internal network. Recall that this super-fast network enables us to separate, compute and storage. So what does each service do? The short answer is that, the services are fully managed, so users like you and me don't have to worry about how BigQuery stores data on disk or how it autoscales machines for large queries. That said, it's a great learning exercise to understand how the services performed their magic, so you can better understand what's happening. First step, is the BigQuery storage service. The storage service automatically manages the data that you ingest into the platform. Data is contained within a project, in what are called datasets, which could have zero to many tables or views. The tables are stored as highly-compressed columns, each column of that table highly compressed and Google's internal Colossus file system which provides durability and global availability. This is the same data backend that powers some of Google's most popular applications like Google Photos and Gmail. All the data stored here is only accessible to you and your project team as governed by your access policy and we'll get into that a little bit more later. The storage service can do both bulk data ingestion and streaming data ingestion. So it'll work with huge amounts of data and also real-time data streams. The Query service runs interactive or batch queries that are submitted through the console, the BigQuery web UI, the BQ command line tool, or via the REST API. The REST API supported for many common programming languages. There are BigQuery connectors to other services such as Cloud Dataproc and Cloud Dataflow, which simplify creating those complex workflows between BigQuery and other GCP data processing services. The Query service runs interactive or batch queries that are submitted through the console, the BigQuery web UI, the BQ command-line tool, or via the rest API. The rest API supported for many common programming languages. There are BigQuery connectors to other services such as the one you've seen previously Cloud Dataproc and Cloud Dataflow, which simplify creating complex workflows between BigQuery and other GCP data processing services, that's why it's called the Google Cloud Platform, everything is inter-operable. The Query service can also run query jobs and data contained in other locations. So get this, you can run queries on tables that are a CSV file, for example, that are hosted somewhere else in Cloud Storage. But, before we get too excited about running queries on data that's not in BigQuery, like Google Sheets, you should know that BigQuery is most efficient when it's working off of data contained within its own what's called, Native BigQuery storage. The storage service and the Query service work together to internally organize the data to make your queries and run efficiently on terabytes and petabytes. They even optimize your SQL statements syntax, whenever possible after you hit that Run Query button. One of the most important controls you have over the resource consumption and costs, is controlling the amount of data that your query ultimately processes. In general, you only want to select the columns of data that you actually want to process in return as part of your output. A good rule of thumb is to start broad when you're first exploring the dataset, and then zoom in on just those critical fields and rows that you need. Keep in mind, at the time of this recording, BigQuery gives you one terabyte of query processing of bytes processed at no charge every month, and 10 gigabytes of storage as well to start you off. I'll provide a link to the pricing guide as a reference.

## 3. Data quality

* As we saw in the demo, a lot of the time you aren't really sure of the quality of the data in your dataset, so you can use SQL to explore and filter for anomalies. Here, we filtered for those customer records who did not provide a birthday by setting "member birth year is not null" as a filter, I'll provide a SQL syntax reference so that you can bookmark and refer to it later. Well, I've been working with SQL for many years, I will freely admit that I don't have all of the functions and clauses committed to memory, and I'm constantly Google searching for that SQL syntax too. If you prefer to use a UI to inspect the quality of your datasets, consider using Cloud Dataprep, which is a GCP product offered in partnership with Trifacta. Using Dataprep, you can load in a sample of your BigQuery dataset into an exploration view, which will then provide you with some neat column histogram charts as you see here. Now, this specific dataset in this visual is the location of all the weather stations around the world for this specific dataset. The histograms in the state column on the right, they highlight the data that skewed towards three or so values. Which we can infer means that there are some states that have many, say a lot of weather stations. Maybe large states like California. You can actually hover over the bars to see the actual values, that's your frequency. What I've highlighted here on the horizontal bar graph is how well the values in a column of data mapped to the expected data type for that column. Dataprep knows what the allowable or enumerated values for US states should be, and it is telling us that 19,940 missing or null values for state. Why do you think that could be? Well as you might've guessed, these weather station readings could come in from all over the world, and state is an optional field. In Dataprep, if I wanted to create a pipeline to filter all incoming records to only include weather station ratings where the country field is equal to the United States or the US, you can use the UI to quickly set up those transformation steps in what's called a recipe. After your transformation recipe is complete, when you run a Cloud Dataprep job, it farms out the work to Cloud Dataflow which handles the actual processing of your data pipeline at scale. The main advantage to Cloud Dataprep is for teams want to use a UI for data exploration, and want to spend minimal time coding to build their pipelines. Lastly, with Dataprep you can schedule your pipeline to run at regular preset intervals. But, if you prefer to do all of your SQL and exploration work inside of BigQuery, you can also now use SQL to setup scheduled queries by using the @run_time parameter, or the query scheduler and the BigQuery UI. I'll provide a link that shows you how to set up the scheduled queries in the course resources. Now that you're familiar with some of the great insights that you can glean from your data, it's time to talk data security. So your insights are only shared with those people who should actually have access to see your data. As you see here in this table, BigQuery inherits data security roles that you and your teams set up in Cloud IAM. For example, if you're an overall project viewer in Cloud IAM, you can start BigQuery jobs, but you cannot create new datasets yourself. If you're an editor you can create datasets, and if you are an owner, you can also delete datasets. Keep in mind that default access datasets can be overridden on a per dataset basis. Beyond Cloud IAM, you can also set up very granular controls over your columns and rows of data in BigQuery using the new data catalog service and some of the advanced features in BigQuery, such as authorized views. Data security is more than simply applying the right project permissions in roles during your dataset creation. You should also have a written and circulated Data Access Policy for your organization, and it should specify how and when and why data should be shared, and with whom. One first step to ensure healthy data access controls is to periodically audit the users and groups associated with your Google Cloud Platform project in your individual datasets. Do these users need to really be owners or admins or would a more restrictive roles suffice for their job duties? A second pitfall is working with testing and editing datasets only in production. Much like with code, it's often wise to have a completely separate GCP project or dataset for testing different environments, to prevent unintentional data loss or accidental errors.

## 4. BigQuery managed storage

* In addition to super fast query execution times, BigQuery also manages the storage and the metadata for your data sets. As we mentioned earlier, BigQuery can ingest data sets from a variety of different formats. Once inside BigQuery native storage, your data is then fully managed by the BigQuery team here at Google and it's automatically replicated, backed up, and set up to autoscale for your query needs. You can even recover recently deleted tables within a certain period too. You also have the option of querying external data sources directly as we talked about, which bypasses BigQuery's managed storage. For example, if you have a raw CSV file in Google Cloud Storage or a Google Sheet, you can write a query against it without having to ingest the data into BigQuery first. A common use case for this is having external data source that is relatively small but constantly changes, like say a price list for commodities that another team maintains and continually updates. But there are a few reasons why you maybe should consider not doing this. The first is that the data consistency coming in from that external or federated data source is not guaranteed. If the external data source changes mid-flight, BigQuery doesn't guarantee that those updates were actually captured. If you're concerned about that, consider building a streaming data pipeline into BigQuery with Cloud Dataflow, which will be the topic that we'll cover in the next module. In addition to ingesting data sets as a batch, like uploading a CSV, you can also stream records into BigQuery via the API. Note that there are a few coder restrictions that you should be aware of. The max row size for a streaming insert is one megabyte and the maximum throughput is 100,000 records per second per project. If you need higher throughput, say in the order of millions of records per second, for use cases you can consider it like application logging or real-time events tracking. Consider using Cloud Bigtable as a data sync instead. Before you start streaming thousands of records into BigQuery and the API, consider the other options you could have for your streaming solution. If you have data that needs to be transformed or aggregated mid-flight into table and row format, or joined against other data sources as side inputs midstream, or if you want to take just a window or a segment of that data, you should really consider using Cloud Dataflow for your streaming data pipeline. We'll cover working with that solution in the very next module. Lastly, if you're familiar with database design, you've likely heard of the concept of normalization which is the act of breaking up one huge master table into component child tables. So you're storing one fact in one place and not repeating yourself across records. Take this taxi company, for example. You could track payments, timestamps of events, and geographic lat-longs, and pickups, and drop-offs, all in separate tables as you see here. What would you need to do to bring all these data sources together for reporting? If you said do one massive big SQL join, that's absolutely right. But while breaking apart database tables into silos is a common practice for relational databases, like mySQL or SQL Server. For data warehousing and reporting, let me show you a cool new way to structure your reporting tables. Take a look at the table at the top. What do you notice that you haven't seen before? First, you've just one row of data. But it looks like you have four, what's going on? Well, event.time, that field is actually an array datatype. You can have multiple data values, in this case, timestamp for a single taxi booking in that single row. Likewise, with the corresponding array of status values at that timestamp. So already in one table, you get the high-level, if you wanted of total number of orders, but you could also get the number of completed orders without having to do any of those complex joins. The second insight is a bit more hidden. It's that some of the column names look almost like a family name, event.status, event.time or destination.latitude, destination.longitude. These grouped fields are part of a SQL data type known as a STRUCT. If you were to inspect this schema for this table, you'd notice that for the event field datatype, it's of datatype record which indicates that this is a data type of a STRUCT. You can think of a STRUCT as essentially a collection of other fields, kind of like a table. From a reporting standpoint, you can have many nested STRUCTs within a single table, which is conceptually like having many other tables pre-joined, which would get you excited, into one big table. Pre-joined for you means faster queries for larger data sets and no more of those complex 15 table joins. Now, a huge benefit of doing it this way is that you have a single table which has all the fields in a single place for you to analyze. You don't need to worry about the join keys or differing levels of table granularity anymore.

## 5. Insights from geographic data

* BigQuery natively supports GIS, or Geographic Information System functions. For gleaning insights from your geographic data points, like longitude and latitude. Let's examine how you can put these into practice. In this SQL query that you see here, we are plotting the path of a hurricane using SQL and GIS functions. We first create a geographic point based on lat long data. We also bring in other useful fields, like wind speed, the distance the hurricane is to land or landfall, and the radius of the hurricane. We query all this raw data from the BigQuery public dataset from NOAA on hurricanes, and filter for one hurricane in particular. You can see that in the Where clause, that's hurricane Maria in 2017. Then, we geographically bound the points that we care about with the "GIS within" function to ensure that it'll fit on the map or the area of focus. So we can visualize these points. Lastly, we explore our points using Geo Viz, which is a web tool for visualization of geospacial data in BigQuery, but it uses the Google Maps APIs. Here you can see and track the hurricane making landfall in the US. One of the best ways to get better at data analysis is by practicing in a variety of data sets. The BigQuery Public Datasets program partners with companies and organizations to host their datasets in BigQuery, and then make them available for analysis by the public. Currently, there are well over a 100 different datasets for you to explore, and you can find them all in the BigQuery web UI under Explore Data, which is located right above your own datasets. So let's do a quick activity. Explore a BigQuery public dataset that interests you. Then think of the following questions to answer. What's the name of the dataset? How many records are in some of the tables? Are there any data quality concerns after you previewed that data? After analyzing this schema, what types of insights do you think you could find? Lastly, are there any other datasets that you can potentially join against this one for additional insights? Take a moment to find and explore your dataset, and then we'll bring in the head of Google Cloud BigQuery Public Datasets Program, Shane Glass, and he'll explore one of his favorite BigQuery public datasets in a demo.

## 6. Choosing a ML model type for structured data

* It's time to revisit the exciting topic of machine learning. Previously in this course, your data science team had to build a recommendation model using Spark ML, that you then ran on Cloud Dataproc for scale. Later in this module, you're going to be building custom models yourself with just SQL using BigQuery ML. But before we jump into the code, we need to expand our Machine Learning Foundation and then cover the models and the key terminology that you need to know before we set you off to make predictions. When people think of AI or machine learning, they generally think of the advanced models like you saw earlier on Google Photos, in video stabilization, and the smart reply feature in Gmail. Yes, later on this course you will build image models and unstructured data sets. But did you know that at Google, the majority of the models deployed are models that operate unstructured data. These aren't your 50 plus layer- deep neural networks that play StarCraft or chess. They're built on rows and columns of data, just like you've seen experimented with inside of BigQuery. So if you have a structured data set that you think is a good use case for machine learning, the next step is to find a model type that is appropriate for your use case. Out of all the models out there. What's a good place to start for you to start prototyping? Here's a decision tree - no pun intended - to help guide us. We'll walk through each of the different branches. The first question is, what kind of activity that you're engaging in? Is there a right answer or a ground truth that exists in your historical data that you want to model and predict? If so, you want to start with supervised learning. Alternatively, if you're interested in just ruminating and exploring the data for unknown relationships, you're welcome to try unsupervised learning with maybe a clustering model to start. Unsupervised learning is outside the scope of this course, but I'll link you to a few resources to show how you can do it quickly with a clustering model inside of BigQuery ML. The majority of the problems we're going to tackle here are in these three areas: First, forecasting. That's like predicting the next month's sales figures, the demand for your product. Second, classifying. Like high medium or low risk events or buy or no buy decisions. Third, maybe you recommending something like a product for a given user. An easy way to tell if you're forecasting or classifying, is to look at the type of label or special column, we'll cover that more later, of data that you're predicting. Generally, if it's a numeric datatype like units sold or profits earned, you're doing forecasting. If it's a string value, you're typically doing classification. This row is either in this class or this other class, and if you have more than two classes or buckets like high, low, medium, you're doing what's called multi-class classification. Now, once you have your problem outlined, it's time to go shopping for models which will be the tools to help you achieve your goal. Now there are many different model types for you to choose from for these problems. We're recommending you start with the simpler ones which can still be highly accurate to see if they meet your benchmark. By the way, you're ML benchmark is the performance threshold that you're willing to accept from your model before you even allow it to be near your production data. It's critical that you set your benchmark before you train your model. So you can really be truly objective in your decision making to use the model or not. Now, on to types of models. For forecasting, try a linear regression. For classification, try logistic regression. By the way it's called binary logistic regression. If you have a just two classes or buckets that an observation could fall into or multi-class if it's more than two. For recommendations, try matrix factorization which is a commonly used algorithm for problems involving a matrix of users and items. Like your housing rentals example, and here's the complete picture again. You'll see later with BigQuery ML that you can just specify a model type equal to linear regression for example, and BigQuery handles the rest for you. What didn't you see here that you might have heard of in terms of a model type? There's many different types of models out there that you may not see on this chart. More complex models like deep neural networks, decision trees, random forests are also available for modeling. You'll even build a custom model using neural architecture search to build a deep neural network later on in this course and you'll do so without using any code - that's what Auto ML. It's my overall recommendation that even if you know how to build advanced models, that you start with the simpler ones first. Because they often train faster and they give you an indication of whether not ML is even a viable solution for your problem.

## 7. Predicting customer lifetime value

* Now that you're familiar with the types of models to choose from, we need to feed in high-quality training data for them to learn. That's the "learn" part of Machine Learning. Machine is the tool or algorithm like linear regression. The learning is the insights that the model has into the relationships between the known and unknown data. That's why we call it modeling. You're trying to approximate reality, based on what you do know. The best way I've found to learn the key concepts of Machine Learning on Structured Datasets, is with this quick example. Here, our scenario will be predicting customer lifetime value with a model. Lifetime value or LTV, is a common metric in marketing, which is where we'll estimate how much revenue, or profit we can expect from a customer, given their history or customers with similar patterns. The dataset that we'll use is a Google Analytics E-commerce dataset, on Google's very own online merchandise store that sells swag, like t-shirts and other products that you see listed here. Our goal is to target high-value customers with special promotions and incentives. After exploring the available data, we came up with a few fields that we thought might be useful to the model, and to determining whether or not a customer is high value based on their behavior with our website. These fields include, how many lifetime page views? How many total visits? What's the average time that they spend on our site? What the total revenue that they've brought in is? The count of their previous transactions with us on our site. Now that we have some data, and note we'll need a lot more than just the seven records that you see here. We'll be more like on the order of tens of thousands, then we can get ready to feed it into our model. But before we do, we need to define our data in columns in the language that data scientists and other ML professionals use. Taking the E-commerce example we had in the previous lesson. A record or a row is an instance or an observation. In this picture that you see here, we have eight instances. A label is the correct answer that's known historically. Like how much this particular customer spent, and that's the field that's in the future data is unknown or missing. Hence the model, training and prediction. For example, if we know that a customer who has made transactions in the past spends a lot of time on our website, often turns out to have a high LTV for revenue, we could predict the same for newer customers, who are on that same spending trajectory. Here we're forecasting a number. So do you remember which model type we should use? If you said linear regression as a starting point, it's exactly right. Now notice how I didn't say that, if transactions are greater than 10 for this row, then you can expect the revenue to be greater than 1000. Remember that in ML, we feed in columns of data, and then let the model figure out the relationship to best predict that label. It may even turn out that some of the columns are not useful at all to the model in predicting the outcome. I'll show you how you can find this out later by inspecting what the model actually learned. Labels can also be binary values. Like if they are a high-value customer, or not. That's shown here. Knowing that what you're trying to predict, the class, the number, again, that will greatly influence the type of model that you're going to choose. So we've got our label field there on the right, as high-value customer, or not. So what do we call all these other data columns in our table? Those columns, are called features, or potential features at least. Those are our inputs to the model. Each column of data is like a cooking ingredient that you can use from your kitchen pantry. As my young daughter often finds out, cooking with all of the ingredients can often spoil your dinner. It's the process of feature engineering and you've got to whittle it down. Now that's sifting process where you're going through your data, that's really expect to spend the majority of your time as a data analyst engineer or even a data scientist. It's an art form as much as it is a science. Understanding the quality of the data in each column, and working with other teams to get more features or more history is often the hardest part of any ML project. You can even combine or transform these features in a process called Feature Engineering. It sounds fancy, right? Well, if you've ever created calculated fields in SQL or combining columns together, you've just done some basic feature engineering. Also, BigQuery ML does a lot of hard work for you, like automatic one-hot encoding of categorical variables, and things like splitting your dataset into training and evaluation automatically too. Let's talk now about predicting on future data. Say some new data comes in, that you don't have a label for. You don't know whether or not these new customers will have a high LTV or not. But you do have a rich history of labeled examples for your model that train on. So you can train a model on the known data up top, and then once trained and we're happy with the model's performance, we can then use it to predict on that bottom set of data, that unknown data.

## 8. BigQueryML: Create models with SQL

* It's now time to cover how you can create these models with SQL in BigQuery. We'll review what the project phases you can expect to hit along the way too. If you've worked with ML models before, you know that building and training them can be very very time intensive. For common data scientists, you must first export small amounts of data from your datastore into your IPython Notebook and into data handling frameworks like pandas for Python. If you're building a custom model, you first need to transform preprocess all that data and perform all that feature engineering before you can even feed the model data in. Then, finally, after you've built your model in say, TensorFlow and other similar library, then you train it locally on your laptop or on a VM. Doing that with a small model then requires you to go back and create more new data features and improve performance and you repeat and repeat and repeat. That's hard so you stop after a few iterations. Also, I mentioned IPython notebooks in Python. In the past, if you weren't familiar with these technologies, ML was left to the data scientists of your team and out of your reach. But now, you can do it ML in your structured data sets inside of BigQuery using SQL in just a few minutes. Take a look at how you can perform ML and BigQuery with just a few steps. Step one, you create the model with just a SQL statement. Here, we'll use a bike share dataset. Step number two, write a SQL prediction query and invoke ML.predict. Step number three, profit, that's it. You've got a model and then you can review the results. Well, in reality, there are more than three steps. You have to evaluate the model, but the main point is that, if you know basic SQL, you can now do machine learning which is pretty cool. BigQuery ML was designed with simplicity in mind. To that end, in order to define the ML hyperparameters, which is a fancy way of saying the knobs that are set on the model before the training starts, things like the learning rate or even a split between training and test data which is critical. BigQuery ML automatically does that for you with default values. In addition, with the model options, if you wanted to, you could also set yourself those regularization or different strategies for splitting or testing that data and manually setting the learning rate and other different parameters hyperparameters. So what do you get out of the box? First, BigQuery ML runs on standard SQL, it's inside of BigQuery. So you can use normal SQL syntax like UDFs, user-defined functions, sub-queries and joins across other different tables to create your training datasets to feed into the model. Now, for model types, right now you can either choose for linear regression for forecasting or binary multiclass logistic regression and a team is busy, very busy adding more types as we speak. So I'll provide a link so you can stay on top of that documentation. As part of your model evaluation, you also get access to fields like the ROC curve as well as accuracy, precision, and recall - this is for classification models that you can simply select from where the SQL statement after your model is trained or if you don't want to write any code you can click on the UI click on that trained model, which is now an object inside of your dataset and there's a tab for that evaluation that you can take a look at as well. One of my favorite things is you can also inspect the weights of the model and perform a feature distribution analysis. Now, we're getting into that a lot more later. Next, let's walk through the phases of a typical BigQuery ML project.

## 9. Phases in ML model lifecycle

* Let's walk through the key phases of your ML project in BigQuery. What you'll find is that writing the code to create the actual model, is going to be the really easy part. Getting all the right data into a model is always the hard part. The entire process is going to look like this. First, we need to bring our data into BigQuery, if it isn't there already. Here again, you can enrich your existing data warehouse with other data sources by simply using SQL Joins. If you're already using other data products like AdWords or YouTube, you can look out for those easy connectors to get that data into BigQuery before you go off and build your own pipeline. Next is the feature selection and pre-processing step which is similar to what you've been exploring so far as part of this course. Here is where you put all your good SQL skills to the test in creating a great training set for your model to learn from. Keep in mind again as we mentioned, that BigQuery ML does some of the preprocessing for you like the one-hot encoding of your categorical variables into those numeric values that your model actually is expecting. After that, here's the actual syntax for creating a model inside of BigQuery, and short enough, that I can fit it all inside this one tiny little box of code. You simply say "Create model," give it a name, specify the mandatory option like model type, and pass in your SQL query with your training dataset, then hit "RunQuery" and watch your model run. After your model is trained, you'll see it as a new dataset object in BigQuery which you can actually click on and look at all its cool metadata stats. Or if you want to write the query against it to look at the evaluation, then you use ML.evaluate to evaluate the performance of your train model against the evaluation dataset. Here you can analyze loss metrics like Root Mean Squared Error, MSE, or RMSE for forecasting models, and other metrics like area under the curve, accuracy, precision recall for your classification models like the one that you see here. Once you're happy with your model's performance, you can then predict with it. With this even shorter query. Just write in and invoke that ML.predicts command on your newly trained model, and then you'll get back predictions as well as the model's confidence in those predictions. Now you'll notice a new field in the results when you run this query, where you see your label field with the word "predicted" added to the field name, and that's your model's prediction for that label.

## 10. BigQuery ML: key features walkthrough

* Now that you're familiar with the key phases of your ML project, it's time to walk you through some of my favorite advanced features of BigQuery ML. Recall that you can create a model with just CREATE MODEL. If you do want to override an existing model, you can do CREATE OR REPLACE MODEL. Models, again, take options which you can specify if you want. The most important and the only required one is that model type. Linear regression for forecasting, logistic regression for classification, with many more types coming soon. I'll add a link in the resources, where you can view all of the available model options. You can set things like the model learning rate, for how fast it should learn and even hyperparameters for things like regularization, to prevent overfitting. You can inspect the importance the model placed on each feature by looking at the weights it learned. You do this by using the ML.WEIGHTS in filtering on a given input column. In the output, each feature column will have a weight from -1 to 1. The closer the number is to -1 or 1 means the more useful that field is in the model size to predicting the value for that label. It's by far one of my favorite things to do. To evaluate the models performance, next you could just run simply ML.EVALUATE against a trained model. You'll get different performance metrics as we covered depending upon the type of model that you choose. Also you can look at the model's performance in the UI just by clicking on the model object and your dataset and taking a look at all that available metadata. I do that pretty often because I'm pretty lazy in BigQuery. Making predictions is as simple as calling ML.PREDICT on a trained model and passing through the dataset that you want to predict on. Now, let's do one final big review and what I'll call the "cheatsheet." If you're going to print something out, it's going to be this page. First, in BigQuery ML. You need to have a field in your training dataset titled label. Or, you need to specify which field or fields are your labels by using the input_ label_cols in your model options. Second, your model features are simply the data columns that are part of your SELECT statement after your CREATE MODEL statement. After a model is trained, you can use ML.FEATURE_INFO to get statistics and metrics about that column for additional analysis. Next is the model object itself. You train many different models, which will all be based on objects stored inside your BigQuery dataset, much like your data tables and views. Try clicking on a model object and you can view information about when it was last updated or how many training runs it completed. Creating a new model is as easy writing CREATE MODEL, choosing that model type, and passing in a training dataset. And again, if you're predicting in a numeric field, like sales for next year, consider looking at linear regression for forecasting. If it's a discrete class like high, medium, or low, spam or not spam, consider using logistic regression for classification. While the model's running, and even after it's complete, you could view training progress with ML.TRAINING_INFO. As we mentioned previously, you can see what the model learned about the importance of each feature as it relates to the label that you're predicting. That's my favorite part. Those are what's called you model's weight. You can see how well the model did against this evaluation dataset by using ML.EVALUATE. Lastly, it's as simple as writing ML.PREDICT in referencing your trained model, and then your prediction dataset to return back predictions. An important note here is that when using ML.PREDICT in passing in a new dataset with an unknown label that you can now add other columns that you didn't want to train on initially. The model is not being retrained naturally during prediction. Note that if you do happen to remove or rename columns from your prediction dataset, that the model is expecting and it saw in training, you'll be given an error. So the four major steps look like this. Write a SQL query to extract training data from BigQuery. Create a model specifying that type, evaluate the model and verify that it meets the requirements. And then predict on that model using data that's extracted from BigQuery. If you are explaining BigQuery ML as a whole to others, I often just list these main points. So as a broad recap, BigQuery ML allows you to, write machine learning models with SQL, experiment and iterate right where your data lives, inside of BigQuery. Currently, build classification, both binary and multi-class and forecasting models, with more model types coming soon. And if you know machine learning already, you can really get into the details of your model options and weights very easily. Common adjustments that you can make above and beyond what BigQuery ML defaults to everyone include things like the learning rate, regularization, the training evaluation dataset split, predefined weights for classes, and much, much more. Check out the BigQuery ML documentation link and bookmark it as a reference, especially as new features continue to be being added.

## QuizNotes

* Which of the below are the core services that make up BigQuery? (choose the correct 2)
	* Query Service
	* Storage Service
* You can query a Google Spreadsheet directly from BigQuery without loading it in first.
	* True
* You have a taxi service data schema that has three columns: ride_id, ride_timestamp, ride_status. You want to use BigQuery for reporting but you don't want to split your table into multiple sub-tables. What native features of BigQuery data types should you explore? (check all that apply)
	* Consider adding lat / long geographic data points as new columns and using GIS Functions to quickly plot the distances your fleet has travelled. 
	* Consider making ride_timestamp an ARRAY of timestamp values so each ride_id row in your table could still be unique and easy to report off of.
* In ML, a row of data is called a(n) ________ and a column of data is called a(n) _______. We mark one or more columns as ________ which we know for historical data and are trying to predict for future data.
	1. instance or observation
	2. feature
	3. label

## Module Resources

[Cloud Pub/Sub Documentation](https://cloud.google.com/pubsub/) + [release blog](https://cloud.google.com/pubsub/docs/release-notes)

[Cloud Dataflow Documentation](https://cloud.google.com/dataflow/) + [release blog](https://cloud.google.com/blog/products/data-analytics) + [templates](https://cloud.google.com/dataflow/docs/guides/templates/provided-templates)

[Data Studio Documentation](https://developers.google.com/datastudio/)

[Blog: Using Google Sheets with BigQuery](https://cloud.google.com/blog/products/g-suite/connecting-bigquery-and-google-sheets-to-help-with-hefty-data-analysis)

[BigQuery - Scheduling queries](https://cloud.google.com/bigquery/docs/scheduling-queries)

[BigQuery ML - Creating a k-means clustering model](https://cloud.google.com/bigquery-ml/docs/kmeans-tutorial)

[accuracy, precision, and recall](https://en.wikipedia.org/wiki/Precision_and_recall)