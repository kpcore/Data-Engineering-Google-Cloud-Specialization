## 1. Data and Machine Learning on Google Cloud Platform Specialization

* This course was designed to showcase real-world data and ML challenges and give you practical hands-on expertise in solving those challenges using Google Cloud. It's a critical course to master because it covers the most common use cases you and your team will encounter on your big data journey. This course is divided into six content modules.
* In the first module, which is on data-driven decision-making, you will learn all about the data and ML tools available on GCP from a high-level organizational perspective. Then in the next four modules, you will be introduced to Google Cloud products in context as they are employed to solve real-world problems. In the four modules, in addition to expanding your knowledge about our products and platform, you will also get to practice with hands-on Qwiklabs. Finally, in the summary module, we'll do a recap of everything you've learned in this course and provide you with additional resources on the topics that you've done.
* In each module, this is the typical order we'll follow. First, we'll have lectures from subject matter experts, where we'll introduce the big data scenario and the associated challenges and how to address them with Cloud technologies. Next, you'll see a demo of the solution and action which will highlight key features that you will learn and practice in your labs. After you understand the scenarios and have watched the demos, it's time for you to practice with Qwiklabs in a real Google Cloud Platform account. Finally, you will explore real customer use cases and architectures, so you can familiarize yourself with best practices and get inspired in your own solutions.
* In other words, we describe a common class of big data and ML problems and hone in on one specific problem and then we show you a demo of a solution to that problem. Then we talk about why we built a solution that way, using that opportunity to cover how and when to use the various products used in the solution. Finally, we widen the lens and show you real-world applications that are variants of the principles covered in the chapter.
* You're already taking this course which means you recognize the importance of big data processing. But why is this skill set in such high demand? According to McKinsey research, by 2020, we'll have 50 billion devices connected in the Internet of Things. These devices will cause the supply of data to double every two years. Unfortunately though, only about one percent of the data generated today is actually analyzed according to McKinsey.
* This state of affairs provides a wide open opportunity because there's a lot of value in data. I believe that the ability to build applications that handle large amounts of data and derive insights from that data in an automated manner. This ability is a skill that will be well rewarded in the marketplace. Individuals who have this skill will have many opportunities open to them and companies that develop this skill will become more successful. So the opportunity for data analysts, data scientists, and data engineers we'll talk about what these roles are and what the differences are.
* The opportunity for all three of these roles is very clear. At its core, this course is primarily geared towards data engineers. That said, if you're an analyst, an ML engineer, or a tech lead for your team, it's a valuable skill to know how all the big data and ML products interact to solve some of the most common challenges that data engineers face.
* What are those challenges? Those challenges are migrating your existing big data workloads to an environment where you can effectively analyze all of your data, interactively analyzing large and by large I mean terabytes to petabytes; analyzing large datasets of historical data. Third, building scalable pipelines that can handle streaming data, so that your business can make data-driven decisions more quickly. Finally, building machine learning models so that you're not just reacting to data, you're able to make predictive forward-looking actions using your data.

## 2. Introduction to Google Cloud Platform

* Welcome to the first module of our Big Data Fundamentals course. It provides an introduction to Google Cloud Platform. In this module, we'll examine the infrastructure behind Google Cloud Platform or GCP, which was originally built to power Google's own applications and is now available to you. Then we'll cover the big data and ML products that are built on top of that infrastructure and when you should choose which products for your solution architecture.
* After that, my favorite part of this course, learning from other customers who are using Google Cloud by exploring their use cases and getting inspired to solve similar challenges for your own teams and projects. You will learn where you can look up reference case studies of GCP customers by industry or by product, and then you'll examine their solution architecture in a short activity. Building the right team structure is critical to solving these big data challenges. We will explore the different types of roles and personas for building a successful big data team within your organization.
* Consider for a second, the impact that Google Search has on our daily lives with timely and relevant responses. Now think of other Google products: Gmail, Google Maps, Chrome, YouTube, Android, Play, Drive. Each of these products has over one billion monthly users. Google has had to develop the infrastructure to ingest, manage, and serve all the data from these applications, and to do so, with a growing user base and data requirements that are constantly evolving. Seven products with a billion users. Actually there's an eighth Google product that has a billion end-users, Google Cloud, except that it's your users.
* Let's look at the building blocks behind Google's big data infrastructure and how you can leverage it with Google Cloud. There are four fundamental aspects of Google's core infrastructure and a top layer of products and services that you will interact with most often.
* The base layer that covers all of Google's applications and therefore Google Cloud's too, is security. On top of that, are compute, storage, and networking. These allow you to process, store, and deliver business changing insights, data pipelines, and machine learning models. 
* Finally, while running your big data applications on bare metal virtual machines as possible, Google has developed a top layer of big data and ML products to abstract away a lot of the hard work of managing and scaling that infrastructure for you.

## 3. Compute Power for Analytic and ML Workloads

* A growing data organization like yours will need lots of compute power to run big data jobs, especially as you design for the future, so that you can outpace the growth in new users and data for the next decade. Let's start with an example illustrating how Google uses its own compute power. 
* Google Photos has recently been introducing smart features like this one, for automatic video stabilization, for when the camera is shaky, as you see here on the left. What data sources do you think are needed as inputs to the model? Well, you need the video data itself, which is essentially lots of individual images called frames ordered by timestamps. But we also need more contextual data than just the video itself, right?
* Absolutely. We need time series data on the camera's position and orientation from the on-board gyroscope and motion on the camera lens. So how much video data are we talking about for the Google Photos ML model to compute and stabilize these videos? If you consider the total number of floating-point values representing a single frame of high-res video, it's a product of the number of channel layers multiplied with the area of each layer, which with modern cameras can easily be in the millions.
* An eight megapixel camera creates images of eight million pixels each, approximately. Multiply that by three-channel layers and you get over 23 million data points per image frame. There are 30 frames per second of video. You can quickly see how a short video becomes over a billion data points to feed into the model.
* From 2018 estimates, roughly 1.2 billion photos and videos are uploaded to the Google Photos service everyday. That is 13 plus petabytes of photo data in total. For YouTube, which also has machine learning models for video stabilization and other models for automatically transcribing audio. You're looking at over 400 hours of video uploaded every minute. That's 60 petabytes every hour.
* But it's not just about the size of each video in pixels. The Google Photos team needed to develop, train, and serve a high-performing machine learning model on millions of videos to ensure that the model is accurate. That's the training dataset for this one feature. Just as your laptop hardware may not be powerful enough to process a big data job for your organization, a phone's hardware is not powerful enough to train sophisticated machine learning models.
* Google trains its production machine learning models on its vast network of data centers and then deploys smaller trained versions of these models to the hardware on your phone for predictions on your video. A common theme throughout this course is that when Google makes breakthroughs in AI research, it continues to invest in new ways to expose these as fully- trained models for everyone. You can, therefore, leverage Google's AI research with pre-trained AI building blocks.
* For example, if you're a company producing movie trailers and quickly want to detect labels and objects in thousands of these movie trailers to build a movie recommendation system, you could use a Cloud Video Intelligence API. You could use that API instead of building and training your own custom model. There are other fully trained models for language and for conversation too. You will learn and practice using these AI building blocks later in this course.
* But getting back to the Google Photos machine learning story. Running that many sophisticated ML models on large structured and unstructured datasets for Google's own products, required a massive investment in computing power.
* That's why Wired says, "This is what makes Google Google. Its physical network, its thousands of fiber miles, and those many thousands of servers that in aggregate, add up to the mother of all clouds." In essence, Google has been doing distributed computing for over 10 years for its own applications, and now, has made that compute power available to you through Google Cloud. But simply scaling the raw number of servers in Google's datacenters isn't enough. Here's an interesting rough calculation by Jeff Dean who leads Google's AI division.
* He realized years ago that if everybody wanted to use voice search on their phones and used it for only three minutes, we would need to double our computing power. Historically, compute problems like this could be addressed through Moore's Law.
* Moore's Law was a trend in computing hardware that describe the rate at which computing power doubled. For years, computing power was growing so rapidly that you could simply wait for it to catch up to the size of your problem. Although computing power was growing rapidly, even as recently as eight years ago, in the past few years, growth has slowed dramatically as manufacturers run up against fundamental physics limits. Compute performance has hit a plateau.
* One solution is to limit the power consumption of a chip, and you can do that by building Application-Specific Chips or ASICs. One kind of application is machine learning. Google's designed new types of hardware specifically for machine learning. 
* The Tensor Processing Unit or TPU is an ASIC specifically optimized for ML. It has more memory and a faster processor for ML workloads than traditional CPUs or GPUs. Google has been working on the TPU for several years and has made it available to other businesses like yours through Google Cloud for really big and challenging machine learning problems.
* One such business that uses TPUs is eBay. They use cloud TPU pods to deliver faster inferences, inference are predictions, faster inferences to their users by a factor of 10. That's a 10X speedup. The decrease in model training time has also led to faster model experimentation. ML model training and future engineering is one of the most time-consuming parts of any machine learning project.
* eBay's VP of new product development remarked that the additional memory of the TPU pods enabled them to improve their turnaround time and iterate faster. One last example of compute power at Google and an inside look at our culture of thinking 10X is how our teams used machine learning to boost Google's own datacenter efficiency.
* The potential impact for machine learning was there, considering the number of datacenters that Google has to keep cooled and powered, and we were already collecting streaming sensor data for our existing monitoring platforms. Engineers and Alphabet's deep mind saw this as an opportunity to ingest that sensor data and train a machine learning model to optimize cooling better than existing systems could.
* The model they implemented reduced the cooling energy by 40 percent, and boosted the overall power effectiveness by 15 percent. I find this example particularly inspirational, because it's a machine learning model trained on machine learning specialized hardware in a datacenter, telling the datacenter how hard it can run the machine learning specialized that the model is training on. Powerful inception level stuff. Later in the course, you will see a demo on how you can set up a streaming data ingestion pipeline for your own IoT devices in less than an hour.
* The model they implemented reduced the cooling energy by 40 percent, and boosted the overall power effectiveness by 15 percent. I find this example particularly inspirational, because it's a machine learning model trained on machine learning specialized hardware in a datacenter, telling the datacenter how hard it can run the machine learning specialized that the model is training on. Powerful inception level stuff. Later in the course, you will see a demo on how you can set up a streaming data ingestion pipeline for your own IoT devices in less than an hour.

## 4. Elastic Storage with Google Cloud Storage

* In the demo at the end of the previous section, I copied the ingested and transformed files out of the compute instance into Cloud Storage. This way, we could stop our compute instance and retain access to the data.
* While we've discussed the demand for a great amount of computing power for today's big data and ML jobs, we still need a place to store all the data that's generated. As with the demo, this needs to be separate from the compute instances, so that we can analyze it, transform it, and feed it into our models. 
* This is one major way that Cloud computing differs from desktop computing. Compute and storage are independent. You don't want to think of disks attached to the compute instance as the limit of how much data you can process and store. Getting your data into your solution and transforming it for your purposes, should be your first priority.
* In the roles and team structured discussions that I'll talk about later in this module, I'll talk about the need for data engineers to build data pipelines, before you can start building machine learning models from that data. When the pipeline is built, the job isn't done. Because once the data is in your system, data engineers are still needed to replicate the data, back it up, scale it, remove it as needed, all at scale.
* Instead of data engineers managing storage infrastructures themselves, data engineers can use Google Cloud Storage which is a durable global file system. Creating an elastic storage bucket, it is as simple as using the WebUI in console.cloud.google.com, or the Google Storage utility function called gsutil in the command line interface. As an additional level of flexibility, a data engineer can choose what type of storage class they want for their data.
* There are four storage classes for you to choose from based on your data needs: multi-regional, regional, nearline, and coldline. For big data analytic workloads, the most common thing is to use a regional cloud storage bucket for staging your data. Since we have mentioned a GCP resource, a Cloud Storage bucket, it's important to cover some of the account management logistics before we get too far in.
* Starting from the most granular objects, you see that resources like a Cloud Storage bucket or Compute Engine instance, these resources belong to specific projects. Bucket names have to be globally unique and GCP assigns your project ID that's globally unique too, and so you can use that project ID a unique name for your bucket. But what's a project?
* A project is a base-level organizing entity for creating and using resources and services for managing billing APIs and permissions. Zones and regions, physically organize the GCP resources you use. Whereas projects logically organize them. Projects can be created, managed, deleted, even recovered from accidental deletions. Folders are another logical grouping, you can have for collections of projects.
* Having an organization is required to use folders. But what's an organization? The organization is a root node of the entire GCP hierarchy. While it's not required, an organization is quite useful, because it allows you to set policies that apply throughout your enterprise to all the projects and all the folders that are created in your enterprise. Cloud Identity and Access Management, also called IAM or IAM, lets you fine tune access control to all the GCP resources you use. You define IAM policies that control user access to resources.
* Remember, if you want to use folders, you must have an organization. Now that you have a Google Cloud Storage bucket created, how do you get your data on the cloud, and work with the data once it's there in the bucket? In the demo, I used gsutil commands. Specifically, we can use cp for copy and specify a target bucket location. If you spin up a Compute Engine instance, the command-line tool gsutil is already available and we can do gsutil copy. On your laptop, you can download the Google Cloud SDK, so that you can get gsutil. Gsutil uses a familiar Unix command-line syntax. So if you know how to use the cp command on Unix, you know how to use gsutil cp.

## 5. Build on Google's Global Network

* So far, we've looked at compute and we've looked at storage. The third part of Google Cloud infrastructure is networking. Google's high-quality private network, petabit bisectional bandwidth, and Edge points-of-presence are combined using state- of-the-art software- defined networking to deliver a powerful solution. 
* First, the private network. Google has laid thousands of miles of fiber optic cable that crosses oceans with repeaters to amplify optical signals, and as you can see in this amusing GIF, it's shark-proof. Google's data centers around the world are interconnected by this private Google network, which by some publicly available estimates, carries as much as 40 percent of the world's internet traffic everyday. This is the largest network of its kind on Earth and it continues to grow.
* Second, the petabit bisectional bandwidth. One of the teams we will discuss in this course is a separation of compute and storage. You no longer need to do everything on a single machine or even a single cluster of machines with their own dedicated storage. Why? Well, if you have a fast-enough network, you can perform computations on data located elsewhere like many distributed servers. 
* Google's Jupiter Network can deliver enough bandwidth to allow 100,000 machines to communicate amongst each other. So for any machine to communicate with any other machine in the data center at over 10 gigabits per second.
* This full duplex bandwidth means that locality within the cluster is not important. If every machine can talk to every other machine at 10 Gbps, racks don't matter for data analytics and machine learning. But you need to ingest data probably from around the world. You need to serve out the results of your analytics and predictions, perhaps to users who are all around the world. This is where Edge points of presence come in.
* The network, Google's Network, interconnects with the public Internet at more than 90 internet exchanges and more than 100 points of presence worldwide. When an Internet user sends traffic to a Google resource, Google responds to the user's request from an Edge network location that will provide the lowest delay or latency. 
* Google's Edge caching network places content close to end-users to minimize latency. Your applications in GCP, like your machine learning models, can take advantage of this Edge network too.

## 6. Security: On-premise vs Cloud-native

* The last piece of core infrastructure underpinning your data pipelines and machine learning models is Google great security. When you build an application on your on-premises infrastructure, you are responsible for the entire stack's security, from the physical security of the hardware and the premises in which they're housed through the encryption of the data on disk, the integrity of your network, and all the way up to securing the content stored in those applications.
* But when you move an application to GCP, Google handles many of the lower layers of security like, the physical security of the hardware and its premises, the encryption of data on disk, and the integrity of the physical network. Because of its scale, Google can deliver a higher level of security at these layers, than most customers could afford to on their own.
* The upper layers of the security stack, including the securing of data, remain your responsibility. But even here Google provides tools like Cloud IAM to help you implement the policies that you define at these layers.
* Communications over the internet to our public cloud services are encrypted in transit. Google's network and infrastructure have multiple layers of protection to defend our customers against denial of service attacks.
* Stored data is automatically encrypted at rest and distributed for availability and reliability. This helps guard against unauthorized access and service interruptions. One specific product I'll highlight here, that you will see a lot of in this course is BigQuery. Google Cloud's petabyte scale analytics data warehouse.
* Data in a BigQuery table is encrypted using a data encryption key. Then, those data encryption keys are themselves encrypted with key encryption keys. This is known as envelope encryption and it provides additional security. BigQuery also allows you to provide your own encryption keys. These are called Customer Managed Encryption Keys.
* Inside BigQuery, you can monitor your team's BigQuery usage and running queries, and proactively limit access to data at a row and a column level. We cover BigQuery as a service in greater detail later. But in this section, we just wanted to point out BigQuery's security controls as an example of the security controls that you will find in every service on GCP.

## 7. Evolution of Google Cloud Big Data Tools

* So far, we've talked about low-level infrastructure, compute, storage, networking and security. However, as a data engineer or a data scientist or data analyst, you will typically work with higher level products. So let's talk about the big data and machine learning products that form Google Cloud Platform.
* We'll talk about the chronology of innovation not for history's sake, but so that you understand the evolution of data processing frameworks. Knowing how these frameworks have evolved can help you understand the typical problems that arise, and how they're addressed.
* One of the interesting things about Google is that historically, we have faced issues related to large data sets, fast changing data and varied data, what is commonly called big data earlier than the rest of the industry. Having to index a World Wide Web will do that, and so as the Internet grew, Google invented new data processing methods. 
* In 2002, Google created GFS, or the Google File System to handle sharding and storing petabytes of data at scale. GFS is the foundation for cloud storage and also for what would become BigQuery managed storage. One of Google's next challenges was to figure out how to index the exploding volume of content on the web.
* To solve this in 2004, Google invented a new style of data processing known as MapReduce to manage large-scale data processing across large clusters of commodity servers. MapReduce programs are automatically parallelized and executed on a large cluster of these commodity machines. 
* A year after Google published a white paper describing the MapReduce framework, Doug Cutting and Mike Cafarella created Apache Hadoop. Hadoop has moved far beyond its beginnings in web indexing, and is now used in many industries for a huge variety of tasks that all share the common theme of volume, velocity and variety of structured, and unstructured data.
* As Google's needs grew, we faced the problem of recording and retrieving millions of streaming user actions with high throughput, that became Cloud Bigtable which was an inspiration behind Hbase or MongoDB. One issue with MapReduce is that developers have to write code to manage all of the infrastructure of commodity servers. Developers couldn't just focus on their application logic. 
* So between 2008 and 2010, Google started to move away from MapReduce to process and query large data sets, and instead they started moving towards new tools. Tools like Dremel. Dremel took a new approach to big data processing where Dremel breaks data into small chunks called shards, and compresses them into a columnar format across distributed storage.
* It then uses a query optimizer to farm out tasks between the many shards of data and the Google data centers full of commodity hardware to process a query in parallel and deliver the results. The big leap forward here was that the service, automanagers data imbalances, and communications between workers, and auto-scales to meet different query demands, and as you will soon see, Dremel became the query engine behind BigQuery.
* Google continued to innovate to solve its big data and ML challenges, and created Colossus as a next-generation distributed data store, Spanner as a planet scale relational database. Flume and Millwheel for data pipelines, Pub/Sub for messaging, TensorFlow for machine learning plus there are specialized TPU hardware we saw earlier, and Auto ML that's going to come later.
* The good news for you is that Google has opened up these innovations as products and services for you to leverage as part of the Google Cloud platform.
* Storing and querying massive datasets can be time consuming and expensive without the right hardware and infrastructure. Google BigQuery is an enterprise data warehouse that solves this problem by enabling super-fast SQL queries using the processing power of Google's infrastructure. Simply move your data into BigQuery and let us handle the hard work. You can control access to both the project and your data based on your business needs, such as giving others the ability to view or query your data.

## 8. Choosing the right approach

* Choosing which big data and machine learning products are the right mix for your solution is a critical skill that you have to learn. Later on in this module, you will get the opportunity to examine the architecture that real Google Cloud customers use so that they can serve as inspiration.
* Let's review the options available to you for compute and storage services so that you can better interpret those use cases later. The service that might be the most familiar to newcomers is Compute Engine which lets you run virtual machines on demand in the Cloud. It's Google Cloud's Infrastructure as a Service or IaaS solution. It provides maximum flexibility for people who prefer to manage server instances themselves.
* GKE, Google Kubernetes Engine is different. Where Compute Engine is about individual machines running native code, GKE is about clusters of machines running containers. Containers have code packaged up with all its dependencies. So GKE enables you to run containerized applications in a Cloud environment that Google manages for you under your administrative control.
* Containerization is a way to package code that's designed to be highly portable and to use resources very efficiently. Since most use cases involve multiple programs that need to execute and communicate with each other, you need a way to orchestrate the containers running these separate programs. That's what Kubernetes does. Kubernetes is a way to orchestrate code that's running in containers. All the Kubernetes in GKE are outside the scope of this course, are linked to our cloud architecture specializations in the course resources.
* App Engine is GCP's fully managed Platform as a Service or PaaS framework. That means it's a way to run code in the Cloud without having to worry about infrastructure. You just focus on your code and let Google deal with all the provisioning and resource management. You can learn a lot more about App Engine in the specialization Developing Applications in Google Cloud Platform. Cloud Functions is a completely serverless execution environment or Functions as a Service, FaaS. It executes your code in response to events whether those events occur once a day or many times a second.
* Google's scales resources as required but you only pay for the service while your code runs. What's the difference between App Engine and Cloud Functions? Typically, App Engine is used for long-lived Web applications that can autoscale to millions, to billions of users. Cloud Functions are used for code that's triggered by an event such as a new file hitting Cloud storage. The fastest way to lift and shift your data workloads is by provisioning a VM and running your code. You will experiment with this later on when you run Spark ML jobs on Cloud Dataproc which spins up Compute Engine instances for your cluster.
* Most applications need a database of some kind. If you've built a cloud application, you can install and run your own database for it on a virtual machine in Compute Engine. You simply start up the virtual machine, install your database engine, and set it up just like in a datacenter. Alternatively, you can use Google's fully-managed database and storage services. 
* What all these - Bigtable, Cloud Storage, Cloud SQL, Spanner, Datastore - what all these have in common is that they reduce the work it takes to store different kinds of data. GCP offers relational and non-relational databases and worldwide object storage. You will learn more about these later on in this course. 
* GCP also offers fully- managed big data and machine learning services. Just as with storage and database services, you could build and implement these services yourself. But why manage the infrastructure for compute and storage where it can be fully managed by Google Cloud? Here's a complete list of big data and ML products organized by where you would likely find them in a typical data processing workload.
* On the left, you will see the foundation to where your raw data is stored. If your data isn't stored on GCP yet, you can ingest it using the tools that you see next. After your data is on GCP, you can analyze it using the tools in the third column and run machine learning on it with the tools in the fourth column. The last column is how you can serve out your data and ML insights out to your users.

## 9. What you can do with Google Cloud Platform

* Now, it's time to explore some cool Big Data and Machine Learning solutions that have been built using Google Cloud. Then, you will get a chance to find a use case and explore it in your own activity. Keller Williams is a US real estate company.
* Keller Williams uses AutoML Vision to automatically recognize specific features of houses, like this house has a built-in bookcase. This helps agents get houses listed faster and buyers find houses that meet their needs. Neil Dholakia, Chief Product Officer of Keller Williams says that by training a custom machine learning model to recognize common elements of furnishings and architecture, customers can automatically search home listing photos for specific features, like granite countertops or even more general styles like, "Show me modern houses."
* This application of machine learning allows Keller Williams' realtors to quickly walk around a home and record a video, and use the object detection capabilities of AutoML Vision to find and tag key aspects of the home that customers might want to search on. A big benefit for their organization is that they already had many existing images and videos of home walk-throughs already. They simply fed them into the pre-built AutoML Vision model and customized it, all without writing a line of code. You will learn more about AutoML Vision and practice creating machine learning models with it later in this course
* Ocado, the UK online only grocery supermarket used machine learning to automatically route emails to the department that needed to actually handle them. This avoids multiple rounds of reading and triaging those emails. With their old process, all the mails went to a central mailbox where the email was read, and then routed to the person or department that could handle it. Unfortunately, the central mailbox with somebody reading all the emails, that doesn't scale. So it led to long delays and poor user experience. So Ocado used machine learning, specifically the ability to read an email, to process natural language, to discover customer sentiment, and what the message was about so that they could route it immediately and automatically.
* One last use case. Kewpie manufactures baby food. In this case, quality is not necessarily a matter of safety, because the food itself is safe. But if baby food is discolored, it tends to get parents very concerned. So Kewpie turned to Google and our partner BrainPad to build a solution that leverages image recognition to detect low-quality or discolored potato cubes. The machine learning algorithm enabled them to free people up from the tiring work of inspection and focused on other more important work.

## 10. Activity: Explore real customer solution architectures

* Now, it's your turn. One of the best ways to inspire and drive your team and projects to the cloud, is to show your stakeholders examples from your industry, examples where someone has already succeeded with building a solution.
* For this activity, navigate to cloud.google.com/customers and scroll down. For Products and Solutions, filter on big data analytics and also on machine learning. Select a customer use case that interests you, then answer these three questions.
* Number one, what were the barriers or challenges the customer faced? The challenges are important, you want to understand what they were. Second, how were these challenges solved with a cloud solution? What products did they use? Three, what was the business impact?
* For our example we chose GO-JEK, because they use a data engineering solution that maps nicely to the topics that we're going to cover as part of this course. GO-JEK is an Indonesia-based company that gives shared motorcycle rides, brings goods, and provides a wide variety of other services for over two million families across 50 cities in Indonesia.
* Their App has over 77 million downloads, and they're connected with over a 150,000 merchants who sell through their delivery platform. If you're interested in GIS data, they have over one million drivers delivering goods and giving rides across 50 cities and GO-JEK gets censored data from all of these drivers. So GO-JEK manages more than five terabytes per day of data for analysis.
* The Chief Technology Officer, the CTO, Ajey Gore, gives this meaningful statistic. For example, we ping everyone of our drivers every 10 seconds. Which means, six million pings per minute and eight billion pings per day, that's Gore who's saying this. If you look at the scale and number of our customer interactions as well, we generate about four terabytes to five terabytes of data every day.
* We need to leverage this data to tell our drivers where demand from customers is strongest and how to get there. With the success of their on-demand motorcycle ride service, GO-JEK faced the challenges when looking to scale their existing big data platform. What challenges? Their management team stated, "Most of the reports are produced one day later so we couldn't identify the problems that were happening as soon as possible."
* GO-JEK chose Google Cloud Platform and migrated the data pipelines to GCP, so that they could get high performance with minimal day-to-day maintenance. Their data engineering team uses Cloud Dataflow for streaming data processing and Google Big Query for real-time business insights. Their end to architecture looks like this.
* First, they ingest data from their mobile App online and IoT devices on vehicles like the GPS tracking for deliveries. They ingest this data into Cloud PubSub. Then, the data is brought into Cloud Dataflow for processing and a variety of other data sources are used to enrich this event data. Finally, after Dataflow has done the processing, Dataflow streams that data into BigQuery and BigQuery in this case is used as a data warehouse to store the data.
* What's the business impact? So here's an example of one of the problems that the GO-JEK team solved. The question was, how could they quickly know which locations had too many drivers or too few drivers? Too few drivers to meet the demand for that area. To solve this problem, what does the team need to do? Number one, they needed to check the demand of bookings by customer against the supply of drivers and do this thing in real-time.
* Then, the team needed to identify who these drivers are and notify them to reroute to higher demand areas. How they achieved this, was by building a streaming event data pipeline using Cloud Dataflow. Driver locations would ping out to Pub/Sub every 30 seconds and these locations would go into Dataflow for processing.
* The data for pipeline aggregates the supply pings from the drivers against the requests for bookings, and then connects to GO-JEK's notification system to alert drivers. From a technology standpoint, the system needs to handle an arbitrarily high throughput of messages and scale up and down to process. Cloud Dataflow automatically manages a number of workers, processing the pipeline to meet demand. The GO-JEK team is then able to visualize and highlight supply-demand mismatch areas for management reporting, as you see in this example here.
* The green dots, the small green dots, represent riders and new booking requests, and the red dots, the small red dots, those are the drivers, so you have riders in the green dots and the drivers in the small red dots. Then you see the areas that the system has identified as a highest mismatch of supply and demand, those areas are highlighted in red like the train station which has many booking requests, but not enough drivers.
* The team can now actively monitor and ensure that they're sending drivers to the areas in highest demand, which means faster booking times for riders and more fairs for the drivers.

## 11. Key roles in a data-driven organization

* So far, you've seen the infrastructure, the software, and the customers who are already using GCP. But the most critical factor to the success of your future big data and ML projects, is your team itself. 
* The people and the core skill sets required, will make or break your next innovation. A common mistake that companies make is that they go out and hire 10 PhD machine learning scientists, and expect magic to happen. I see this a lot with companies who are new to building data science and ML teams. They focus on the ML researchers, and forget about all the help and guidance that the ML researchers will need. 
* The reality as my colleague Cassie has noted in a blog post, looks more like this. You need data engineers to build the pipelines and get you clean data. 
* Decision makers, to decide how deep you want to invest in a data-driven opportunity while weighing the benefits for the organization. Analysts, to explore the data for insights and potential relationships that could be useful as features in a machine learning model. Statisticians, to help make your data-inspired decisions become true data-driven decisions, with their added rigor. Applied machine learning engineers, who have real-world experience building production machine learning models from the latest and best information and research by the researchers. Data scientists, who have the mastery over analysis, statistics, and machine learning. Analytics managers to lead the team. Social scientists and ethicists to ensure that the quantitative impact is there for your project and, it's the right thing to do.
* As I've written in a blog post on this subject, it's linked below, a single person might have a combination of these roles, but this depends on the size of your organization. Your team size is one of the biggest drivers in whether you should hire for a specific skill set, up-skill from within, or combine the two.
* Do you remember these big data challenges? Can you see how different roles would map to this? Within Google Cloud training, my team and I have thought about the different types of data science teams and roles that are using Google Cloud, so that we can best tailor our data in ML courses and labs. One of the core challenges we face, is how different types of users engage with our GCP big data and AI products.
* We'll be using a few personas in this course. Their backgrounds, goals, and challenges, might be similar to yours. Let's meet them now, and you'll see them again later.
* Brittany and Theo lead their data engineering team in managing their Hadoop cluster for the organization's data pipelines and compute jobs. Their organization was an early adopter of Hadoop for distributed computing back in 2007, and they've built up Hadoop jobs over time. Brittany and Theo's job is to actively ensure that the Hadoop jobs are all run and that the cluster is well maintained.
* They say, "Our CTO has challenged our data engineering team to find ways we can spend less on managing our on-prem cluster. Right now, we just want to show her options that don't require any code changes to our 100+ Hadoop jobs." Brittany and Theo are data engineers who manage a company's data platform, and are focused on reducing maintenance burden.
* Jacob is a data analyst who has a background in building and querying his company's MySQL transactional and reporting database. As the company grows, the reporting tables in his RDBMS are already starting to slow down. Users are reporting long query and dashboard loading times. He wants to find an easy path for scaling his company's data reporting and not have to manage another data system, as the data needs to grow. Jacob is a data analyst who wants to be able to derive insights from data and disseminate them with as little friction as possible.
* Rebecca is a data engineer, whose company specializes in harnessing data from the Internet of Things or IoT devices. She says, "I really want to design our data pipelines for the future. For us, that means lots and lots of streaming data from our IoT devices with low latency." Her team lead has asked her to come up with a plan to handle the expected 10X growth in streaming data volumes this year. She wants to future proof her team's pipelines, but doesn't want to spend hours manually scaling hardware up and down as streaming volume changes. Additionally, her business stakeholder team wants insights from all the IoT devices in the field on their dashboards, with minimal delay.
* Vishal says, "I pitched my team on the value machine learning can add, and I've got buy-in to build a prototype. But now I've got to build a prototype. What are some of the easiest ways I can see whether machine learning is feasible for my data?" Vishal is an applied machine learning engineer, who has a background in building machine learning models in TensorFlow and Keras. His team is growing rapidly, and he's often asked by his leadership, to assess the feasibility of machine learning for a wide variety of projects.
* He doesn't have the time to train and test all of the ideas with custom models. He wants to empower his data analysts and data engineering teams, by teaching them machine learning. Do these personas sound familiar to your role and your team? 
* Next, we'll learn more about the Google Cloud Platform, big data and machine learning approaches and solutions, so that we can address each of these challenges.
## QuizNotes


The cloud is a constantly changing environment and Google Cloud Platform is continually evolving and releasing new products and features. It's a good idea to bookmark the below links to stay ahead of updates:

[Google Cloud Platform blog](https://cloud.google.com/blog/products)
[GCP big data product list](https://cloud.google.com/solutions/smart-analytics)
[GCP customers and case studies](https://cloud.google.com/customers/#/)

Need to practice SQL a bit more?

[BigQuery standard SQL guide](https://cloud.google.com/bigquery/docs/reference/standard-sql/enabling-standard-sql)
[Qwiklabs BigQuery quest for Data Analysts](https://www.qwiklabs.com/quests/55)

Learn more about big data infrastructure:

Compute Engine: https://cloud.google.com/compute/
Storage: https://cloud.google.com/storage/
Pricing: https://cloud.google.com/pricing/

We'll keep the module resources links up-to-date with the latest news and tips. Found a great blog post? Share it in the forums so everyone can benefit.


## QuizNotes

* You don't need a table to hold the dataset
* Check which you can use to access BigQuery
	* Third-party tools
	* Web UI
	* Command line tool
	* Make calls to BigQuery REST API
* What are the common big data challenges that you will be building solutions for in this course? (check all that apply)
	* Migrating existing on-premise workloads to the cloud
	* Analyzing large datasets at scale
	* Building streaming data pipelines
	* Applying machine learning to your datasets
* You have a large enterprise that will likely have many teams using their own Google Cloud Platform projects and resources. What should you be sure to have to help manage and administer these resources? (check all that apply)
	* A defined Organization
	* Folders for teams and/or products
	* A defined access control policy with Cloud IAM
* Which of the following is NOT one of the advantages of Google Cloud security
	* Google Cloud will automatically manage and curate your content and access policies to be safe for the public
* Which of the following is one of the advantages of Google Cloud security
	* Google Cloud will secure the physical hardware that is running your applications and infrastructure 
	* Google Cloud has tools like Cloud IAM that help you administer and set company-wide security policies 
	* Google Cloud will manage audit logging of access and use of resources in your account
* If you don't have a large dataset of your own but still want to practice writing queries and building pipelines on Google Cloud Platform, what should you do?
	* Practice with the datasets in the Google Cloud Public Datasets program
	* Find other public datasets online and upload them into BigQuery
	* Work to create your own dataset and then upload it into BigQuery for analysis 
* As you saw in the demo, Compute Engine nodes on GCP are:
	* Allocated on demand, and you pay for the time that they are up. 

## Resources

[BigQuery](https://cloud.google.com/bigquery/#bigquery-solutions-and-use-cases)

[Google Cloud customers](https://cloud.google.com/customers/#/products=Big_Data_Analytics)

