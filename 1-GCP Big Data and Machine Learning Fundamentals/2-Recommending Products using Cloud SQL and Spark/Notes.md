## 1. How businesses use recommendation systems

* This module is on performing product recommendations using Cloud SQL and Apache Spark. We'll cover product recommendations, which is probably the most common machine learning problem employed by businesses.
* Along the way, we'll also get to look at two Google Cloud products; Cloud SQL, which is a managed relational database, and Cloud Dataproc, which is a managed environment on which you can run Apache Spark.
* We'll start out talking about recommendation systems. What they are, what business applications they power, and consider a typical scenario. In this case, that is product recommendations using machine learning for recommending rental houses. Let's say that our data science team is familiar with Hadoop and Spark, and they've already built a recommendation system.
* Our mission, which we have chosen to accept, is to migrate this existing recommendation system that our team has built from on-premises cluster to the Cloud. We look at how you would go about migrating on-premises applications to Google Cloud platform.
* In particular, why you would want to migrate an on-premise application to Google Cloud platform. Maybe it is to avoid the challenges that are associated with utilizing and tuning on-premise clusters. But also, when you move an on-premise application to Google Cloud, you're also moving from dedicated storage to off-cluster storage with Google Cloud Storage, and we'll talk about why that is. Let's start with where you find recommendation systems adding value to businesses.
* Where have you seen recommendation systems before? One common place that comes to mind is e-commerce product recommendations. A model learns what you like and don't like, what you buy and don't buy, and then starts to suggest similar products that you may not have discovered by yourself.
* You can then help inform the model explicitly by starring, up voting, or down voting your own personal rating of an item, or maybe by adding the item to the shopping cart. Notice how I said explicitly. A model can learn from your preferences implicitly, too. Can you think of a few data points that an e-commerce model could learn without you explicitly rating items? Perhaps, time spent on the website of particular products, maybe items that you have last viewed, maybe where you navigate it from, what device you're on, and if you've allowed it, geographic personalization as well, things that people who live in your neighborhood or your country tend to buy.
* You have probably seen recommendation engines in Netflix, Amazon, and so on. It's also what Facebook uses to put posts on your newsfeed, and what Google uses to personalize your search results. Product recommendations are the first machine learning application the public recognizes.
* As in, "How does Netflix know that I would like this movie?" The general population is pretty unaware of machine learning applications like medical diagnoses, or airline fares or detecting illegal logging or playing chess until it got to product recommendations where if you like x, you will also like y, and that tuned the public in to the idea of machine learning. 
* One cool example of a recommendation system is Smart Reply in Gmail. This is a machine learning model that recommends three possible answers to your emails. Another is how Google Photos recommends similar photos or groups of similar photos so that you can add them into an album and you can order a printed version of it. Another example is how Google Maps now serves personalized restaurant recommendations. When you explore a new area, the app provides users with a "For you" tab, which lists restaurants and venues that it thinks you will enjoy based on your history. For example, here you can see that Good Barbecue is a 95 percent match for me, as well as a recommendation for a new bistro that is trending.
* A core aspect of a recommendation system is that you need to train and serve it at scale. Think of the machine learning system that drives YouTube recommendations for users. It identifies things that a user may like based on what they've watched in the past and serves them these video recommendations, like these video recommendations that I got when I logged into YouTube. Most of the recommendations here are about programming Tensor Flow or machine learning, which I guess says a lot about the videos that I like. These recommendations are relevant and useful to me. In the following lessons, you'll implement your own recommendation system for housing rentals.

## 2. Introduction to machine learning

* Now that you've seen how recommendation systems are used at Google and in other businesses, it's time to look into the specific scenario that we'll use in the next lab. The core pieces of a recommendation system are: data, the model, and infrastructure to train and serve recommendations to users.
* Our data set for this scenario will be housing rentals that we want to recommend to our users based on their preferences. We'll use a machine learning model to make these recommendations. A core tenet of machine learning is to let the model learn for itself what the relationship is between the data that you have like user preferences and housing features, and the data that you don't have, like a user's rating on a property that they haven't yet seen. What would this user rate this property if we showed it to them? You will not be writing custom logic. 
* Logic like, if the house is a beach house and the season is summer and the user likes cozy houses, then house number 22 is what we recommend. Can you think of the issues if we used such logic instead of letting the model figure it out? What if there were many different kinds of beach houses, with all kinds of different features like the number of bedrooms, locations, amenities, you get the picture. What if you have different kinds of users. Some users would like to go to beach houses in summer and others would like the shoulder season.
* Hard-coding logic for all these features and different kinds of user segments isn't scalable, and it assumes that we always know the right answer for every scenario. Let's illustrate this point with an example. Take Google Search. Say you go to Google and search for "giants." What should we show you as the results to make it most relevant for you? If you're in California, should we show results for the San Francisco baseball team and the local games nearby? What about if you're based in New York? Should we tailor the results to show the New York Giants football team instead, and should we write this as a rule?
* Well, up until a few years ago, this is exactly how Google search worked. There were tons of rules that were part of the search engine code base to decide which sports team to show a user when they search for "giants." The query is "giants" and the user is in the Bay Area, show them results about San Francisco Giants. If the user's in New York area, show them results about New York Giants. If they're anywhere else, show them results about tall people.
* Just imagine how many, if-then or case statements this would be, and how hard it would be to maintain, and this is just for one query. Multiply this by the large variety of queries that people make, where they make them from, what device they're on, and what kinds of interests different people have, and you can imagine how complex a code base would become. The code base starts to get unwieldy because hand-coded rules are really hard to maintain. So this is where machine learning comes into play.
* Machine learning scales much better because it doesn't require hard-coded rules. It's all automated. Learning from data in an automated way, that's what machine learning is. Our data set in this case is that we know historically when people searched for "giants" and we showed them a bunch of links, which of those links did people click on?
* Since we know this, we have a data set of a query term and the links that people liked. Why can't we train a machine learning model to basically provide a ranking of these links? That's exactly what Google itself did internally with a deep learning model called Rank Brain. After rolling it out, the quality of our search ranking results improved dramatically, with the signal from Rank Brain becoming one of the top three for influencing how different link results are ranked.
* We'll revisit machine learning in greater depth in each of the modules in this course. But for now, just remember that machine learning is this idea that we want to teach the computer using examples, not with rules. Any business application where you have those long switch or case statements or if-then logic manually coded, and you have a history of good labeled data, that is data for which you know a good answer or a bad answer, a history of good labeled data, any such application is a possible application for machine learning.

## 3. Challenge: ML for recommending housing rentals

* So how would housing recommendations work in our system? First, we need to ingest the ratings of all the houses that have already been done by our users when we showed them specific houses. So we have to go to an inventory of rentals and ingest the ratings for the houses. These ratings could come from explicit ratings. Maybe we showed the user the house in the past and they've clicked four stars after seeing the house details, or the ratings have come from implicit ratings. 
* Maybe they've spent a lot of time looking at the website corresponding to this property. Then, we'll train a machine learning model to predict a user's rating of every house that we currently have in our listing database. We will then pick the top five rated houses that they haven't already seen. 
* People who are not aware of recommendation engines somehow think of a car appearing on their Facebook feed because they happened to read an automobile review article. They say, "Oh, I was reading this review article and then just kept seeing cars in my Facebook feed." That's not the case. Reading the article caused the rating of all cars to go up and the ratings of other items that are not cars to stay relatively the same. This caused the highest ranking car to get into the top five. So there was always a rating for the car. The car was always there but its rating was simply lower before they read the article in question.
* So how would we predict a user's rating of a house, particularly, if they haven't seen it before. The model is based on two things. It's based on your other ratings, what have you rated other houses, and other people's rating of this particular house. A particularly simple model could be to look at all the users who rated this particular house and find the three users in their list who are most like you, maybe, they live in your country, maybe they have the same age, maybe they went to the same college, find the three users on that list who are most like you.
* Then, the prediction is the average of those three users ratings. This is not a great model of course. It's too easy to game. Think about what happens if three people gang up together and rate a house, a house that nobody else cares about. So there are only three ratings for this house. So the three closest users are going to be these three people. So for sparse houses, it's really, really, really easy to game. But this idea of using user ratings of a particular house and users like you helps convey the basic premise of how recommendation models work.
* So where is the machine learning here? Where's the learning? The model would have to figure out how to find the users who are most like you. How many users to consider. Three users, five users, seven users, how many? And how to weight the different factors such as the overall popularity of the items you have in common and so on. This can be done by seeing what parameters help predict if you intentionally withheld ratings best. So we may have thousands of items and only 2-3 reviews per item.
* The chances are those reviewers have nothing in common with the user that we want a rating for. So because this rating matrix is extremely sparse, we need to cluster items and users together. To put this in a more intuitive way, imagine that all of your friends drive SUVs, Sport Utility Vehicles, and you read an article on Porsche. The car that appears on your feed might be a Porsche SUV even if the article was on Porsche cars and all your friends drive Toyota SUVs. The machine learning model is imputing a rating for a Porsche SUV that is quite high even though none of your friends rated it. 
* So the machine learning model is essentially asking "Who is this user like?" Secondly, is this subjectively a house that people tend to rate highly? The predicted rating is a combination of both these factors. All things considered, the rating of a house for a particular user will be the average of the ratings of users like this user but it's calibrated with the quality of the item itself.
* Now that we understand the problem and approach, we need to address the last question, the infrastructure question. How often and where will you compute the predicted ratings, and once you have them, once you've computed these ratings, where would you store them? What do you think? 
* How often and where will you compute the predicted ratings? It's not as if rental recommendations have to be updated every time some users somewhere rates a house. We don't need to update the rental recommendations every time a new rating appears in our system. It's probably sufficient if we update the recommended houses for users once a day, maybe even once a week. 
* In other words, this does not need to be streaming. It could be batch. On the other hand, we will probably have thousands of houses and millions of users. So it's probably best that we compute the rating that every user will give every house, we do that computation in a scalable way. We don't want to do it on a single machine, we want to do it in a fault tolerant way that can scale to large datasets.
* So a typical solution for computation that has to happen over large datasets in a fault tolerant way is to do it in a big data platform like Apache Hadoop. Finally, where will you store the computed ratings? Why would you want to store them? Well, we probably want to power a web application with these recommendations and we don't want to compute recommendations when the user reads a webpage.
* We want to precompute these recommendations, as we said, it's a batch job. So once a day, once a week, we pre-compute it. Then, when the user logs on, we want to show that user the recommendations that we precomputed specifically for them. So we need a transactional way to store the predictions.
* Why transactional? So that while the user is reading these predictions, we can update the predictions table as well. Assuming that there are five predictions for every user and we have a million users, that's a table of just 5 million rows. It's small enough and compact enough that a typical solution for this would be to store the data in a relational database management system, an RDBMS like mySQL.

## 4. Approach: Move from on-premise to Google Cloud Platform

* So we decided to use a big data platform like Hadoop and then RDBMS like MySQL to solve the housing recommendations problem. These are both open source technologies. You probably have Hadoop clusters and MySQL database running on-premises. So let's assume that your team already has this recommendation system working on-prem and see how to migrate it from on-premises to Google Cloud Platform. Of course you want to do that only if the migration can add value. So we will look at that as well. For a housing recommendation model, let's say our data science team already has a working on-premise model using SparkML job on your Hadoop cluster. They're interested in the scale and flexibility of GCP, and they want to do a like- for-like migration of their existing SparkML jobs from on-premises, and they want to do this as a pilot project. As explained in the previous lesson, we can get away with computing predicted ratings once a day. We don't need to compute recommendations in real time. So Hadoop batch processing is enough. Here, we'll use SparkML, but instead of doing it on-premises, we'll run the machine-learning job on Cloud Dataproc and store the ratings in an RDBMS in Cloud SQL, because this is a relatively small dataset of five recommendations for every user. So this is what you will practice in your first lab. Incidentally, this is why the smart reply in Gmail is so amazing. Unlike our housing recommendations, Spark replies' recommendations have to be done in real time, when a new e-mail pops up in your mailbox. Needless to say Gmail is doing something far more sophisticated than what we're talking about here. Here's how we decided on using Cloud SQL among the other big data products available for storing our ratings. This is a good reference to follow based on your storage access pattern. We'll cover the solutions in your other scenarios in this course, but briefly, use Cloud Storage as a global file system. Use Cloud SQL as an RDBMS as a relational database management system for transactional relational data that you access through SQL. Use Datastore as a transactional No-SQL object-oriented database. Use Bigtable for high-throughput No-SQL append-only data. So not transactions, append-only data. So a typical use case for Bigtable is sensor data for connected devices for example. Use BigQuery as a SQL data warehouse to power all your analytics needs. So here we wanted a transactional database and we expect to have data volumes in the gigabytes or less, and so hence Cloud SQL. If you're a more visual learner, here is another good map of visualizing where to store your data in GCP. If your data is unstructured, like images or audio, use Cloud Storage. If your data is structured and you need transactions, use Cloud SQL or Cloud Datastore, depending on whether you want your access pattern to be SQL or No-SQL, and by No-SQL we mean a key-value pair. In other words, you'll be trying to search for data based on a single key, use Datastore if you'd be finding data using SQL use Cloud SQL. Cloud SQL generally plateaus out at a few gigabytes. So if you wanted transactional database that is horizontally scalable so that you can deal with data larger than a few gigabytes, or if you need multiple databases so you want them spread globally use Cloud Spanner. So another way to say it as if one database is enough use Cloud SQL. If you'll need multiple databases, either because you have a lot of data or because your application needs to be transactional across different continents, use Cloud Spanner. If your data is structured and you want Analytics, consider either Bigtable or BigQuery. Use Bigtable if you need real-time high-throughput applications. Use BigQuery if you want Analytics over petabyte-scale datasets. For our housing recommendation use case, we want to store our ratings and predictions somewhere. This is a structured dataset of user ratings and houses. It's built for a transactional workload, writes and reads, and one database is enough for a small dataset and hence we pick Cloud SQL. So what's Cloud SQL? It's a Google-hosted and managed relational database in the Cloud. Cloud SQL supports two open-source databases: MySQL and Postgres, and other database solutions. In our case, we'll be using MySQL. One of the advantages of Cloud SQL is that it's familiar. Cloud SQL supports most MySQL statements and functions even stored procedures, triggers and views. It brings the benefits of Cloud economics in the form of flexible pricing. You can pay for what you use. What do we mean by that? GCP manages the MySQL Instance for you. This means things like backup and replication, it's on the Cloud so you can connect to it from anywhere. You can assign it a static IP address, and you can use typical SQL connector libraries. Because it's behind the Google firewall it's fast. You can place your Cloud SQL Instance in the same region as your App Engine or Compute Engine applications and get great bandwidth. Also, you get Google Security. Cloud SQL resides in secure Google datacenters. So that covers where we'll be storing the recommendations. But how will we compute these recommendations in the first place? Where would we be doing the compute? Let's review where big data computations have been done historically and even today. Before 2006, big data meant big databases. Database design came from a time when storage was relatively cheap and processing was expensive. So it made sense to copy the data from its storage location to the processor to perform data processing, and then the result would be copied back to storage. Around 2006, distributed processing of big data became practical with Hadoop. The idea behind Hadoop is to create a cluster of computers and leveraged distributed processing, HDFS. The Hadoop Distributed File System, stored the data on machines in the cluster, and MapReduce provided distributed processing of this data. A whole ecosystem of Hadoop- related software grew up around Hadoop, including Hive, Pig and Spark. Around 2010, BigQuery was released which was the first of many big data services developed by Google. Around 2015, Google launched Cloud Dataproc which provides a managed service for creating Hadoop and Spark clusters, and managing data processing workloads. The other piece of our system is a software that runs on Hadoop. The software in this case is to train a machine learning model for creating housing recommendations. In this case we used SparkML which is part of ApacheSpark. ApacheSpark is an open source software project that provides a high-performance analytics engine for processing batch and streaming data. Spark can be up to 100 times faster than equivalent Hadoop jobs because it leverages in-memory processing. Spark also provides a couple of abstractions for dealing with data including what are called RDDs, or Resilient Distributed Datasets and DataFrames. We'll be using our Spark job for the housing rental recommendations. We'll be using our Spark job for the housing rental recommendations, but we'll be doing the computation on Cloud Dataproc.

## 5. Challenge: Utilizing and tuning on-premise clusters

* We said we would talk about common Big Data challenges and how they're addressed. One of the most common challenges for managing on-premise Hadoop clusters is making sure they're efficiently utilized and tuned properly for all the workloads that their users throw at them. So let's check in with our team and see what challenges they're facing running the Spark ML job on the on-premise cluster. Our team has an on-premises data center running Hadoop represented by the blue box here. On any given week, they manage four different jobs for the organization. Our Spark ML recommendation job is one of the four. The data is persisted in traditional HDFS disk based storage. So here's an example scenario that may seem familiar to you. The team launches job number 1, say for argument it's a big data pipeline job which ramps up and consumes 50 percent of the available on-premise's cluster resources. Then, the team has a special request from marketing to run their prediction job for an upcoming campaign later today. That could be job number 2, and it ramps up and consumes the other 50 percent of available cluster resources. You probably see the problem now. If our Spark ML job was number 3 or number 4, it would get starved out of resources because the clusters compute capacity is under provisioned for the demands of the organization's Hadoop jobs. Or a costly alternative, is where only one job is running, and uses 50 percent of the cluster resources and the other 50 percent of machines are powered and available but they are not needed. The problem here lies in the static nature of the on-premise cluster capacity. The team needs a better way to optimize the usage of the cluster's resources without the headache of continually tuning, and adding, and removing servers themselves. This is where GCP can help. You can now think of clusters as flexible resources. With Cloud Dataproc, GCP's managed Hadoop product, you can spin up as many or as few cluster resources as your job needs in the cloud. Notice how I said cluster resources. You use them when you need to run jobs and turn them off when they're not needed anymore. So in this scenario, we can have jobs 1 and 2, run on their customized cluster number 1, and jobs number 3 and number 4 can then run on their own clusters too. Your jobs get the resources that they need. You can shut down the clusters when they're not in use. The clusters themselves become fungible resources. You can even automate when the cluster shut down, so you don't pay for resources that you're not using. You can set shutdown triggers based on how long a cluster has sat idle, or a specific timestamp, or a duration in seconds to wait, or when you submit a job, run this job and shut down. But what if the needs of a job change, and on some days we need a bigger cluster, and on some days we need a smaller cluster? Cloud Dataproc autoscaling provides flexible capacity to help you meet that need. It makes scaling decision by looking at Hadoop's YARN metrics. You can use autoscaling as long as when you shut down the cluster's node, it doesn't remove any data. So you cannot store the data on the cluster, but that's why we store our data on Cloud Storage or Bigtable or BigQuery, we store our data off cluster. So, autoscaling works as long as you don't store your data in HDFS. In addition to autoscaling, another advantage of running Hadoop clusters on GCP is that, you can incorporate preemptible virtual machines into your cluster architecture. Preemptible VMs are highly affordable, short-lived compute instances that are suitable for batch jobs and fault-tolerant workloads. Why fault tolerant? Because, preemptible machines, they offer the same machine types and options as regular compute instances, but they last only up to 24 hours and they can be taken away whenever somebody else comes along and offers up new compute needs for them. So if your applications are fault tolerant, and Hadoop applications are, then preemptible instances can reduce your Compute Engine costs significantly. How significant? Preemptible VMs are up to 80 percent cheaper than regular instances. The pricing is fixed, you get an 80 percent discount. So you always get low cost and financial predictability without taking the risk of gambling on variable market pricing. But just like autoscaling, preemptible VMs work when your workload can function without the data being stored on the cluster. You want to store that data off cluster, and that's what we will look at next.

## 6. Move storage off-cluster with Google Cloud Storage

* As you might remember from the demo earlier, separating compute and storage enables cost-effective scale for workloads. With the notion of turning down clusters, you may be worried about all the data that's currently stored on disk HDFS. You may be using HDFS on-premises. What happens to that with our new Google Cloud Platform architecture? Your data is not stored in the cluster, your data is now stored off-cluster in Google Cloud Storage buckets. Really, won't that make things slow to reach out across a network, each time the cluster needs some data? Recall that earlier we mentioned Google's data center bandwidth between compute and storage. We talked about a petabit bisectional bandwidth. What that means is that, if you draw a line somewhere in the network the bisectional bandwidth is the rate of communication at which servers on one side of the line can communicate with servers on the other side. With enough bisectional bandwidth, any server can communicate with any other server at full network speeds. With petabit bisectional bandwidth, the communication is so fast that it no longer makes sense to transfer the files and store them locally. Instead, it makes sense to use the data from where it is stored. So here's the plan. We'll use HDFS, but we'll use HDFS only on the cluster for working storage, storage during processing, but we will store all actual input and output data on Google Cloud storage. Because the input and output data is off the cluster, the cluster can be created for a single job or type of workload and can be shut down when not in use. Changing your code that works on prem to have it work with the data on cloud storage is easy. Just replace HDFS in your Spark or Big Cloud with GS. So you take the HDFS URLs, HDFS :// and replace it with GS :// This will make the Spark of big job read or write to cloud storage, that's it. There's also an H-Base connector for Cloud Bigtable. So if you have your data in Bigtable you can use the HBase connector. There's a Big Query connector that you can use to work with data, if the data is in the analytics warehouse. So when we say off-cluster storage, we're talking about cloud storage or we are talking about BigQuery or we are talking about Bigtable. Cloud Data-proc and Google cloud storage are tightly coupled with other GCP products. So storing your data off cluster, means that you can process this data not just with Spark from Data-proc but also from all these other products. Not to mention, storing it off cluster in GCS is generally cheaper, since: a, disks attached to computer instances are expensive in and of themselves and b, if your data are off cluster, you'd get to shutdown the computer nodes when you're not using them. So bottom line, store your data in Google Cloud Storage. Friends don't let friends use HDFS. So let's recap what we've covered. You can get a cloud data-proc cluster up and running in about 90 seconds. This gives you all of the power of Hadoop without having to manage clusters. As you saw, you can lift-and-shift your existing Hadoop and Spark workloads, by simply replacing HDFS:// URLs with GS:// URLs. You can connect Cloud Data-proc to Google Cloud Storage and unlock the benefits of both scale and cloud economics. You get to provision a cluster per job if you want to, and shut the cluster down when you're done. Lastly, your clusters are customizable, which could include autoscaling and preemptible VMs so that you get cost savings.


## QuizNotes

* Complete the following: You should feed your machine learning model your _______ and not your _______. It will learn those for itself!
	* data, rules
* Cloud SQL is a transaction RDBMS or relational database management system. It is designed for many more WRITES than READS. Whereas BigQuery is a big data analytics warehouse which is optimized for reporting READS.
* True or False: If you are migrating your Hadoop workload to the cloud, you must first rewrite all your Spark jobs to be compliant with the cloud.
	* False, you can run your same Spark job code running on the same Hadoop software but running on cloud hardware with Cloud Dataproc.
* You are thinking about migrating your Hadoop workloads to the cloud and you have a few workloads that are fault-tolerant (they can handle interruptions of individual VMs gracefully). What are some architecture considerations you should explore in the cloud? Choose all that apply
	* Use PVMs or Preemptible Virtual Machines 
	* Migrate your storage from on-cluster HDFS to off-cluster Google Cloud Storage (GCS)
	* Consider having multiple Cloud Dataproc instances for each priority workload and then turning them down when not in use 
* Google Cloud Storage is a good option for storing data that:
	* May be required to be read at some later time (i.e. load a CSV file into BigQuery)
	* May be imported from a bucket into a Hadoop cluster for analysis
* Relational databases are a good choice when you need:
	* Transactional updates on relatively small datasets
* Cloud SQL and Cloud Dataproc offer familiar tools (MySQL and Hadoop/Pig/Hive/Spark). What is the value-add provided by Google Cloud Platform?
	* Fully-managed versions of the software offer no-ops
		* No-ops is the main value-add here.
	* Running it on Google infrastructure offers reliability and cost savings
		* You pay only for the resources you use. Cloud SQL can be shut down when itâ€™s not being used. Hadoop clusters can be of preemptible nodes, and so on.

## Resources

[Large-Scale Deep Learning For Building Intelligent Computer Systems](https://research.google/pubs/pub44921/)
Stay up to date with the latest resources on migrating your workloads to the cloud:
[Migrating Hadoop to Google Cloud Platform](https://www.coursera.org/learn/gcp-big-data-ml-fundamentals/supplement/N74lx/module-resources)
[Cloud SQL Documentation](https://cloud.google.com/sql/) + [release blog](https://cloud.google.com/blog/products/databases)
[Cloud Dataproc Documentation](https://cloud.google.com/dataproc/) + [release blog](https://cloud.google.com/blog/products/dataproc)