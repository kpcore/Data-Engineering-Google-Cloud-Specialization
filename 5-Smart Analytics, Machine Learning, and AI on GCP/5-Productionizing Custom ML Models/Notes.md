## 1. Phases of ML Projects

* Say you've been working with the machine-learning expert who's built an important model on their computer that can help their business grow, how do we get that model into production? That's what we'll discuss in this course. First, we will provide an overview of ways to do ML on GCP. Then, we will talk about a tool, Kubeflow, for deploying machine learning models in a Kubernetes environment. Finally, we will discuss AI Hub, a repository of machine learning resources which can be made publicly available or available for only certain users. Before we get into the technology, let's first cover what steps your team will take when doing machine learning. Here's an overview of the ML development lifecycle on GCP from ingestion to model development. Note that the tools will cover later on how you actually do each step whether in a UI or with code will vary based on your level of experience and end goal. These high-level phases however will be true regardless of the tool you will use. So let's walk through each phase. First, we ingest data from some source either internal or external to GCP to start pre-processing for training. We can use tools like Google Cloud Storage, BigQuery or even something like the transfer service to bring our data into the Cloud. Next, we can prepare our data using tools such as Dataprep, Data Fusion, BigQuery, or even Dataproc. Once we've prepared and understood our data, we can apply these same tools to pre-process our data at scale. At this point, so as not to reinvent the wheel, we can explore the resources available on AI Hub to see if any relevant models or pipelines are already available. If so, this can save a lot of development time. We'll get into AI Hub later. Once we have an idea about what we want to work with, we can start developing. We can use the data labeling service to flush out the labels for our data and use AI platform notebooks to experiment in a Jupyter environment. When we're done with experimentation, we're ready to train at scale. We could do so locally using tools like Kubeflow or on the Cloud using AI platform training, which is actually built on top of Kubeflow. Using tools like TFX, we can test and analyze our model before finally deploying it to AI platform or Kubeflow.

## 2. Ways to do custom ML on GCP

* In a previous module, you leverage pre-trained ML APIs to process natural text. These are great options for seeing if your use case can just use a model that's already created and trained on Google's data. But you may want a more tailored model trained on your own data. For that, we will need a custom model. Let's talk about the different ways of building custom models. As we covered previously, here are the three ways you can do machine learning on CGP. You've already looked at pre-trained models on the right and did a few labs invoking those APIs. Now, we're going to visit the other side of the spectrum and build your own custom model and productionize it on GCP. There are a few ways of doing custom model development, training, and serving. We will highlight a few major ways and then focus on four for the remainder of this course, AI platform, CubeFlow, BigQuery ML, and AutoML. Briefly discuss each of these in turn. You worked with the notebooks component of AI platform earlier in the course when you connected BigQuery and ran commands in the notebook cells. Just as you wrote SQL in the notebooks to process data, data scientists and ML engineers can write custom TensorFlow code to train and evaluate their models right within the notebooks. Creating custom TensorFlow models in notebooks is out of scope for this data engineering course, but we do have series of dedicated TensorFlow courses for those data engineers who want to be cross-trained as ML specialists as well. For now let's highlight where are you as a data engineer are likely to be involved in Cloud AI platform projects. What is a high platform exactly? It's a fully managed service for custom machine learning models both training and serving predictions. It can scale from the experimentation stage all the way to production. You can also using features of TensorFlow include transformations on input data and perform hyper parameter tuning to choose the best model for your case. You can deploy your models to AI platform to serve predictions which will autoscale to the demands of your clients. AI platform also supports CubeFlow, which is Google's open source framework for building ML pipelines and you'll have a lab on this later. Essentially, AI platform is the engine behind doing machine learning at scale on GCP. A data scientist can train and deploy production models from an AI platform notebook with just a few commands. Getting the data ready and managing the entire pipeline of machine learning is a much broader task. As a data engineer, you will likely encounter and create CubeFlow pipelines with your ML teams. CubeFlow is an open source project that packages machine learning code for Kubernetes. ClubFlow pipelines is a platform for composing, deploying, and managing end-to-end machine learning workflows. The main components of CubeFlow pipelines include a user interface for managing and tracking experiments, jobs, and runs, an engine for scheduling multi-step ML workflows, an SDK for defining and manipulating pipelines and components, notebooks for interacting with the system using the SDK. These tools are used to define, experiment with, run and share pipelines. The pipeline consists of pipeline components which are ML steps that are assembled into a graph that describes the execution pattern. The key benefits are reuseability and portability. You can run it on GCP or other Cloud providers, so you're not locked in. Before we dive more into CubeFlow, let's see the other two ways of building custom models on GCP which will be the focus of future modules, BigQuery ML and AutoML. BigQuery ML allows your team's to easily build machine learning models on structured data using SQL-like syntax. You can quickly get a model created for forecasting, classification, and recommendations right where your data already lives in BigQuery. Teams use BigQuery ML as a quick prototyping tool to see if machine learning will work for their data set and project. Then you can dive into more of the advanced features of BigQuery ML like hyper parameter tuning and data set splitting methods to fine-tune your models performance. We won't dive into the details here, but we'll see how we can easily create, evaluate, and then make predictions on our data all in BigQuery. It's just SQL commands like create model or evaluate, or predict just as you would write a normal SQL query. You'll do labs on this later. Lastly, another option for custom model building is AutomML. Assuming we have labeled training data, we can train, deploy, and serve predictions using AutoML without having to write any code. How do we generate predictions with an easy to use REST API? And again, since we're using AI platform and CubeFlow, we will often be thinking about using TensorFlow models. However, this isn't the course to dive into the details of TensorFlow, an example of where you can learn more about this is the intro to ML on GCP specialization on Coursera linked here.

## 3. Kubeflow

* Where do data engineers come into the picture? Don't forget data engineers build data pipelines, and machine learning pipelines are no different. If we want to have a flexible pipeline for all stages of machine learning, Kubeflow is a great option. Many people think that machine learning products are all about the code that ML scientists write locally on their machines. Does this code ensure that data going into it is clean? Can the code autoscale to clients who want to use it for serving predictions? What if we have to retrain the model, does it go off line at that point? The truth is, Production Machine Learning systems are large, complicated distributed system. There's a lot of DevOps involved with things like monitoring and processing management tools. Google started building Kubeflow to tackle these DevOps challenges using Kubernetes and Containers. One option to help manage the overhead of productionizing ML pipelines is to use Kubeflow. The capabilities provided by Kubeflow Pipelines can largely be put into three buckets: ML workflow orchestration, share, reuse, and composed, rapid reliable experimentation. You can think of the benefits as similar to those of Cloud composer but better tailored for ML workloads. Let's see what a pipeline looks like. To make things more concrete, let's look at a screenshot of an illustrative workflow that was run on Kubeflow Pipelines. This is just an illustrative workflow, and users can author and run many different kinds of workflow topologies with different code and tools in the various steps of the workflow. For each workflow that is run on Kubeflow Pipelines, you get a rich visual depiction of that topology so that you know what was executed as part of the workflow. In this workflow, we start with a data preprocessing and validation step. The preprocessed data then flows to a feature engineering step. This is followed by a fork where we train many different kinds of models in parallel, like a wide and deep model, and XG boost model, and a convolutional neural network or CNN. During training, you can click on the model for UI to view critical performance characteristics in the model. Here we can see the ROC curve of false positive rates versus true positive rates for the model during training. If you're familiar with TensorBoard, you can also view the TensorBoard metadata for the model as well. Once training is complete, the models that are trained are then analyzed and compared against each other on a test dataset, and you can choose from which one performed the best for your use case. Most ML products will stop here and iterate back to the beginning to continue improving model performance before moving to production. Finally, once you're happy with model performance, you can have a Kubeflow node, push it to production serving endpoint. For each step of the workflow, you can see the precise configuration parameters, inputs, and outputs. Thus, for a model trained with Kubeflow Pipelines, you never have to wonder how exactly did I create this model. Here you can quickly see how long the model training took, where the train model is, and what data was used for training and evaluation. You can define the ML workflow using Kubeflow as Python SDK. By defining the workflow, we mean specifying each steps; inputs and outputs, and how the various steps are connected. The topology of the workflow is implicitly defined by connecting the outputs of an upstream step to the inputs of a downstream step. You can also define looping constructs as well as conditional steps. Another nice Kubeflow feature is the ability to package pipeline components. This adds an element of portability since you can then move your ML pipelines even between Cloud providers. Kubeflow Pipelines separate the work for different parts of the pipeline to enable people to specialize. For example, a ML engineer can focus on feature engineering and the other parts of creating the model such as hyperparameter tuning. The ML engineer solutions can then be bundled up and used by data engineer as part of a data engineering solution. The solution can then appear as a service used by data analysts to derive business insights. Kubeflow makes it easy to run a number of ML experiments at the same time. For example, if you're doing hyperparameter optimization, you can easily deploy a number of different training instances with different hyperparameter sets. Kubeflow's run overview makes it easy to hone in on the techniques or parameters generating the best results. You can quickly identify what works and what did not work.

## 4. AI Hub

* We mentioned that Kubeflow pipelines can be packaged and shared with other users. This leads us to a discussion of AI Hub. AI Hub is a repository for ML components. Don't reinvent the wheel. Avoid buildings some component when someone else has already built it, and most likely has already optimized it. You can find and deploy not just containerized applications for machine learning, but also full ML pipelines on AI Hub. What asset types can we find on AI Hub? Among the assets stored on AI Hub are entire Kubeflow Pipelines, Jupyter notebooks, TensorFlow modules, fully trained models, services and VM images. Here you see what a typical asset looks like. You can see information about the pipeline such as inputs and outputs and download options. The assets on AI Hub are collected into two scopes; public assets and restricted assets. Public assets are available to all AI Hub users. Restricted scope assets contain AI components you have uploaded and those that have been shared with you. For example, you could have assets only available to people within your organization or team.

## 5. Summary

* To summarize, Google Cloud Platform has several options to suit your machine-learning needs. Depending on the time and resources you have available, you have the option to use AI platform, AutoML, or Perception APIs. You can use Kubeflow to deploy end-to-end ML pipelines and remember don't reinvent the wheel for your ML pipeline, leverage pipelines on AI Hub.

## QuizNotes

* Which technology was developed to attack DevOps challenges in ML using Kubernetes and containers?
	* Kubeflow
* AI Hub has templates for which of the following?
	* Kubeflow pipelines and components
	* Jupyter notebooks
	* Trained models