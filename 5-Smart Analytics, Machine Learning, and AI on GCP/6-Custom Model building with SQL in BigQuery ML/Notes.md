## 1. BigQuery ML for Quick Model Building

* BigQuery Machine Learning allows you to build machine learning models using SQL syntax. This is a really powerful feature since anyone who knows only SQL can build ML models. BigQuery ML brings us one step closer to democratize and Machine Learning and AI, one of our core missions here at Google. First, we'll introduce BigQuery ML and walk through the steps for an end to end use case. We'll then discuss the current [inaudible] of support and models. At the end, you'll work through a couple of labs, so you can get your hands on BQML. Why does BQML fit into the greater picture of Google Cloud's AI and ML options. Well, unlike the ML APIs, you are able to create your own custom models. We'll get to auto ML in the next course but in short it allows us to leverage Google's ML models in certain cases to build our own models from scratch using transfer Learning and a form of neural architecture search. To work with BQML, there are only a few steps from beginning to inference. First, we must write a query on data stored in BigQuery to extract our training data. Then, we can create a model where we specify a model type and other hyperparameters. After the model is trained, we can evaluate the model and verify that it meets our requirements. Finally, we can make predictions using our model on data extracted from BigQuery. Are you familiar with Hacker News? Hacker News is a social news aggregator website focused on computer science and entrepreneurship. We won't get into all the details, but let's ask a question. Given these three articles on the right, where were they actually published? TechCrunch, GitHub or New York Times. Of course, we may be able to tell based on the style of the articles. But what if we wanted to build an ML model to do this? First, let's write an Ad-hoc Query to explore our data. Here's a quick example where we're simply looking at the title of the article and the URL. We can extract the publisher information from the URL using regular expression functions. That's exactly what we'll do using the query on this slide. Our plan will be the following. Let extract the publication source using a regexextract function and then separate out the first five words at each title or replaced them with null values, in the case the title is too short. Our goal will then be to decide if the article title is from GitHub, New York Times or TechCrunch using BigQuery ML. Let's look at the syntax needed to create a model. We have the create or replace model statement at the top where we give the name of the model, the options of the model, and other hyperparameters. After the "as" clause, we have our query which defines the training set from the previous slide. That's it. In this case, we're specifying that we want to build a logistic regression model. This is a type of classification model we can use to classify our input as being either a GitHub, New York Times or TechCrunch article. Once we have a trained model, we want to know how it performs. Since it's a classification model, we can use a variety of metrics to see how well it knew where the article originated based on just the title. To get those metrics, you simply call ML dot evaluate on your train model or click on the model and the BigQuery UI and click the evaluation tab. For the metrics, they're on a score from zero to one. Generally speaking, the closer to one, the better, but it really depends on your metric. Precision means for the articles we made a guess on, how accurate were those guesses. High precision means a low false positive rate, meaning we really punish a model's precision if it makes a ton of bad guesses. Recall on the other hand is the ratio of correctly predicted positive observations to all observations in the actual class. How many did we get right out of both true positives and false negatives? Accuracy is simply true positives plus the true negatives over the entire set of observations. There are other metrics like, f1-score, Log-Loss and ROC curves that you can use as well. A link out to a description of each and when you should use which metric in your classification models. We can also see the confusion matrix, where we can see where the model made incorrect predictions. As we can see here, our model had some confusion between TechCrunch and New York Times articles. If the model meets our requirements, great, we're ready to predict with our model. We don't need to worry about deploying our model in a separate process. It's automatically available to serve predictions here in BigQuery. Here's an example of performing batch prediction using BQML. The first example here has government, shutdown, leaves, workers, and reeling as the first five words of the title. The model's prediction in this case is New York Times

## 2. Classification, Regregression, and Recommender Models

* Before we move into our first lab on BigQuery ML, let's give a brief overview of the various model types that are currently supported. For the article prediction task in the previous section, recall that we utilized a logistic regression model. There are other models that could have been used. Here are the model options available in BigQuery ML to classify things. First, we have another example of linear classification or what is also called logistic regression. In this example, we're using a flights arrivals data set to predict whether a flight would be on time or not, which is a binary outcome. For logistic regression, we simply need to specify the type of model and the label we're trying to predict. For more advanced users, there are other options such as if you want your model to use regularization. While logistic regression models are the Swiss army knife of machine learning, deep neural networks or DNN's allow you to better model nonlinear relationships in your data. An example of a non-linear relationship would be in a car depreciation. A car loses a vast majority of its value within the first few years after which the value more or less stabilizes. We won't go into the details of DNN's in this course, but know that you can use them in BQML. All you have to do is specify DNN as the model type in the create or replace model header. This slide shows how to do that for the same flight arrival time example used in the previous slide. If you're familiar with gradient boosted trees and XGBoost, you can use this algorithm to train models in BQML. In this example, we specify the model type as boosted tree classifier to use gradient boosted trees as our classifier. There are a number of hyperparameters you can select for this model, including the maximal tree depth. Again, we won't go into fine detail here, but the idea behind gradient boosted trees is to iteratively fit a decision tree to the residuals of preceding decision trees. Here are the modal options available in BigQuery ML to forecast things like numeric values. So far, we've only talked about classification. You can also use BQML to do regression. In this example, we're fitting a linear regression model to predict taxi fare based on features such as the hour of day, pick-up and drop off location, and the day of the week. If you need a model more complex than linear regression, you can use a DNN regressor in BQML. All you have to do is specify the model type as DNN regressor and define the hidden units, just like in the classification version of DNN's. Just like for classification, we can also use XGBoost for regression. Just specify you want to use a boosted tree regressor. What if you have already trained a model on TensorFlow? Well, you can import your saved model into BigQuery to serve batch predictions. In this example, the first statement is where we load our model into BQ. We specify the model name, the model path in Cloud Storage, and we tell BigQuery that the model type is TensorFlow. When we want to serve predictions, we can do it as shown on the bottom half of the query in this example. Notice the ML.PREDICT statement being used to generate predictions and then that actual query being used to print the predictions in a reasonable format. Here are the model options available in BigQuery ML to recommend things based on ratings. We can also serve recommendations to users in BQML. The type of model being used for this is called a matrix factorization model. In short, matrix factorization models are a form of collaborative filtering algorithms used in recommender systems. Matrix factorization algorithms work by decomposing the user item interaction matrix. That is, the matrix consisting of users and recommendations gets broken apart into the product of two lower dimensionality rectangular matrices. This matrix will be really sparse, since users will only review a few products. For example, if we have one million movies on our streaming platform, you don't expect a user will review more than a handful. For this reason, we expect our matrices to be low rank and thus can store the information in smaller matrices without much loss of information. The full details of this can be found in many different online sources for recommendation systems. This family of methods became widely known during the Netflix Prize challenge due to its effectiveness in making recommendations. To build a recommendation engine in BQML, you must set the model type to matrix factorization and provide a table of user item interactions. Once we have a trained recommender model, we can use it to serve recommendations. Here's an example query where we're getting our users and products with the query and getting the suggested products using the ML.PREDICT statement. Here's what the output will look like for a single user. We could sort by the predicted rating for each user ID and give the top five or so recommendations using a simple SQL query.

## 3. Unsupervised ML with Clustering Models

* Let's wrap up with unsupervised machine learning. Supervised is when you have labeled training data, meaning you know the answer in the past and unsupervised is where you are using no such label. Here are the model options available in BigQuery ML to explore our data using unsupervised machine learning when we have sparse or no labeled data. Finally, we will discuss an unsupervised learning algorithm in K-means clustering that's available in BQML. We won't go into the details of the algorithm but, in short, in this case, we do not have a label we are trying to predict, rather we are trying to explore patterns in our data. In this example, we will cluster the data into four clusters, standardize the features to account for different feature value ranges which is really important for clustering, and remove any ID fields so that we're not grouping based on uniquely identifying information. After we've trained a clustering model, we can easily see which cluster our data belongs to with a quick SQL query. Here's an example of the query to do so. The centroid ID column refers to the cluster assigned to each row of data. The data set used here is for a bike rental company that has stations spread out across London. Once you have clusters, how do you actually get utility out of them? The key is to do some analysis on the data in each cluster. We can explore feature ranges and aggregate statistics to try to understand what is special about the individual clusters. For example, we could use Data Studio to visualize some of the attributes of the data in the clusters. Here we see that cluster one has the longest duration rights. Using Transform ensures that the raw inputs expected from clients will be transformed to avoid training and serving skew at prediction time. On the left we see that the transformations are done in the query defining the training data set. In this case, the user of the model would have to perform these same transformations. However, how would they know what transformations to perform in general and should this even be a worry on the client side? We instead move the casting into a transform statement. Now when the client sends in a prediction, the transformations on the start date field will be automatically applied before serving a prediction. We've covered a lot of syntax in BQML. There's syntax for training a model, evaluating it, and making predictions, among other things. Use this cheat sheet to help you when you first start building models in BQML.

## 4. Summary

* To summarize what we have learned in this course, the key takeaway here is that you can build powerful machine learning models in BigQuery. If your data lives in BigQuery, this is a very useful functionality since you don't have to worry about moving your data elsewhere to train your model. 

## QuizNotes

* You can train and evaluate machine learning models directly in BigQuery.
	* True
* BigQuery ML has support for which of the following modeling tasks:
	* Classification
	* Regression
	* Clustering