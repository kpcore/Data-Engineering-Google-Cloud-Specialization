## 1. Explore the role of a data engineer

* In this module we will look at the job of a data engineer. So who's a data engineer? A data engineer is someone who builds data pipelines. And so we'll start by looking at what this means, what kind of pipelines a data engineer builds and what the purpose of these pipelines is. Having looked at the task of a data engineer we will look at why we suggest that you do data engineering in the cloud, on Google Cloud platform to be specific. We will look at the challenges associated with the practice of data engineering and how many of these challenges are easier to address when you build your data pipelines in the cloud. Then we will look at what the goal of the data pipelines that you build is. If you build them correctly, what kind of things do you enable in the organizations that you're building these data pipelines for? Finally, we will look at a few example reference architectures. Reference architectures are architectures that you adapt. And in order to do that you will have to know the purpose of each of the pieces of that data architecture. So we will look at the component products in greater detail in the specialization. But here we start with a high-level view of the purpose of the various products. What does the data engineer do? As we mentioned a data engineer builds data pipelines. But why? Why does the data engineer build data pipelines? Because they want to get their data into a place such as a dashboard or a report or a Machine Learning model from where the business can make data-driven decisions. The point of a data pipeline is to make data-driven decisions. But in order to be able to do that that data has to be in a usable condition so that someone can use this data to make their decisions. Many times the raw data is by itself not very useful. One term, you will hear a lot when you do data engineering is a concept of a Data Lake. A data lake brings together data from across the Enterprise into a single location. So you might get the data from a relational database or from a spreadsheet and store the raw data in a data lake. One option for this single location to store the raw data is to store it in a cloud storage bucket. So what are some of the key considerations when deciding between different data lake options? What do you think? What are some of the key considerations you need to keep in mind as you build a data lake? One, does your data lake handle all the types of data that you have? Can it all fit into a cloud storage bucket? If you have an RDBMS, a relational database you might need to put the data not in a cloud storage bucket but in Cloud SQL, a manage database. Two, can it elastically scale to meet the demand? As your data collected increases will you run out of disk space? Of course, this is more of a problem with on-premises systems than with cloud. Third, does it support high throughput ingestion? What is a network bandwidth? Do you have Edge points of presence? Four, is there fine-grained access control to objects? Do users need to seek within a file? Or is it enough to get a file as a whole? Cloud storage is blob storage. So you might need to think about the granularity of what you store. And fifth can other tools connect easily? How do they access the store? Don't lose sight of the fact that the purpose of a data lake is to make data accessible for analytics. We mentioned our first Google Cloud product the cloud storage bucket, which is a good option for staging all of your raw data in one place before building transformation pipelines into your data warehouse. But why choose Google Cloud Storage? Commonly businesses use cloud storage as a backup and archival utility for their businesses. Because of Google's many data center locations and high network availability, storing data in a GCS bucket is durable and performed. As a data engineer, you will often use a cloud storage bucket as part of your data lake to store many different types of raw data files, CSV, JSON, Avro, parquet, etc. You could then load or query them directly from BigQuery, which is a data warehouse. Later in the course you will create cloud storage buckets using the console and the command line as you see here. Other Google Cloud platform products and services can easily query and integrate with your bucket once you've got it set up and loaded with data. Speaking of loading data, what if your raw data needs additional processing? You may need to extract the data from its original location transform it and then load it. One option is to carry out the data processing on something like Cloud Dataproc or Cloud Dataflow. We'll discuss using these products to carry out batch pipelines later in this course. But what if batch pipelines are not enough? What if you need real-time analytics on data that keeps arriving continuously and endlessly? In that case you might receive the data in cloud pub/sub, transform it using cloud dataflow and stream it into BigQuery. We'll discuss streaming pipelines also later in this course.

## 2. Analyze data engineering challenges

* Let's look at some of the challenges that a data engineer faces. As a data engineer, you will usually really encounter a few problems when building data pipelines. You might find it difficult to access the data that you need. You might find that the data even after you can access it, doesn't have the quality that is required by the analytics or machine learning model. You might plan to build a model and even if the data quality exists, you might find that the transformations require computational resources that may not be available to you. Finally, you might run into challenges around query performance and being able to run all of the queries and all of the transformations that you need with the computational resources that you actually have. Let's take the first challenge. This is the challenge that you need to consolidate different datasets, different data formats, and to manage access to your data at scale. For example, you want to compute say a customer acquisition cost. How much does it cost in terms of marketing and promotions and discounts to acquire a customer? Now, that data might be scattered across a variety of different marketing products and customer relationship management software and so on, and finding a tool that can analyze all of this data might be difficult because it comes from different organizations, it comes from different tools, it has different schemas, and maybe some of that data is not even structured. So in order to find something as essential to your business as how much getting a new customer costs, so you can figure out what kind of discounts to offer to keep them from leaving, you can't have your data exist in these kinds of silos. So what makes data access so difficult? Primarily, it's because of this problem. The fact that data in many businesses is siloed department by department and each department creates its own transactional systems because they want to support their own business processes. So for example, you might have operational systems that correspond to store systems at different operational system that's maintained by a product warehouses that manages your inventory. Then you might have a marketing department, and the marketing department may manage all the promotions. Now, suppose you need to do an analytics query, a query such as give me all the in-store promotions for recent orders, and also tell me how many items we have in the inventory. So the query is give me all the in-store promotions for recent orders and their inventory levels. Given such a query, you need to know how to combine data from the stores, data from the promotions, data from the inventory levels. Because these are all stored in separate systems and usually some of these systems will have restricted access, building an analytic system that uses all three of these data sets to answer an ad hoc query like this, it can be quite difficult. The second challenge is that cleaning, formatting, and getting the data ready for insights requires that you build extract transform load or ETL pipelines. ETL pipelines are usually necessary to ensure data accuracy and quality. The cleaned and transformed data are typically stored not in a data lake, but in a data warehouse. A data warehouse is a consolidated place just like a data lake, it's a consolidated place. But this time, the data that we're storing is all easily joinable and queryable. Unlike a data lake where the data is in a row format in the data warehouse, the data is stored in a way that makes it very efficient to query. Because data becomes useful only after you clean it up, you should assume that any raw data that you collect from source systems has to be cleaned and has to be transformed. If you're going to be transforming it, you might as well transform it into a format that makes it efficient to query. In other words, ETL the data and store it in a data warehouse. So let's say you're a retailer and you have to consolidate data from multiple source systems. Then think about what the use case is, and if the use case is to get the best-performing in-store promotions in France, then you need to get the data from the stores and you have to get that data from the promotions, and then maybe you figure out that the store data is missing some information. Maybe some of the transactions are in cash, and for those cash transactions perhaps there's no information on who the customer is. Or some transactions might be spread over multiple receipts or you might need to combine these transactions because they come from the same customer. Or perhaps the timestamps of the products are stored in local time. Whereas you have to spread all across the globe, and so before you can do anything, you need to convert everything into UTC. Similarly, the promotions may not be stored in the transaction database at all. They might be just a text file that somebody loads on the webpage and it has a list of codes that are used by the web application to apply discounts. It can be extremely difficult to do a query like performing the in-store promotions, getting the best-performing in-store promotions, because the data as we talked about has so many unique problems. Whenever you have data like this, you need to get the raw data and transform it into a form with which you can actually carry out the necessary analysis. It's obviously best if you can do this clean up and consolidation just to once, and store the resulting clean data to make further analysis easy, and that is a point of a data warehouse. If you need to do so much consolidation and clean up, a common problem that arises is, where do I carry out this compute? The availability of computational resources can itself be a challenge. Why? If you're on an on-premises system, data engineers will need to manage server and cluster capacity and make sure that enough capacity exists to carry out the ETL job at the time that you need the ETL job to be finished. The problem is that the compute that's needed by any specific ETL job is not constant over time. Very often, it varies week to week and depending on factors like holidays and promotional sales. This means that when traffic is low, you're going to be wasting money because you have computers out there doing nothing, and when traffic is high, those computers are so busy that your jobs are taking way too long. Once your data is in your data warehouse, you need to optimize the queries that your users are running to make the most efficient use of your compute resources. This is difficult. If you're managing an on-premise data analytics cluster because you will be responsible for choosing a query engine and installing the query engine software and keeping it up-to-date as well as provisioning more servers for additional capacity. Isn't there a better way to manage server overhead, so that we as a business can focus on insights?

## 3. Intro to BigQuery

* There totally is a better way to manage server overhead so that we can focus on insights. That better way is to use a Serverless Data Warehouse. BigQuery is Google Cloud's petabyte scale Serverless Data Warehouse. You don't have to manage clusters. Just focus on insights. The BigQuery service replaces the typical hardware setup for a traditional data warehouse. That is, it serves as a collective home for all the analytical data in an organization. Datasets are collections of tables that can be divided along business lines or a given analytical domain. Each dataset is tied to a GCP project. At data lake, might contain files in Cloud Storage or Google Drive or transactional data in Cloud Bigtable or Cloud SQL. BigQuery can define an external schema and issue queries on that external data as a federated data source. Database tables and views function the same way in BigQuery as they do in a traditional data warehouse allowing BigQuery to support queries written a standard SQL Dialect that is NC 2011 compliant. Cloud Identity and Access Management, or Cloud IAM is used to grant permission to perform specific actions BigQuery. This replaces the SQL grant and revoke statements that are used to manage access permissions in traditional SQL databases. A key consideration behind agility is being able to do more with less, and it's important to make sure that you're not doing things that don't add value. If you do work that's common across multiple industries, it's probably not something that your business wants to pay for, it's something that you want to buy. The Cloud lets you, the Data Engineer, spend less time managing hardware and more time doing things that are much more customized and specific to your business. You don't have to be concerned about provisioning and reliability and utilization improvements in performance or tuning on the Cloud. Because you don't have to be worried about all of those things, you can spend your time thinking about how to get better insights from your data. We said, you don't need to provision resources before using BigQuery unlike many RDBMS systems. How does that work? BigQuery allocates storage and query resources dynamically based on your usage patterns. Storage resources are allocated as you consume them, and deallocated as you remove data or you drop tables. Query resources are allocated according to the query type and complexity. Each query uses some number of what are called slots. Slots are units of computation that comprise a certain amount of CPU and RAM.

## 4. Data Lakes and Data Warehouses

* We've defined what a data lake is and what a data warehouse is. Let's look at these in a bit more detail. Recall that we emphasized that the data has to be in a usable condition so that someone can use this data to make decisions. Many times the raw data is by itself not very useful. We said that raw data gets replicated and stored in a data lake. In order to make the data usable, you will use the extract transform load or ETL pipelines to make the data usable, and store this more usable data in a data warehouse. Let's consider what are the key considerations when deciding between different data warehouse options. We need to ask ourselves these questions. The data warehouses going to definitely serve as a sink. You're going to store data in it. But will the data warehouse be fed by a batch pipeline or by a streaming pipeline? Does a warehouse need to be up-to-the-minute correct or is it enough to load data into it once a day or once a week? Will the data warehouse scale to meet my needs? Many cluster-based data warehouses will set per cluster concurrent query limits. Will those query limits cause a problem? Will the cluster size be large enough to store and traverse the data that I have? How is the data organized? How is it cataloged? How is it access controlled? Will you be able to share access to the data to all your stakeholders? What happens if they want to query the data? Who will pay for the querying? Is the warehouse design for performance? Again, carefully consider concurrent query performance, and whether that performance comes out of the box, or whether you ended go around creating indexes and tuning the data warehouse. Finally, what level of maintenance is required by your engineering team? Traditional data warehouses are hard to manage and operate. They were designed for a batch paradigm of data analytics and for operational reporting needs. The data in the data warehouse was meant to be used by only a few management folks for reporting purposes. BigQuery is a modern data warehouse that changes this conventional mode of data warehousing. Here, we can see some of the key comparisons between a traditional data warehouse and BigQuery. BigQuery provides mechanisms for automated data transfer and powers business applications using technologies like SQL that teams already know and use. This way, everyone has access to data insights. You can create read-only shared data sources that both internal and external users can query and make query results accessible for anyone through user-friendly tools such as Google Sheets, Looker, Tableau, Qlik, or Google Data Studio. BigQuery lays the foundation for artificial intelligence. It's possible to train TensorFlow and Google AI platform models directly with dataset stored in BigQuery, and BigQueryML can be used to build and train Machine Learning models using simple SQL. Another extended capability is BigQuery GIS, which allows organizations to analyze geographic data in BigQuery, and this is essential to many critical business decisions that revolve around location data. BigQuery also allows organizations to analyze business events real time as they unfold by automatically ingesting data and making it immediately available to query in the data warehouse. This is supported by the ability of BigQuery to ingest hundreds of thousands of rows of data per second and for petabytes of data to be quarried at lightning-fast speeds. Due to Google's fully-managed, serverless infrastructure and globally available network, BigQuery eliminates the work associated with provisioning and maintaining a traditional data warehousing infrastructure. BigQuery also simplifies operations through the use of IAM to control user access to resources creating roles and groups and assigning permissions for running jobs and queries in a project and also provides automatic data backup and replication. Even though we talked about getting data into BigQuery by running ETL pipelines, there is another option. That is to treat BigQuery as just a query engine and allow it to query the data in the data lake, data in place. For example, you can use BigQuery to directly query database data in Cloud SQL, that is managed relational databases like PostgreSQL, MySQL, and SQL Server. You can also use BigQuery to directly query files and Cloud Storage as long as those files are in formats like CSV Apache. The real power comes when you can leave your data in place and still join it against other data in the data warehouse.

## 5. Transactional Databases vs Data Warehouses

* Data Engineers may be responsible for both the backend transactional Database systems that support your company's applications and the Data Warehouses that support your analytic workloads. In this module we'll explore the differences between databases and data warehouses and the Google Cloud solutions for each of these workloads. If you have SQL Server, MySQL, Postgres as your relational database, you can migrate it to Cloud SQL, which is Google Cloud's fully managed relational database solution. Cloud SQL delivers high performance and scalability with terabytes of storage capacity and tens of thousands of IOPS, and the 400 500 gigs of RAM per instance. You can take advantage of storage autoscale to handle growing database needs with zero downtime. One question you might get asked is, why not simply use Cloud SQL for reporting workflows? You can run SQL directly on the database, right? That is a great question and we'll discuss it in much more detail in our data warehouse module. For now, let's just say that loud SQL is optimized to be a database for transactions, for rights, and BigQuery is a data warehouse that is optimized for reporting work clothes which consists mostly of reads. The fundamental architecture of the data storage options is very different. Cloud SQL databases are record based storage, meaning the entire record must be opened on disk. Even if you just selected a single column in your query, BigQuery is column based storage, which as you might guess, allows for really wide reporting schemas since you can simply read individual columns out from disk. This is not to say that RDBMSs aren't as performant as data warehouses. What we are saying is that they serve two very different purposes, a relational database helps your business manage transactions, take this point of say terminal at of storefront. Each order and product ordered is likely written out as a new record in a relational database somewhere. That database might store all the orders received from their website, all the products not in the catalog or the number of items in their inventory. This is so that when an existing order is changed, it can be quickly updated in the database. Transactional systems allow for a single row in the database table to be modified in a consistent way. They're also built on relational database principles, like referential integrity, to guard against cases like a customer ordering a product that doesn't exist in a product table. So where does all this raw data end up in our data lake and data warehouse discussion. What's a complete picture? Here it is. Our operational systems like a relational databases that store online orders, inventory and promotions are our raw data sources on the left. Note that this is inexhaustive, you could have other source systems that are manual, like CSV files or spreadsheets too, these upstream data sources get gathered together in a single consolidated location, and that's what we call that data lake. The data Lake is designed for durability and high availability. Once in the data lake, the data often needs to be processed through transformations that output the data into a data warehouse where it is ready for use by downstream analytics teams. So here are three quick examples of the kind of analytics teams that often build pipelines that get data from our data warehouse. You might have a machine learning team who wants to build a pipeline to get features for their models. You might have an engineering team, another data engineering team that's using our data as part of their data warehouse, or you might have a business intelligence team that wants to build dashboards off some of our data. So who works on these teams and how do they partner with our data engineering team?

## 6. Partner effectively with other data teams

* Since the data warehouse serves other teams also, it is crucial to learn how to provide access to the data warehouse while keeping to data governance best practices. Remember that once you've got data where it can be useful and it's in a usable condition. We need to add value to the data through Analytics and Machine Learning, and that's typically done by other teams. So these teams will rely on our data. There are many such teams that rely on your data warehouse and partnerships with data engineering teams so that they can build and maintain new data pipelines. The three most common clients are Machine Learning engineers, data or business analysts and other data engineers. Let's examine how each of these roles interacts with your data warehouse and how each data engineers can best partner with them. As you will see in our course on Machine Learning, a Machine Learning teams models rely on having lots of high quality input data to create, train, test, evaluate and serve their models. They'll often partner with data engineering teams to build pipelines and data sets that they can use in their models. Two common requests that you might have to fill are how long does it take for a transaction to make it from raw data all the way into the data warehouse? What the Machine Learning team is asking for is that they want the data that they train their models on to be available at prediction time as well. If there is a long delay in collecting and aggregating the raw data. It will impact the Machine Learning team's ability to create useful models. A second question that you will definitely get asked is how difficult it would be to add more columns or more rows of data into certain datasets? Again, the Machine Learning team relies on teasing out relationships between columns of data and having a rich history to train models on. You will earn the trust of your partner Machine Learning teams by making your data sets easily discoverable, documented and available to them to experiment on quickly. A unique feature of BigQuery is that you can create high-performing Machine Learning models directly in BigQuery using just SQL by using BigQuery ML. Here is the actual model code to create a model, evaluate it and then make predictions. You will see this again in our lectures on Machine Learning later on. Another critical stakeholder is your business intelligence or data analyst teams. These teams rely on good clean data so that they can query it for insights and build dashboards. These teams need datasets that have clearly defined schema definitions, the ability to quickly preview rows, and the performance to scale too many concurrent dashboard users. One of the Google Cloud products that helps manage the performance of dashboards is BigQuery BI Engine, which is in beta at the time we recorded this. BI Engine is a fast in memory analysis service that is built directly into BigQuery and available to speed up your business intelligence applications. Historically, BI teams would have to build, manage and optimize their own BI servers and what are known as OLAP cubes to support reporting applications. Now, with BI Engine you can get sub-second query response time on your BigQuery datasets without having to create your own cubes. BI Engine is built on top of the same BigQuery storage and compute architecture and serves as a fast in memory intelligent caching service that maintain state. One last stakeholder of your data engineering team is other data engineers that rely on the uptime and performance of your data warehouse and pipelines for their downstream data lakes and data warehouses. They will often ask how can you ensure that this data pipeline that we depend on will always be available when we need it? Or they might ask we're noticing high demand for certain really popular datasets, can you monitor and scale the health of your entire data ecosystem so we know that the data is available whenever we needed? One popular way to monitor the health of your ecosystem is to use built-in stackdriver monitoring of all resources on Google Cloud platform. Since Google Cloud Storage and BigQuery are resources, you can set up alerts and notifications for metrics like query count or bytes of data processed, so that you can better track usage performance and cost. Another reason why you might use stackdriver is so that you can track spending of all the different resources used and what the building trends are for your team or organization. You can even use stackdriver to create cloud audit logs to view actual query job information and look at granular level details about which queries were run and who ran them. This is particularly useful if you have sensitive datasets that you need to monitor closely. And that's a topic that we will discuss later.

## 7. Manage data access and governance

* As part of being an effective partner, your engineering team will be asked to set up data access policies. And overall governance of how data is to be used and not used by your users. This is what we mean when we say a data engineer must manage the data. This includes critical topics, like privacy and security, so what are some key considerations when managing certain data sets? Clearly communicating a data governance model for who should access and who should not be able to access. How is personally identifiable information, PII information, like phone numbers or email addresses handled. Even more basic tasks, how will our end users discover different data sets that we have for analysis? One solution for data governance is Cloud Data Catalog and the Data Loss Prevention API. Data Catalog makes all the metadata about your data sets available for users to search. You group data sets together with tags, flag certain columns at sensitive, etc, and why is this useful? If you have many different data sets with many different tables, which different users have different access levels to. Then Data Catalog can provide a single unified user experience to discover those data sets quickly. No more hunting for specific table names in SQL first. Often used in conjunction with the Data Catalog is the Data Loss Prevention or DLP API. This helps you better understand and manage sensitive data. It provides fast scalable classification and reduction for sensitive data elements, like credit card numbers, names. Social Security numbers, selected international identifier numbers, phone numbers, GCP credentials, etc. Let's take a quick look at this API.

## 8. Build production-ready pipelines

* Once your data lakes and data warehouses are setup and your governance policy is in place, it's time to productionize the whole operation and automate and monitor as much of it as we can. That's what we mean when we say, "Productionize the data process," it has to be an end-to-end and scalable data processing system. Your data engineering team is responsible for the health of the plumbing, that is the pipelines and ensuring that the data is available and up-to-date for analytic and ML workloads. Common questions that you should ask at this phase are; How can we ensure pipeline health and data cleanliness? How do you productionize these pipelines to minimize maintenance and maximize uptime? How do we respond and adapt to changing schemas and business needs? Are we using the latest data engineering tools and best practices? One common workflow orchestration tool used by enterprises is Apache Airflow and Google Cloud has a fully managed version of Apache Airflow called Cloud Composer. Cloud Composer helps your data engineering team orchestrate the pieces to the data engineering puzzle that we discussed to date, and even more that we haven't come across yet. For example when a new CSP file gets dropped into Cloud storage, you can automatically have that trigger an event that kicks off a data processing workflow and puts the data in the CSP file directly into your data warehouse. The power of this tool comes from the fact that GCP big data products and services have API endpoints that you can call. A Cloud Composer job can then run every night or every hour and kickoff your entire pipeline from broad data to the data lake and into the data warehouse for you. We'll discuss workflow orchestration in greater detail in later modules, and you'll do a lab on Cloud Composer as well.

## 9. Review GCP customer case study

* We have looked at a lot of different aspects of what a data engineer has to do. Let's look at a case study of how the Google Cloud customer solves a specific business problem. This will help tie all these different aspects together. Ocado is the world's largest online grocer. Ocado's customer service department receives a large number of messages. They may get a message saying, my order is missing 12 items and I need them for a party in six hours, help. Or another message that says I love Ocado, you're the best. And a third message that says my delivery is a few hours late, can I get a refund? Or maybe another message that says, I scheduled a delivery for 3 PM today, I need to reschedule it, please. Which of these has a higher priority? The customer service agent has to read the message to know the priority, right? But at that point, they might as well handle the message. So how do you optimize the handling of customer service messages to figure out what the high priority messages are? That was the problem facing the data engineering team at Ocado. The engineering team use Google Cloud platform to learn how to respond to urgent customer emails four times faster, how do they do that? They use Google's text processing capability and built several custom machine learning models to essentially read those emails. Determine what the message was, what was being talked about and whether each message was urgent or not. They built an ML model that classifies messages in terms of priority. And puts them in different queues so that the urgent messages can be acted on faster. A request for a refund is not an urgent message and can probably be handled in a few hours. But if somebody says, I have a party this evening and my items haven't arrived yet. Or I have a delivery scheduled for 3 PM and I'd like to reschedule it. Those are more time-sensitive, they are more urgent and they should be handled higher up in the priority queue. A machine learning model that's able to understand natural language text will be able to do this. Ocado was able to build such an ML model using Google Cloud technologies. As a second example of companies that can democratize data access through the use of Google Cloud, take Twitter. Twitter has large amounts of data and they also have very high-powered sales teams and marketing teams. But these really great sales and marketing people for a very long time did not have access to the data that was internal to Twitter. So they couldn't use their company's data to carry out the kind of analysis that they wanted to be able to do, why? Because most of the data was stored on Hadoop clusters that were already completely overtaxed. So what Twitter's engineering team did was that they replicated some of that data from the Hadoop clusters on to cloud storage. Loaded it into BigQuery and provided access to BigQuery to the rest of the organization to the sales and marketing teams. And because these were some of the most frequently requested data sets within Twitter. The sales and marketing teams were able to now go ahead and start analyzing data. With ready access to the data, many people who were not data analysts, those people are now analyzing data and making better decisions as a result.

## 10. Recap

* Let's summarize the major topics we have covered so far in this introduction. Recall that your data sources on the left are your upstream systems like RDBMS and other raw data that comes from your business in different formats. We covered a few concepts here, a data lake. A data lake is your consolidated location for raw data that is durable and highly available. Our data lake is often Google Cloud storage. The second concept we looked at was a data warehouse. Data warehouse is the end result of preprocessing the raw data in your data lake and getting it ready for analytic and Machine Learning workloads. You will notice a lot of other GCP products icons here, like batch and streaming data into your data lake and running Machine Learning onto your data. We'll cover those topics in detail later in this course. A useful cheatsheet to bookmark as a reference is a Google Cloud Platform products in four words or less. This is maintained on GitHub by our Google Developer Relations team. It's a great way to stay on top of new products and services that come out just by following the GitHub commits.

## QuizNotes

* Which of the following are the jobs of a data engineer?
	* Get the data to where it can be useful
	* Get the data into a usable condition
	* Add new value to the data
	* Manage the data
	* 	Productionize data processes
* Which statements are true?
	* Cloud SQL is optimized for high-read data
	*  BigQuery is optimized for high-read data