## 1. Introduction to Data Lakes

* Hi, I'm Evan Jones a Technical Curriculum Developer here with Google. Welcome to the chapter on building a data lake. Let's start with a discussion about what data lakes are, and then where they fit in as a critical component to your overall data engineering ecosystem. So what is a data lake? Well it's a fairly broad term, but it generally describes a place where you can securely store various types of data of all scales for processing and analytics. Data lakes are typically used to drive data analytics, data science and ML workloads, or batch and streaming pipelines. Data lakes will accept all types of data finally, data links are portable onpremise or in the cloud. Now here is where data lakes fit into the overall data engineering ecosystem for your team. You have to start with some originating system or systems that are the source of all of your data. Those are your data sources, then as a data engineer, you need to build those reliable ways of retrieving and storing that data. Those are your data sinks, the first line of defense in an enterprise data environment is your data lake. Again, it's the central give me whatever data you have in a variety of formats, volume, and velocity and I got it, and I'll take care of it. We'll cover the key considerations and options for building a data lake in this module. Once your data is off the source systems and inside of your environment, generally a ton of cleanup and processing is required to massage that data into a useful format for your business. It will then likely end up in your data warehouse, that's the focus for our next module. What actually performs the cleanup and processing of data? Those are your data pipelines, they're responsible for doing the transformations and processing on your data at scale, and bring your entire system to life with fresh newly processed data available for analysis. Now an additional abstraction layer above your pipelines is what I like to call the entire workflow. You'll often need to coordinate efforts between many of the different components at a regular or an event driven cadence. While your data pipeline may process data from your lake to your warehouse, your overall orchestration workflow. Maybe the one responsible for kicking off that data pipeline in the first place, when it notice that there was a new raw data file available from one of your sources. Before we move into what cloud products fit into one of these roles, I want to leave you with an analogy that has help me disambiguate each of these components. All right, just for a second take off your data engineering hat and put on a civil engineering hat for a moment. You're tasked with building an amazing skyscraper in a downtown city somewhere. Before you break ground, you need to ensure that you have all the raw materials that you're going to need to achieve your end objective. Sure, some materials can be sourced later inside the project, but let's keep this example simple and say you have all the materials on your job site to begin with. The act of bringing the steel, the concrete, the water, the wood, the sand, the glass. Whatever it is from whichever sources elsewhere in the city, onto your construction site is analogous to data coming from all those different source systems and into your data lake. Great, now you have a ton of raw materials, but you can't use them as is to build your building. You've gotta cut the wood, corrugate the metal, measure and cut the glass and format everything before it's suited for the purpose of building your building. The end result that cut glass, shaped metal, that's the formatted data that's then stored inside of your data warehouse. It's ready to be used to directly add value to your business, which in our analogy is actually building the building itself. How do you transform these raw materials into those useful pieces, while continuing with the analogy on the construction site? That's the job of the worker, as you'll see later when we talk about data pipelines, it's actually pretty funny. The individual unit behind the scenes is literally called a worker on cloud dataflow, and a worker is actually just a virtual machine. And it takes some small piece of data and it transforms that piece for you. And you might be asking, what about the building itself? Well, that's whatever end goal or goals that you have for this engineering project. In the data engineering world, the shiny new building could be a brand new analytical Insight that wasn't possible before, or a machine learning model. Or whatever else you want to achieve now that you have that cleaned data available. The last piece of the analogy is the orchestration layer, on a construction site, you generally have a manager or supervisor that directs when work is to be done. And if there are any dependencies they could say hey, once that new metal gets here, send it to this area of the site for cutting and shaping, and then alert this other team that it's available for them to start building with. In the data engineering world, that's your orchestration layer or your overall workflow. So you might say, every time a piece of a CSV file any of that data drops into this Google Cloud Storage bucket. I want you to automatically pass it to our data pipeline for processing, and once it's done processing I want you the data pipeline to then stream it into our data warehouse. And then we're not done yet, once it's in the data warehouse, I will notify the machine learning model that new cleaned training data is now available for training and retraining. And then I can direct it to start training a new model version. Can you see mentally the graph of actions that we're building out here? What if one step fails, or what if you want to run this every day or every hour or a triggered on an event? You're beginning to see the need for an orchestrator which in our solutioning, will be Apache airflow running on a cloud composed environment later. Let's bring back one example solution architecture diagram that you saw earlier in the course. The data lake here is Google Cloud Storage buckets, right in the center of that diagram. It's your consolidated location for raw data, and it's durable and highly available. In this example our data lake is those Google Cloud Storage buckets, but that does not mean that Google Cloud Storage is your only option for data legs on GCP. I'll say it again, cloud storage is one of a few good options to serve as a data like but it's not the only one. In other examples we'll look at, BigQuery maybe your data lake and your data warehouse, and you're not using Google Cloud Storage buckets at all. This is why it's so important to first understand what you want to do first, and then finding which of the solutions best meets your needs. Regardless about which tools and technologies you use with the cloud, your data like generally serves as that single consolidated place for all of your raw data. I like to think of it as a durable staging area, everything gets collected here and then sent out elsewhere. Now this data may end up many other different places like a transformation pipeline that cleans it up, and moves it to the data warehouse. And then it's read by a machine learning model, but it all starts with getting that data into your data lake first. And now let's do a quick overview of some of the core Google Cloud Big Data products that you need to know as a data engineer, and that you'll get hands-on practice with those inside of your labs later. Here is a list of the big data in ML products organized by where you will likely find them, in a typical big data processing workflow. From storing the data on the left, to ingesting it into your cloud native tools for analysis, training machine learning models and ultimately serving up some kind of insights. In this data lake module, we'll focus on two of the foundational storage products which make up your data link, Google Cloud Storage and Cloud SQL if you're using relational data. Later on in the course, you will practice with Cloud Bigtable as well if you want to do high throughput streaming pipelines. You may have been surprised as I was, when I first started learning Google Cloud platform to not see BigQuery in the storage column, generally BigQuery is uses a data warehouse. So let's remind ourselves, what's the core difference between the data lake and a data warehouse then? A data lake is essentially that place where you've captured every aspect of your business's operations raw. Because you want to capture every aspect you tend to store the data in its natural raw format, and that's the format is thrown out by your applications. You might have a log file and all those log files and other raw data files, all will get jammed together inside of your data Lake. You can basically store anything that you want and in any format and flexibility that you want. So you tend to store things like object blobs or files, the advantage of this data lakes flexibility as the central collection point, is also the problem. With a data lake, the data format is very much driven by the application that writes the data in, and it's whatever format that ends up as. And the advantage of the data like is that whatever the application gets upgraded, it can start writing the new data immediately, because it's just a capture of whatever raw data exists. But, how do you take this flexible and a large amount of raw data and then ultimately do something useful with it for your business? Enter the data warehouse, so on the other hand the data warehouse is much more thoughtful than a data lake. You might load the data into the data warehouse only after you have a schema clearly defined, and a use case identified, and so no garbage data in the data warehouse. You might take the raw data that exists in the data lake, transform it, organize it, process it, clean it up, and then store it as immediately useful data inside of your warehouse. Why are you getting the data warehouse? Well, maybe because the data in the data warehouse is used to generate charts, reports, dashboards as a back-end for your machine learning models. Whatever the use case for your business is that data is immediately able to be used from the warehouse. Now the idea is that because the schema is consistent and shared across all the applications, someone like a data scientists or data analyst could go right in and derive insights much, much faster. So a data warehouse tends to be structured and semi-structured data that's organized and placed into a format that makes it conducive for immediate querying and analysis.

## 2. Data Storage and ETL options on GCP

* Next let's discuss your data storage and then your options for extracting, transforming and loading your data into Google Cloud platform. Your options on gcp for building a data lake are the storage solutions that you saw earlier. You've got cloud storage as a great catch all, Cloud SQL and Cloud Spanner for relational data and Cloud Firestore and Cloud Bigtable for nosql data. Choosing which option or options to use depends heavily on your use case and ultimately what you're trying to build. In this chapter we'll focus on cloud storage and Cloud SQL, but you'll see the no SQL options like Cloud big table later on in the course. When we talk again about high-throughput streaming. So how do you decide on which path to take for your Lake? The final destination for where your data lands on the cloud and the paths that you take to get your data to the cloud depends on where your data is now. How big your data is, this is the volume component of the 3 vs of big data and ultimately where does it have to go? In architecture diagrams that ending point for the data as you mentioned earlier, that's your data sink, a common data sink after a data Lake, that's your data warehouse. Don't forget a critical thing to consider is how much processing and transformation your data needs before it's ultimately useful for your business. Now you may ask do I have to do the processing before I load it into my data lake or afterwards before it gets shipped off somewhere else. Let's talk about these different patterns because there's multiple right answers, the method that you use to load the data into the cloud depends on how much transformation is needed from that raw data. That you have into a final format that you want. In this chapter we'll look at some of the considerations for the final format that you could want your data in. The simplest case might be that you have data in a format that's readily ingested by the cloud product that you want to store it in. Let's say for example that you have data in an Avro format and you want to store the data in bigquery because your use case fits what bigquery is good at. Now we can simply just do el extract and load. Bitquery will directly load Avro files, el is extract and load and this is when the data can be imported as is into a system. Examples include importing data from a database where the source and the target already have the same schema, one of the features that makes bigquery unique is that as you saw before with the federated query example, you may end up not even having to load that data into bigquery to still query off of it. Avro or could park a CSV files are now all supported for a federated querying as is directly connecting to a cloud SQL database and running a query across databases. Sort of an external data connection. So what's an elt or extract load and transform the data? That's when the data is loaded into a cloud product is not in the final form that you want it in. You may want to clean it up or you may want to transform the data in some way such as perhaps you want it corrected. In other words, you would extract it from your on-premise system loaded into the cloud product and then do the transformation. That's an extract load and transform or elt, you tend to do this when the amount of the transformation that's needed is not very high and the transformation will not greatly reduce the amount of data that you have. Elt allows raw data to be loaded directly into the target and then transformed there. For example, a very common example inside a big query you could use SQL to transform that raw data that's loaded into bigquery and just simply write it to a new table. Now this third option is extract then transform and then load or ETL. This is the case where you want to extract the data, apply a bunch of processing to it. And then finally load it into the cloud product or solution of choice.That's usually what you pick when this transformation is essential or if the transformation greatly reduces the data size. So by transforming the data before loading it into the cloud you might be able to greatly reduce the network bandwidth that you need. If you have data in some binary proprietary format and you need to convert it before loading. Then you will need ETL as well. To summarize, extract transform load or the ETL, that's your data integration process in which the transformation takes place in an intermediate service before it's loaded into the target. As an example, the data might be transformed in a data pipeline like by using Cloud dataflow, you're going to see later. Before ultimately being loaded into bigquery.

## 3. Building a Data Lake using Cloud Storage

* Google Cloud Storage is the essential storage service for working with data, especially unstructured data, as you can see in the machine learning world later on. Let's do a deep dive into why Google Cloud Storage is a popular choice to serve as a Data Lake. Data and Cloud Storage persists beyond the lifetime of virtual machines or clusters. It's persistent and it's also relatively inexpensive compared to the cost of compute. So for example, you might find it more advantageous to cache the results of previous computations inside of cloud storage for archiving. Or, if you don't need an application running all the time, you might find it helpful to save this state of your application in the cloud storage, and then shut down the machine that's running or when you don't need it. Google Cloud Storage is an object store, so it just stores and retrieves binary objects without regard to what data is contained in the objects. However to some extent, it also provides file system compatibility and can make objects look like and work like as if they were files, so you can copy files in and out of it. Data stored in cloud storage will basically stay there forever meaning that it's durable, but it's available instantly or it's strongly consistent. You can share data globally, but it's encrypted and completely controlled and private, if you want it to be. It's a global service, you can reach the data from anywhere, that means it offers global availability. But the data can also be kept in a single geographic location if you need that too. Data is served up with moderate latency and high throughput. As a data engineer, you need to understand how cloud storage accomplishes these apparently contradictory qualities, and when to employ them in your solutions. A lot of cloud storage amazing properties have to do with the fact that ultimately it's an object store, and that all the other features are built on top of that base. The two main entities in cloud storage our buckets and objects, buckets are containers which hold objects, and objects exist inside of those buckets and not apart from them. So, buckets are containers for our purposes for data, buckets are identified in a single globally unique name space. So that means, once a name is given to a bucket, it can't be used by anyone else until that bucket's deleted and that name is released. Having a global name space for buckets greatly simplifies locating any particular bucket. When the bucket is created it's associated with a particular region or multiple regions, choosing a region close to where the data will be processed will reduce latency. And if you're processing the data using cloud services within that region, it will see you on network egress charges. When an object is stored, cloud storage replicates that object, it'll then monitor the replicas and if one of them is lost or corrupted it'll replace it automatically with a fresh copy. This is how cloud storage get many of the nines of durability, from multi-region bucket the objects are replicated across regions. And for a single region bucket as you might expect, the objects are replicated across zones within that one region. In any case, when the object is retrieved it's served up from the closest replica to the requester. And that's how the low-latency happens, multiple requesters could be retrieving the objects at the same time from different replicas, and that's how high throughput is achieved. Finally, the objects are stored with metadata, metadata is information about that object. Additional cloud storage features use the metadata for purposes such as, access control, compression, encryption and lifecycle management of those objects and buckets. For example, cloud storage knows when an object was stored, it can automatically set up to delete it after a period of time. This feature uses the object metadata to determine when to delete that object. When you create a bucket, you need to make several decisions. The first is the location of that bucket, location is set when a bucket is created and it can never be changed. If you need to move a bucket later, you'll have to copy all the contents to the new location and pay for the network egress charges. So choose your location very carefully, their location could be a single region such as Europe north one or Asia South one. It might be multiple regions, like EU or Asia, this means that the object is replicated in several regions within the European Union or Asia respectively. The third option is to have the location to be a dual region bucket. For example, North America for means that the object is replicated in the US Central one and US East one. So how do you choose a region? Well, let's say if all of your computation and all of your users are in Asia, you can choose an Asian region to reduce network latency. But beyond that, how do you choose between Asia South one and Asia multi-region? You can select one region and the data will be replicated to multiple zones within this region. This way a single zone might go down but you'll still have access to the data, different zones within the same region provide isolation from most types of physical infrastructure and infrastructure software service failures. But if the entire region goes down such that there's like a flood in the region, for example, you won't be able to access that regional data. If you want to make sure that the data is available during a natural disaster. You could select multi-region or dual region, in which case is the replicas will be stored in physically separate data centers. And you can read more about this in the current SLAs online, I'll provide a link to it. Third, you need to determine how often you need to access or change your data. You can get deep discounts on storing the data if you're willing to pay a lot more when you need to access it. The discount starts to make sense, if you'll access the data no more than once a month or once a quarter, what are some scenarios? Well, good examples are archival storage, backups or disaster recovery, and the discount really works in your favor. If you access the data only once a quarter or once a year, these are called storage classes. Look at the link to see the SLAs and the costs associated with each of these storage classes. Cloud storage uses the bucket name and the object name to simulate a file system. This is how it works, the bucket name is the first term in the URI, a forward slash is appended to it, and then it's concatenated with the object name. The object name allows the forward slash character as a valid character in the name, the very long object name with forward slash characters in it. Looks like a file path system even though it's just a single name, in the example shown the bucket name is de class. The object name is de/modules/O2/script.sh, the forward slashes are just characters in the name. If this path were in a file system, it would appear to be shown on the left with a set of nested directories beginning with d class. Now for all practical purposes, Google Cloud Storage works like a file system, but there are some key differences. For example, imagine that you wanted to move all the files in the 02 directory to the 03 directory inside of the modules directory. In a file system, you'd have an actual directory structure or structures and you would simply modify the file system metadata, so the entire move is atomic. But in an object store simulating a file system, you'd have to search through all of the objects contained in the bucket for names that had 02 in the right position in the name. Then you'd have to edit each object name and rename them using 03. This would produce apparently the same result though moving the files between directories. However, instead of working with a dozen files in a directory, the system had to search over possibly thousands of objects in the bucket to locate the ones with the right names and then change each of them. So the performance characteristics are a little different. It might take longer to move a dozen objects from the directory 02 to the directory 03, depending on how many other objects are stored within that bucket. During the move there will be a list inconsistency with some files in the old directory and some in the new directory. A best practice is to avoid the use of sensitive information as part of bucket names, because bucket names are in a global namespace. Bucket names not the data in the buckets, the data in the buckets can be kept private if you need it to be. Google Cloud Storage can be accessed using the file access method, that allows you for example to use a copy command from the local file directory to Google Cloud Storage. You'll use the tool gsutil or Google storage utility to do this, cloud storage can also be accessed over the web. The site for it is storage.cloud.google.com, and it uses TLS or HTTPS to transport your data, which protects the credentials as well as the data in transit. Cloud storage has many object management features. For example, you can set a retention policy on all of the objects in a bucket like, the object should be expired after 30 days automatically. You can also use versioning, so you can have multiple versions of an object and they're each tracked and available if necessary, you might even set up lifecycle management. So automatically move objects that have been accessed in 30 days to nearline storage class or after 90 days to coldline storage to help optimize your costs. Let's take a look at how you can manage these object lifecycles a bit more programmatically, to help optimize the use of your cloud storage buckets, and save on storage costs.

## 4. Securing Cloud Storage

* Securing your data lake running on cloud storage is of paramount importance. We'll discuss the key security features that you need to know as a data engineer to control access to your objects. Cloud storage implements two completely separate but overlapping methods of controlling access to your objects. Cloud IAM, the policy and the access control lists or ACLs. Cloud IAM is standard across the Google Cloud platform. It's set at the bucket level and uniformly applies access rules to all objects within that bucket. Access control lists can be applied at the bucket level or to individual objects. So it provides more fine-grained access control. The Cloud IAM controls are as you would expect. Cloud IAM provides project roles and bucket roles. Which include bucket reader, bucket writer, and bucket owner. The ability to create or change access control lists is an IAM bucket role. In the ability to create and delete buckets and to set IAM policy, is a project level role. Custom roles are also available. Project level viewer, editor, and owner roles make user members of special internal groups that give them access by being members of those bucket roles. You can check out the online documentation for more details. When you create a bucket, you're offered the option of disabling access lists and only using Cloud IAM. Access lists are currently enabled by default. This choice used to be immutable. But now you can disable access lists, even if they were in force previously. As an example, you might have some name at email.com, Bob at example.com giving a reader access to a bucket through IAM. And you've also given them write access to a specific file in that bucket through an access control list. You can give such permissions to service accounts as well as an individual application. All data in Google Cloud is encrypted at rest and in transit and there is no way to turn off the encryption. The encryption is done by Google using encryption keys that we manage, Google managed encryption keys or GMEK. We use two levels of encryption. First, the data is encrypted using a data encryption key, and then the data encryption key itself is then encrypted using a key encryption key or a KEK. These KEKs are automatically rotated on a schedule that use the current KEK stored in Cloud KMS, or the Key Management Service. You don't have to do anything. This automatically happens. If you want to manage the KEK yourself, you can. Instead of Google managing the encryption key, you can control the creation and the existence of the KEK that is used. This is what we call customer managed encryption keys or CMEK. You can avoid Cloud KMS completely and supply your own encryption and rotation mechanism. This is called CSEK. Which data encryption option you use generally depend on your business, legal and regulatory requirements. So be sure to talk to your company's legal counsel. The fourth encryption option is client-side encryption. Client-side encryption simply means that you've encrypted the data before it's uploaded and then you have to decrypt the data yourself before it's used. Google Cloud Storage still performs GMEK, CMEK, or CSEK encryption on the object. It has no knowledge of the extra layer of encryption that you might have added. Cloud storage supports logging of data access and those logs are immutable. In addition to cloud audit logs in Google Cloud Storage access logs. There are various holds and locks that you can place in the data itself. For auditing purposes, you can place a hold on an object and all the operations that could change or delete the objects are suspended until that hold is released. You can also lock a bucket and no changes or deletions can occur until the lock is released. Finally, there is a locked retention policy as we previously discussed. And it continues to remain in effect and prevent deletion whether a bucket lock or an object hold or enforce or not. Data locking is different from encryption. Where encryption prevents somebody from understanding the data, locking prevents them from modifying the data. There are a whole host of special use cases supported by cloud storage. For example, decompressive coding. By default, the data you upload is the same data that you get back from cloud storage. This includes gzip archives, which are usually returned as gzip archives. However, if you tag an object properly in the metadata, you can cause cloud storage to decompress the file as it's being served. Benefits of the smaller compressed file are faster upload and lower storage cost compared with the uncompressed files. You can set up a bucket to be requester pays. Normally, if data is accessed from a different region, you have to pay network egress charges, that's traffic going out of the network. You can make the requester pay, so that you pay only for data storage. You can create a sign URL to anonymously share an object in cloud storage. And even have the URL expire after a period of time. It's possible to upload a single object in pieces and create a composite object without having to concatenate the pieces after upload. Now, there's so many more useful features in Google Cloud Storage, but we have to move on.

## 5. Storing All Sorts of Data Types

* As I highlighted before, Google Cloud Storage is not your only choice when it comes to storing data in a data lake on Google Cloud. You don't want to use Cloud Storage for transactional workloads. Even though the latency of Cloud Storage is low, it's not low enough to support high frequency writes. For transactional workloads, use Cloud SQL or Firestore depending upon whether or not you want SQL or NoSQL. You don't want to use Cloud Storage for Analytics unstructured data. If you do that, you'll spend a significant amount of compute parsing the data. It's better to use Cloud Bigtable or BigQuery for analytic workloads unstructured data depending upon your latency required. So we keep talking about transactional versus analytical workloads. What exactly do we mean here? Transactional workloads, those ones that require fast inserts and updates of records. You want to maintain a snapshot, a current state of the system. The trade-off is that queries tend to be relatively simple, and tend to affect only a few records. For example, take a banking system. Deposit your salary from your company to your account, that's a transaction. It updates your balance field, hopefully going up. The bank is using an online transaction processing or OLTP. In analytics workload on the other hand tends to read the entire dataset, and it is often used for planning our decision support. The data might come from a transaction processing system, but it's often consolidated from many different OLTP systems. For example, a bank regulator might require us to provide a report of every customer who transferred more than $10,000 to an overseas account. They might ask the bank to include costumers who tried to transfer the 10,000 in smaller chunks over a period of a week. A report like this will require significant scanning of that large dataset, and require a complex query that includes aggregating over moving time Windows in SQL. This is an example of an online analytical processing or OLAP workload. The reason why we treat these use cases differently is that transactional systems are heavily right focused. These tend to be operational systems. For example, a retailer's catalog data will require continual updating every time the retailer adds a new item, or changes its price. The inventory data will need to be updated every time the retailer sells one of those items. This is because the catalog in the inventory systems have to maintain an up to the moment snapshot of the existing business. Analytical systems on the other hand can be periodically populated from the entirety of the operational systems. We can use this once a day to generate a report of all the items in our catalog whose sales are increasing but whose inventory levels are low. Such a report left have reading a bunch of data but it's not going to write much. So OLAP systems or OLAP systems are read focused. So recall what we said, analytical systems can be periodically populated from the operational systems. Data engineers build those pipelines to populate the OLAP or OLAP system from the OLTP, the transactional processing system. One simple way might be to export the database as a file and load it into the data warehouse, and we call that EL, or extract and then load. On Google Cloud, a data warehouse option that's very popular tends to be BigQuery. There is a limit to the size of the data that can be loaded directly into BigQuery. This is because your network might affect that bottleneck. Rather than load the data directly into BigQuery, it can be much more convenient to first stage it and load it into Google Cloud Storage is that data lake, and then load Google Cloud Storage through a pipeline into BigQuery as your data warehouse. This is because Google Cloud Storage supports a multithreaded resumable loads of data. If you're using gsutil, you can literally just provide a -m option there. Loading data from Cloud Storage will also be much faster because of the high throughput it offers. A feature that has recently been released is the ability for BigQuery to query data files that live in Google Cloud Storage, directly query them without having to first load those files into BigQuery zone native storage. This is called a federated query or creating an external data source connection. A super popular use case is if you have your data files in a variety of formats like CSV, Avro, newly supported you can do Parquet or Apache ORC, and then query them directly into BigQuery. So let's see a quick demo of how this looks from GCS to BigQuery using a federated SQL query.

## 6. Storing Relational Data in the Cloud

* Let's get back to the discussion on those transactional workloads. You've got a couple of different options for relational databases to store those transactions. The default choice here is Cloud SQL if you've got things in MySQL, Postgres or even SQL Server. But if you require a globally distributed database, then you could use Cloud Spanner. You'd want a globally distributed database if your database will see updates from applications running in different geographic regions. The true time capability of Spanner is very appealing for this kind of use case. Another reason you might choose Spanner is if your database is too big to fit in a single Cloud SQL instance. For database that has many gigabytes, you might want to consider using a distributed database. The scalability of Spanner is appealing for this use case. Other than that, you can use Cloud SQL because it's more cost effective. For analytical workloads, the default choice on Google Cloud Platform is usually BigQuery. However, if you require really high throughput inserts, like more than a million rows per second or all sure low latency on the order of milliseconds, consider Cloud Bigtable. Higher than that, you'd probably use BigQuery because it's usually more cost effective.

## 7. Cloud SQL as a relational Data Lake

* Cloud SQL as we just mentioned is a default choice for those OLTP or Online Transaction Processing workloads on Google Cloud. Let's dive a little bit deeper. Cloud SQL is an easy-to-use service that delivers fully manage relational databases, Cloud SQL lets you hand off to Google all those mundane but necessary and time-consuming DBA tasks, like applying patches and software updates, managing your backups and configuring replications. So you can put your focus on building your great application and not being a DBA. Cloud SQL is Google's managed service for third-party RDBMSs or Relational Database Management Systems. It has MySQL, recently added PostgresSQL and Microsoft SQL server in there as well, we'll continue to add additional RDBMSs over time. What this means is that we provide a compute engine instance that already has that database like MySQL already installed, we'll manage the instance on your behalf. We'll do the backups, we'll do security updates, update the minor versions of the software, so you don't have to worry about that. In other words, Google Cloud manages the database to the point where you can treat it as a service. We even do DBA-like things, you can tell us to add a failover replica for your database, we'll manage it for you, and you'll have a 99.95 availability SLA. Another benefit of cloud SQL instances is that they're accessible by other gcp services as you saw with our Federated query from BigQuery to Cloud SQL, I love that demo. You can also use Cloud SQL with app engine, using standard drivers like connector J for Java and MySQLdb for Python. You can authorize compute engine instances to access Cloud SQL instances, configure the cloud SQL instance to be in the same zone as your virtual machine. Cloud SQL also supports other applications and tools that you might already be familiar with, for example, if you're using MySQL, MySQL Workbench, Toad and other external applications using those standard MySQL drivers. One of the advantages of Google managing your database is that you get the benefits of Google grade security. Cloud SQL's customer data is encrypted when on Google's internal networks, and when stored in database tables temporary files in those backups that we mentioned earlier. Every Cloud SQL instance includes a network firewall allowing you to control network access to your database instance by granting a revoking access. Cloud SQL is easy to use, it doesn't require any software installation or maintenance, and Google manages the backups. It takes care of the securely storing your backup data and then it makes it easy for you to restore from a backup at a point in time and perform that recovery to a specific state of that instance. Cloud SQL retains up to 7 backups for each instance, which is included automatically in the cost of your instance. You can vertically scale Cloud SQL, just increase the size of your machine, you can scale it up to 64 processor cores and more than a hundred gigabytes of RAM. Horizontally, you can quickly scale out read replicas, Google Cloud SQL supports 3 read replica scenarios. The first, Google Cloud SQL instance is replicating from a Cloud SQL Master instance, replicas are other instances in the same project and location as a master instance. Second, Cloud SQL instance is replicating from an external Master instance, therefore the master instance is external to Google Cloud SQL. For example, it can be outside the Google Network or in a Google compute engine instance, you can use this for backing up an on-premise SQL instance. Third, external SQL instances replicating from a Cloud SQL Master instance or replicas that are in a hosting environment that's outside of Cloud SQL. And keep in mind, if you need a horizontal read and write scaling, consider the use of Cloud Spanner. For the special case of failover, Cloud SQL does support this, Cloud SQL instances can be configured with a failover replica in a different zone within the same region. Then, Cloud SQL data is replicated across zones within a region for durability. In an unlikely event of a complete data center outage, a Cloud SQL instance will automatically become available in another zone. All changes made to the data on the Master are then replicated to a failover. If the Master instances zone has an outage, Cloud SQL automatically fails over to the replica. If the Master has issues not caused by his own outage, failover doesn't occur, you can however initiate that failover and get those benefits manually. Now, there are a few caveats, note that the failure of a replica is charged as a separate instance. And when a zonal outage occurs and your master fails over to your failover, replica any existing connections to that instance are closed. However, your application can reconnect using the same connection string or IP address. You don't need to update your application after a failover. After a failover occurs, the replica becomes the Master, and Cloud SQL automatically creates a new failover replica in another zone. If you located your Cloud SQL instance to be near other resources, such as a compute engine instance, you can relocate your Cloud SQL instance back to its original zone when that zone becomes online again, otherwise, there's no need to relocate your instance after a failover. You can use the failover replica as a read replica to offload read operations from the master to help avoid those bottlenecks. For more details on failover replicas, check those docs that I'll provide on high availability. We kept saying that cloud SQL is fully managed. You might also seen us use the word serverless to describe like BigQuery. For example, what's the difference between fully managed and serverless? By fully managed, we mean that the service runs on a hardware that you can control, you can SSH into a Cloud SQL instance for example. That said, Google does help you manage that instance by automating backups and setting over those failover instances that we just talked about. Serverless is the next step up, you can treat a serverless product that's just like an API that you're calling, sure you pay for using the product, but you don't have to worry about or manage any of the servers, for all you know, it could be completely manual behind the scenes, you just get the benefits of the output. BigQuery is serverless, as is cloud pub/sub for asynchronous messaging and data flow for parallel data processing, you can think of cloud storage as being serverless as well. Sure cloud storage uses discs, but you never actually have to interact with any of the hardware. One of the unique things about Google Cloud is that you can build a data processing pipeline on well design components all of which fully is serverless. Dataproc on the other hand is fully managed, meaning it'll help you run Spark and Hadoop workloads, without having to worry about the setup, you can get to those servers if you wanted to. Given the choice between doing a brand new project on BigQuery or data flow which are serverless or Dataproc which is fully managed, which one should you choose? Well it really depends on your use case, but all things being equal, if it's easy enough to get your data into either, you want to choose this serverless product.

## QuizNotes

* Which statement best describes a data lake?
	* The place where you capure every aspect of your business operations. Data is stored in its natural, raw format.
* Which of the following statements on Cloud Storage are true?
	* Cloud Storage simulates a file system
	* Cloud Storage allows you to set retention policies on all objects in a bucket
	* Cloud Storage implements both Cloud IAM policy and Access Control Lists