## 1. The Modern Data Warehouse

* This is the third module in the course, modernizing data lakes and data warehouses with GCP. In this first section, we'll describe what makes a modern data warehouse, will also talk about what distinguishes a data lake from an enterprise data warehouse. An enterprise data warehouse should consolidate data from many sources. If you recall from the previous module, a data lake does something very similar. The key difference between the two, is what consolidates. A data warehouse imposes a schema, a data lake is just raw data, but an enterprise data warehouse brings the data together and makes it available for querying and data processing. To use a data warehouse, an analyst needs to know the schema of the data. However, unlike for a data lake, the analysts doesn't have to write code, to read and pass the data. Another reason to consolidate all of your data besides standardizing the format and making it available for querying, is to make sure the query results are meaningful. You want to make sure the data is clean, accurate and consistent. The purpose of a data warehouse is not to store data. That's the purpose of a data lake. If you have raw data that you want to keep around but not necessarily query, don't bother with cleaning and streamlining it, leave it in a data lake. All data in a data warehouse should be available for querying. It's important to ensure that those queries are quick. You don't want people waiting hours or days for results. We described an enterprise data warehouse and how it's different from a data lake. So what makes a data warehouse modern? Businesses data requirements continue to grow. You want to make sure that a data warehouse can deal with datasets that don't fit into memory. Typically, this is gigabytes to terabytes of data but occasionally it can be petabytes. You don't want separate warehouses for different datasets, instead you want a single warehouse that can scale from gigabytes to petabytes of data. Second, you want the data warehouse to be serverless and fully no-Ops, you don't want to be limited to clusters that you need to maintain or indexes that you need to find too. Having your hands off of these responsibilities will now data analysts to carry out ad-hoc queries faster which is important because you want the data warehouse to increase the speed at which your business makes decisions. Next, your data warehouse is not productive if it allows you to do queries but doesn't support rich visualization and reporting. Ideally, data warehouses can seamlessly plug-in to whichever visualization or reporting tool your businesses most familiar with. Similarly, because the data warehouse requires a clean and consistent data, you will often have to build data pipelines to bring data into the Warehouse. The modern data warehouse should be able to integrate with an ecosystem of processing tools for building ETL pipelines. Your data pipelines should be capable of constantly refreshing data in the warehouse in order to keep it up-to-date. You need to be able to stream data into the warehouse and not rely only on batch updates. Also predictive analytics is becoming increasingly important for data analysts. As a result, a modern data warehouse has to support machine learning without moving the data out of the warehouse. Last but not least, in a modern data warehouse it should be possible to impose enterprise-grade security like data exfiltration constraints, it should also be possible to share data and queries with collaborators.

## 2. Intro to BigQuery

* In the next section, we're going to introduce BigQuerya, data warehouse Solution on Google Cloud platform. BigQuery has many capabilities that make it an ideal data warehouse. When we talked about a modern data warehouse, we talked about having the warehouse be able to scale from gigabytes to petabytes seamlessly, we talked about being able to do ad hoc queries and no-ops. BigQuery cost-effectively handles large petabyte scale data sets for storage and querying. In fact, it's similar to the cost of Google's Cloud storage. This enables you to store your data without having to worry about archiving off older data to save on storage. Unlike traditional data warehouses, BigQuery has features like geospatial and Machine learning built-in, it also provides capabilities to stream data in so you can analyze your data in near real-time. Because it's part of Google Cloud, you get all of the security benefits that the cloud provides while also being able to share data sets inquiries. BigQuery supports standard SQL queries and is compatible with ANSI SQL 2011. Let's take a look at how easy it is to query large datasets in BigQuery.

## 3. Getting Started

* Now that you're familiar with the basics of BigQuery, it's time to talk about how BigQuery organizes your data. BigQuery organizes data tables into units called data sets. These data sets are scoped to your GCP project. When you reference a table from the commands line and SQL queries are in code, you refer to it by using the construct project.dataset.table. What are some reasons to structure your information into datasets, projects and tables? These multiple scopes, project, dataset and table can help you structure your information logically, you can use multiple datasets to separate tables pertaining to different analytical domains and you can use project level scoping to isolate datasets from each other according to your business needs. Also, as we'll discuss later, you can align projects to billing and use data sets for Access Control. You store data in separate Tables based on logical schema considerations. The project is what the billing is associated with. For example, if you query a table that belong to BigQuery public data project, the storage costs are build to that data project. To run a query, you need to be logged into the GCP console. You'll run a query in your own GCP project and the query charges are then build to your project, not the public data project. In order to run a query in a project, you need Cloud IAM permissions to submit a job. Remember that running a query means that you must be able to submit a query job to the service. Access control is through Cloud IAM, and is that the data set level, not individual tables in the data set. In order to query some data in a table, you need at least read permission on the data set in which the table lives. Like cloud storage, BigQuery data sets can be regional or multi-regional. Regional data sets are replicated across multiple zones in the region, multi-regional means replication among multiple regions. Every table has a schema, you can enter the schema manually through the GCP console or by supplying a JSON file. As with Cloud Storage, BigQuery storage encrypts data at rest and over the wire using Google-managed encryption keys, but it's also possible to use customer-managed encryption keys. authentication is through Cloud IAM, and so it's possible to use Gmail addresses or G Suite accounts for this task. Access control as we talked about is through Cloud IAM roles and involves giving permissions. We discussed two of those in read access and the ability to submit query jobs. However, many other permissions are possible. Remember that permissions are at the data set level. When you provide access to a dataset, either read or write, you provide access to all of the tables in that dataset. Logs and BigQuery are immutable and are available to be exported to Stackdriver. Admin activities and system events are logs, an example of a system event is table expiration. If when creating a table you can figure out to expire in 30 days, at the end of 30 days a system of event will be generated in logs. You will also get immutable logs of every access that happens to a data set under your project. BigQuery provides predefined roles for controlling access to resources. You can also create Cloud IAM roles consisting of your defined set of permissions and then assign those roles to users or groups. You can assign a role to a Google email address or a G Suite group. An important aspect of operating a data warehouse is allowing shared but controlled access against the same data to different groups of users. For example, Finance, HR and marketing departments all access the same tables, but their levels of access differ. Traditional data warehousing tools make this possible by enforcing row-level security. You can achieve the same results in BigQuery by defining authorized views and row-level permissions. Sharing access to datasets is easy. Traditionally onboarding new data analyst involves significant lead time, to enable analysts to run simple queries, you had to show them where the data source is resided, setup ODBC connections and tools and access rights. Using GCP, you can greatly accelerate an analyst time to productivity. To onboard an analyst on GCP, you grant access to the relevant projects, introduce them to the Google Cloud platform console and the BigQuery web UI, then you share some queries to help get them acquainted with the data. The GCP console provides a centralized view of all assets in your GCP environment, the most relevant asset to data analyst might be cloud storage buckets where they can collaborate on files. The BigQuery web UI presents the list of data sets that the analyst has access to, analyst can perform tasks in the GCP console according to the role you grant them, such as, viewing metadata, previewing data, executing, saving and sharing queries. You can only control access to datasets, when you provide read access to a dataset to a user, every table in that dataset is readable by the user. But what if you want more fine-grained control? That's when you use views, for example, in this example, we're creating a view and dataset B and the view is a subset of the table data in dataset A. Now, by providing users with access to dataset B, we're creating an authorized view that is only a subset of the original data. Note that you cannot export data from a view and dataset B has to be in the same region or multi-region as dataset A. A view is a SQL query that looks like and has properties similar to a table. You can query a view just like you query a table, BigQuery also supports materialized views. These are views that are persisted so that the table does not need to be queried every time the view is used. BigQuery will keep the materialized view refreshed and up-to-date with a contents of the source table. In the queries we saw earlier, we wrote the query in SQL and hit run on the UI, what this did was submit a query job to the BigQuery service. The BigQuery query service is separate from the BigQuery storage service, however, they're designed to collaborate and be used together. In this case, we were querying native tables in the bigquery public data project. Querying native tables is the most common case and the most performant way to use BigQuery. BigQuery is most efficient when working with data contained in its own storage service, the storage service and the query service work together to internally organize the data to make queries efficient over huge datasets of terabytes and petabytes in size. The query service can also run query jobs on data contained in other locations such as tables in CSV files hosted on cloud storage. So you can query data and external tables or from external sources without loading it into BigQuery, these are called Federated queries. In either case, the query service puts the results into a temporary table and the user interface pulls and displays the data in the temporary table. This temporary table is stored for 24 hours, so if you run the exact same query again, and if the results would not be different then BigQuery will simply return a pointer to the cached results. Queries that can be served from the cache do not incur any charges. It's also possible to request that the query job right to a destination table, in that case you get to control when the table is deleted. Because the destination table is permanent and not temporary, you'll get charged for the storage of the results. To calculate pricing, you can use BigQuery's query validator and combination with a pricing calculator for estimates. The query validator provides an estimate of the size of data that will be processed during a query, you can plug this into the calculator to find an estimate of how much running the query will cost. This is valid if you're using an on-demands plan where you pay for each query based on how much data is processed by that query. Your company might have opted for a flat rate plan, in that case your company will be paying a fixed price, and so the cost is really how many so-called slots your query uses. You can separate the cost of storage and the cost of queries, by separating projects A and B, it's possible to share the data without giving access to run jobs. In this diagram, users 1 and 2 have access to run jobs and access the datasets in their respective projects. If they run a query, that job is built to their own project. What if user 2 needs the ability to access the data in project A? This person who owns project a can allow user 2 to query the project A dataset and the charges will go to Project B. The public dataset project owner granted all authenticated users access to use their data, the special setting all authenticated users makes the dataset public. Authenticated users must use BigQuery within their own project and have access to run BigQuery jobs so that they can query the public dataset. The billing for the query goes to their project even though the query is using public or shared data. BigQuery offers one terabyte of querying for free every month so public data sets are an easy way to try out BigQuery. The BigQuery data transfer service allows you to copy large datasets from different projects to yours in seconds. We'll talk more about the BigQuery data transfer service in the next section on data loading.

## 4. Loading Data

* Next we'll talk about how to load new data into BigQuery. Recall from earlier modules that the method you use to load data depends on how much transformation is needed. EL or extract and load is used when data is imported as is where the source and target have the same schema. ELT or extract, load, transform, is used when raw data will be loaded directly into the target and transformed there. ETL or extract, transform, load is used when transformation occurs in an intermediate service before it's loaded into the target. You might say that the simplest case is EL, if the data is usable in its original form, there's no need for transformation just load it. You can batch load data into BigQuery, in addition to CSV you can also use data files with delimiters other than commas by using the field delimiter flag. BigQuery supports loading gzip compressed files as well however, loading compressed files isn't as fast as loading uncompressed files. For time-sensitive scenarios or scenarios in which transfer uncompressed files to cloud storage is bandwidth or time-constrained, conduct a quick loading test to see which alternative works best. Because load jobs are asynchronous, you don't need to maintain a client connection while the job is being executed. More importantly load jobs don't affect your other BigQuery resources. A load job creates a destination table if one doesn't already exist, BigQuery determines the data schema as follows. If your data is an Avro format, which is self-describing BigQuery can determine the schema directly, if the data is in JSON or CSV format BigQuery can auto detect the schema, but manual verification is recommended. You can specify a schema explicitly bypassing the schema in as an argument to the load job. Ongoing low jobs can append to the same table using the same procedure as the initial load, but do not require the schema to be passed with each job. If your CSV files always contain a header row that should be ignored after the initial load and table creation. You can use the skip leading rows flag to ignore the row, for details see the documentation on BQ load flags. BigQuery sets daily limits on the number and size of load jobs that you can perform per project and per table. In addition, BigQuery sets limits on the sizes of individual load files and records. You can launch load jobs through the BigQuery web UI, and to automate the process you can set up cloud functions to listen to a cloud storage event. That is associated with new files arriving in a given bucket and launched a BigQuery load job. BigQuery can import data stored in the JSON format so long as it's a new line delimited. It can also import files in Avro, parquet and RC, the most common import is with CSV files which are the bridge between BigQuery and spreadsheets. BigQuery can also directly import firestore and datastore export files. Another way that BigQuery can import data is through the API, basically any place where you can get code to run can theoretically insert data into BigQuery tables. You could use the API from a compute engine instance, a container on Kubernetes, app engine, or from cloud functions. However, you'd have to recreate the data processing foundation in these cases. In practice the API is mainly used from either cloud data proc or cloud dataflow. The data transfer service or DTS provides connectors and pre-built BigQuery load jobs that perform the transformations necessary to load report data, from various business services directly into BigQuery. You can transfer files to cloud storage in the schema that is native to the existing on-premises data storage. Loading into a set of staging tables and BigQuery and then transformed into the ideal schema for BigQuery using BigQuery SQL commands. It's a common practice to automate execution of queries based on a schedule or an event, and cache the results for later consumption. You can schedule queries to run on a recurring basis, scheduled queries must be written in standard SQL, which can include data definition language, and data manipulation language statements. The query string and destination table can be parameterised allowing you to organize query results by date and time. By maintaining a complete 7-day history of changes against your table, BigQuery allows you to query a point-in-time snapshot of your data. You can easily revert changes without having to request a recovery from backups. This slide shows you how to do a select query to query the tables as of 24 hours ago, because this is a select query, you can do more than just restore table. You can join against some other table or correct the values of individual columns, you can also do this using the BigQuery command line tool as shown in the second snippet. Here, we're restoring data as of 120 seconds ago. You can recover a deleted file only if another table with the same ID in the dataset has not been created. In particular, this means you cannot recover a deleted table if it's being streamed into. Chances are that the streaming pipeline would have already created an empty table and started pushing rows into it. Be careful using create or replace table because this makes the table are recoverable. BigQuery is a managed service, so you don't have the overhead of operating, maintaining, or securing the system. A typical data warehouse system requires a lot of code for coordination and interfacing. You can get BigQuery DTS running without coding, the core of DTS is scheduled an automatic transfers of data from wherever it's located. In your data center, on their clouds and south services, you can get all of that data into BigQuery. Transferring the data is only the first part of building a data warehouse. If you're assembling your own system, you would need to stage the data so that it can be cleaned for data quality. And transformed using ELT like we talked about before, and processed and put into its final stable form. A common issue with data warehouse systems is late arriving data. For example, a cash register closes late and does not report its daily receipts during the schedule transfer period. To complete the data you would need to detect that not all of the data was received, and then request the missing data to fill in the gap. This is called data backfill, and it's one of the automatic processes provided by BigQuery DTS. Backfilling data means adding a missing past data to make a dataset complete with no gaps, and to keep all analytic processes working as expected. Use the data transfer service for repeated, periodic scheduled imports of data directly from software as a service systems, and to tables in BigQuery. The data transfer service provides connectors, transformation templates, and the scheduling. The connectors establish secure communications with the source service and collect standard data exports in reports. This information is transformed with in BigQuery, the transformations can be quite complicated resulting in 25 to 60 tables. And the transfer can be scheduled to repeat as frequently as once a day. The BigQuery data transfer service automate state of movement from SAS applications to be query on a scheduled managed basis. It can also be used to efficiently move data between regions. Notice that you don't need cloud storage buckets, data transfer service runs BigQuery jobs that transform reports from SAS sources into BigQuery tables and views. Google offers several connectors including Campaign Manager, Cloud Storage, Amazon S3, Google Ad Manager, Google ads, Google Play Transfers. YouTube channel, YouTube content owner, Teradata migration, and over a hundred other connectors through partners. Keep in mind if your data transformations are simple enough, you may be able to do them with just SQL. BigQuery supports standard DML statements such as insert, update, delete, and merge. But you should not treat it as an OLTP system, avoid individual updates and inserts as are strict limits to the number of update statements every day. BigQuery also supports DDL statements like create or replace table. In the example on this slide, the replace statement is used to transform a string of genres into an array, we'll cover arrays in greater detail soon. A unique feature of BigQuery is that some data can be queried without first importing it into BigQuery tables. For example, it can look in the first sheet of a Google worksheet or CSV or JSON file. You could use a federated query to import data from a CSV and cloud storage and transform at using SQL, all in one query. However, importing the data into BigQuery will provide much faster performance. Here's a quick demo of how you can query a cloud SQL database which is hosted MySQL, Postgres or SQL Server directly from BigQuery using external connections syntax and the from clause. Lastly, what if your transformations went beyond what functions were currently available in BigQuery? Well, you can write your own, BigQuery supports user defined functions or UDF. A UDF enables you to create a function using another SQL expression or an external programming language. JavaScript is currently the only external language supported, we strongly suggest you use standard SQL though, because BigQuery can optimize the execution of SQL much better than it can for JavaScripts. UDFs allow you to extend the built-in SQL functions. They take a list of values which can be arrays or struts, and return a single value which can also be an array or a struct. UDFs written in JavaScript can include external resources such as encryption or other libraries. Previously UDFs were temporary functions only, this meant you could only use them for the current query or command line session. Now we have permanent functions, scripts and stored procedures, and beta, but they might even be generally available by the time you're seeing this. Please check the documentation, when you create a UDF BigQuery persistent and stores that as an object in your database. What this means is that you can share your UDFs with other team members or even publicly if you want to. The BigQuery team has a public GitHub repository for common user-defined functions at the link you see here.

## 5. Exploring Schemas

* Now let's dive into the world of Data Warehouse schemas. Designing efficient schemas that scale is a core job responsibility of any data engineering team. BigQuery host over 100 public datasets and schemas for you to explore on popular topics like daily weather readings, taxi cab logs, health data and more. Let's explore some of these public datasets schemas using SQL.

## 6. Schema Design

* Next, we'll talk about efficient data warehouse Schema Design. Take a look at the original data table here and the normalized data tables which contain the same data. The data and the original table is organized visually, as you might have used merge cells or columns in a spreadsheet. But if you had to write an algorithm to process the data, how might you approach it? Could be by rows, by columns, by rows then fields, and the different approaches would perform differently based on the query. Also, your method might not be parallelizable. The original data can be interpreted and stored in many ways in a database. Normalizing the data means turning it into a relational system. This stores the data efficiently and makes query processing a clear and direct task. Normalizing increases the orderliness of the data, it's useful for saving space. Many people with database experience will recognize this procedure. Normalizing data usually happens when a schema is designed for a database. Denormalizing is a strategy of allowing duplicate field values for a column in a table in the data to gain processing performance. Data must first be normalized before it can be denormalized. Denormalization is another increase in the orderliness of data. Data is repeated rather than being relational, flatten data takes more storage. But the flattened non-relational organization makes queries more efficient because they can be processed in parallel using columnar processing. Specifically, denormalizing data enables BigQuery to more efficiently distribute processing among slots. Resulting in more parallel processing and better query performance. You would usually denormalized data before loading it into BigQuery. However, there are cases where denormalizing data is bad for performance. Specifically, if you have to group by a column with a one too many relationships. In this example shown, OrderID is such a column. In this example, to group the data, it must be shuffled. That often happens by transferring the data over a network between servers or systems. Shuffling is slow, fortunately BigQuery supports a method to improve the situation. BigQuery supports columns with nested and repeated data. In this example, a denormalized flattened table is compared with one that has been denormalized. But the schema takes advantage of nested and repeated fields. OrderID is a repeated field, because this is declared in advance, BigQuery can SOAR and process the data, respecting some of the original organization of the data. For this reason, nested and repeated fields is useful for working with data that originates in relational databases. Nested columns can be understood as a form of repeated field. It preserves the relationalism of the original data and the schema, while enabling columnar and parallel processing of the repeated nested fields. It's the best alternative for data that already has a relational pattern to it. Turning the relation into a nested or repeated field improves BigQuery performance. Nested and repeated fields help BigQuery work with data source in relational databases. Look for nested and repeated fields, wherever BigQuery is used in a hybrid solution in conjunction with traditional databases.

## 7. Nested and Repeated Fields

* Let's take a closer look at BigQuery support for nested and repeated fields, and why this is such a popular schema design for Enterprises. I'll illustrate by using an example from a real business running on GCP, go-jek is a ride booking service among other services that space out of Indonesia. And they process over 13 petabytes of data on BigQuery per month from queries to support business decisions. What kind of decisions? >> For GO-JEK they track whenever a new customer places an order like hell's a ride with their mobile app, that order is stored in an orders table. Each order has a single pickup location and drop off destination, for single order you could have one or many events. Like ride ordered, ride confirmed, driver and roots, drop-off complete, etc. As a data engineer, how do you efficiently sore these different pieces of data in your data warehouse? Keep in mind you need to support a large user base querying petabytes per month. Well as you saw earlier, we could store one fact in one place with the normalization route, which is typical for relational systems. Or we could go the fully denormalized route and just store all levels of granularity in a single big table. Where you would have one order ID, like one, two, three, repeated in a row for each event that happens on that order, faster for querying sure but what are the drawbacks? For relational schemas, often the most intensive computational workloads are JOINs across very large tables. Remember, RDBMS systems are record based so they have to open each record entirely, and pull out the JOIN key from each table where a match exists. And that's assuming you know all the tables and need to be joined together. Imagine for each new piece of information about an order like promotion codes are user information, and you could be talking about a ten plus table JOIN. The alternative has different drawbacks, prejoining all your tables into one massive table makes reading data faster, but now you have to be really careful if you have data at different levels of granularity. In our example, each row would be at the level of granularity of a specific event like driver confirmed, for any given order. What does that mean for an order ID like one two, three? It's duplicated for each event on that order, imagine if you're looking to join higher level information like the revenue per order. And now you have to be exceedingly careful with aggregations to not double or triple count your duplicate order IDs, see the problem? One common solution in an Enterprise data warehouse schemas is to take advantage of nested and repeated data fields. You can have one row for each order and repeated values within that one row for data that is at a more granular level. For example, you could simply have an array of timestamps as your events, l let's see an example to illustrate this point. Here you see it clearly, shown here are the screen are just four rows for four unique order IDs, notice all the gray space in between the rows, that's because the event status in event time is at a deeper level of granularity. That means there are multiple repeated values for these events per each order, an array is a perfect data type to handle this repeated value and keep all the benefits of storing that data in a single row. I mentioned the field event status and event time, if this is one giant table, what is a DOT doing in those column names? There are no other table aliases we've joined on, so what's up with those fields? Event pick up and destination are what are called struct or structure type data fields in SQL. This isn't BigQuery specific, structs are standard SQL and BigQuery just supports them really well, structs you can think of as prejoin tables within a table. So instead of having a separate table for events and pick up and destination, you simply nest them together within your main table. So let's recap. You can go deep into a single field and have it be more granular than the rest of your data by using an array data type, like you see here for status and time. And you can have a really wide schemas by using struts, which allow you to have multiple fields of the same or different data types within them, much like a separate table would. The major benefits of structs is that the data is conceptually prejoined already, so it's much faster to query. People often ask, with really wide scheme has like 100 columns, how is it still so fast to query? Remember the BigQuery is column based storage not record based. If you did just a count order ID here to get your total order as BigQuery wouldn't even care that you have 99 other columns. Some of which are more granular with array and data types, it won't even look at them. That's what gives you the best of both worlds, lots of data all in one place and no issues with multiple granularity pitfalls when doing aggregations. Now it's your turn to practice reading one of these schemas that has nested and repeated fields, take a moment and spot those struts. As a hint you can look at the field name to see any field with a dot in the name, or you can look at the data type for any field values of the type record, which means struct. Did you get them all? Here are the four strokes in this dataset you saw earlier, events, pickups, destination, and duration. Duration is a new one, but we can simply keep adding more dimensions to our data set by adding more struts. Remember structs let you build really wide and informative schemas, now it's time to go deep, find the array data types in the schema as a hint look at mode and finds the repeated values. Have you got them? In this schema, the repeated value is the event struct which means the struct happens to have all columns in it that are all repeated. You can see that in the preview of the data below, the status field is an array of all the statuses for order and the time field is an array of all the time stamps for that order. A critical point I like to make here, is that struct and array data types in SQL can be absolutely independent of each other. You can have a regular column in SQL be an array column that has nothing to do with any struct. Likewise you can have a struct that has zero array field types in its columns. The benefit of using them together is a raised allow a given field to go deep into granularity, and structs allow you to organize all those useful fields into logical containers instead of separate tables. So here's the cheat sheet, structs are of type record when looking at a schema, and arrays are of mode repeated. Arrays can be of any single type like an array of floats or an array of strings. Arrays can be part of a regular field or be part of a nested field, nested inside of a struct. A single table can have zero to many struts and lastly, the real minds bending point is that a struts can have other struts nested inside of it. As you will soon see in your upcoming lab, which you Is a real Google analytics schema. We've been talking a lot about nested and repeated fields. So you're probably wondering what to do with your existing star schema, snowflake and third normal form data. The great news is a big query also works well with those schema types, use arrays and struts when you're dating naturally arrives in that format, and you'll benefit immediately from optimal performance. For the other schema types bring them directly to BigQuery and you'll likely be pleased with their performance as well. Lastly if you really love this topic and how it all came about, you can check out the original drama white paper that Google published in 2010. Drum will being the massively parallel SQL engine that powers BigQuery. These diagrams come from that paper and explain how this nested structure is stored when you're using column-oriented storage. As you see here every column in addition to its value also stores two numbers, definition and repetition levels. This encoding ensures that the full or partial structure of the record can be reconstructed by reading only requested columns. And never requires reading parent columns as is the case with alternative encodings, I'll link the way paper if you want to read more.

## 8. Optimizing with Partitioning and Clustering

* Next up is optimizing with partitioning and clustering. One of the ways you can optimize the tables in your data warehouse is to reduce the cost and amount of data read by partitioning your tables. For example, assume we have partitioned this table by the eventDate column, BigQuery will then change its internal Storage so the dates are stored in separate shards, and a table partitioned by a date or timestamp column. Each partition contains a single day of data. When the data is stored BigQuery ensures that all the data in a block belongs to a single partition. A partition table maintains these properties across all operations that modify it. Query jobs, DML statements, DDL statements, Load jobs, and copy jobs. This requires BigQuery to maintain more metadata than a non partitioned table. As the number of partitions increases, the amount of metadata overhead increases. When you do a query with the where clause that looks for dates between January 3rd and 4th, BigQuery will have to read only two-fifths of the full data-set. This can lead to dramatic cost and time savings. You enable partitioning during the table creation process. This slide shows how to migrate an existing table to an ingestion time partitioned table. Using a destination table, it will cost you one table scan. As new records are added to the table, they will be put into the right partition. BigQuery creates new date-based partitions automatically with no need for additional maintenance. In addition, you can specify expiration time for data in the partitions. Partitioning can be set by ingestion time on a date time column or a based on a range of an integer column. Here we're partitioning customer ID and the range 0-100 in increments of 10. Although more metadata must be maintained by ensuring the data is partitioned globally, BigQuery can more accurately estimate the bytes processed by your query before you run it. This cost calculation provides an upper bound on the final cost of the query. A good practice is to acquire the queries always include the partition filter. Make sure that the partition field is isolated on the left side because that's the only way that BigQuery can quickly discard unnecessary partitions. Clustering can improve the performance of certain types of queries, such as queries that use filter clauses and those that aggregates data. Once data is written to a cluster table by a query or a low job, BigQuery sorts the data using the values in the clustering columns. These values are used to organize the data and some multiple blocks and BigQuery storage. When you submit a query containing a closet filters data based on the clustering columns, BigQuery uses assorted blocks to eliminate scans of unnecessary data. Similarly, when you submit a query that aggregates data based on the values in the clustering columns, performance is improved because the sorted blocks co-locate rows with similar values. In this example, the tables partitioned by eventDate and clustered by user ID. Now because the query looks for partitions in a specific range, only two of the five partitions are considered. Because the query looks for user ID in a specific range, BigQuery can jump to the row range and read only those rows for each of the columns needed. You set up clustering a table creation time. Here we're creating the table partitioning by eventDate and clustering by user ID. We're also telling BigQuery to expire the partitions that are more than three days old. The columns you specify in the cluster are used to co-locate related data. When you cluster a table using multiple columns, the order of columns you specify is important. The order of the specified columns determines the sort order of the data. Over time, as more and more operations modify a table, the degree to which the data is sorted begins to weaken and the data becomes only partially sorted, in a partially sorted table, queries that use a clustering columns may need to scan more blocks compared to a table that is fully sorted. You can re-cluster the data in the entire table by running a select star query that selects from and overwrites the table. But guess what? You actually don't need to do that anymore. The great news is that BigQuery now periodically does auto re-clustering for you. So you don't need to worry about your clusters getting out of date as you get new data. Automatically clustering is absolutely free and automatically happens in the background. You don't need to do anything additional to enable it. Currently, BigQuery supports clustering over a partitioned table. Table clustering is supported for both ingestion time partitioned tables and for tables partitioned on a date or timestamp column. Currently clustering is not supported for non partitioned tables. When you use clustering and partitioning together, the data can be partitioned by a date or timestamp column and then clustered on a different set of columns. In this case, data on each partition is clustered based on the values of the clustering columns. Partitioning provides a way to obtain accurate cost estimates for queries and guarantees improved cost and performance. Clustering provides additional cost and performance benefits in addition to the partitioning benefits.

## 9. Preview: Transforming Batch and Streaming Data

* In the next section, we're going to discuss transforming data. What if your incoming data is not usable in its raw form? We'll learn how to deal with the situation in the next course on data processing. A few years ago, it would have been easy to put the schematic into one diagram, the technology and Market has evolved significantly since then making it difficult to capture everything in one diagram. This is not a consistent or complete schematic but it does give you a sense of how everything fits together. You can see the seven services, the main trees among the forest that will help you find the pathways through the cloud necessary for data engineering. Cloud Pub/Sub sends streaming data into Cloud Dataflow, Cloud Dataflow stores the data and transforms it in BigQuery or in Cloud Bigtable. Cloud Storage holds Batch data and sends it to Cloud Dataproc or to Cloud dataflow. This outlines the data processing Solutions. Then you add in Artificial Intelligence services and finally the user and business interfaces. If you have endless streaming data, Google Cloud has several tools to help process it. These are primarily Cloud Pub/Sub, Cloud Dataflow and Cloud Bigtable, we'll cover these in the next course. You can stream real-time data directly into BigQuery or process it first with Cloud Dataflow, streams are unbounded meaning that there is no defined ends. This creates a special challenge for algorithms that normally rely on the end of data to trigger some action. This discussion is continued in the course on streaming data processing. Streaming is not a low job, rather, it's a separate BigQuery method called streaming inserts. This method allows you to insert one item at a time into a table, new tables can be created from a template table that identifies the schema to be copied. Usually the data is available in seconds, the data enters a streaming buffer where it's held briefly until it can be inserted into the table. Data availability and consistency are considerations for streaming data. Candidates for streaming are our analyses or applications that are tolerant of late or missing data or data arriving out of order or duplicated. The stream can pass through other services introducing additional latency and the possibility of errors. Because streaming data is unbounded, you need to consider the streaming quotas. There's both a daily limit and a concurrency rate limit. You can find more information about these in the online documentation. When should you ingest a stream of data instead of using a batch approach to load data? When the immediate availability of the data is a solution requirements. In most cases, loading batch data is not charged, streaming is charged, use batch loading or repeated batch loading instead of streaming unless that is a requirement of the application.

## 10. Recap

* Let's summarize what we've learned. In this module, we discussed some of the requirements of a modern data warehouse. We introduced BigQuery as the scalable data warehouse solution on Google Cloud platform. Recall that you don't need to provision resources before using BigQuery, unlike with many RDBMS Systems. BigQuery allocates storage and query resources dynamically based on your usage patterns. We discussed how you can load your raw data into your data lake, which you can siege in a Google Cloud Storage bucket before processing it into your data warehouse for analytic workloads. We talked about the BigQuery data transfer service, which helps you build and manage your data warehouse. With a service you can quickly schedule queries and transfers and more. We examined core BigQuery concepts like how it organizes data at the project dataset and table-level, and remember that every table has a schema, which you can enter manually or provide a JSON file for. Those table schemas can also have array data types which can make them repeated and or struct data types, which make them nested. This type of de-normalization will often give you a performance boost because it avoids intensive joins. Lastly, we discussed how you can set up table partitioning and clustering to reduce the amount of data scanned and speed up your queries. That's it for data warehousing. Next in the course we'll cover batch and stream processing and you'll practice building pipelines near your new datasets in the BigQuery.

## QuizNotes

* Which of the following statements on BigQuery are true?
	* Data is run length-encoded and dictionary-encoded
	* Data on BigQuery is physically stored in a redundant way separate from the compute cluster
	* A BigQuery slot is a combination of CPU, memory, and networking resources
* True or False: ARRAYS can be part of regular fields or STRUCTS in BigQuery?
	* True

## Resources

* BigQuery/Dremel White Paper - BigQueryTechnicalWP.pdf