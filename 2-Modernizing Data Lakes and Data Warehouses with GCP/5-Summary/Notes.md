## 1. Course Summary

* Let's review some key concepts we covered in this course on data lakes and Data Warehouses. The primary role of a data engineer is to build data pipelines. The ultimate purpose of a data pipeline is to enable stakeholders in a business to use data to make faster and better decisions to improve their business. While the role of a data engineer is not new, being able to build data pipelines entirely in the Cloud is relatively new. We argue that doing data engineering in the Cloud is advantageous because you can separate compute from storage and you don't have to worry about managing infrastructure and even software. This allows you to spend more time on what matters, getting insights from data. We introduced data lakes and data warehouses and the key differences between the two. At a high level, a data lakes is a place to store unprocessed data. A data warehouse is a place to store transformed data that you ultimately want to use for analytics, machine learning, and dashboards. Next, we discussed Cloud Storage as a data lake solution GCP in some technical depth. We also presented other GCP solutions for low latency requirements, transactional workloads, and structure data. Finally, we introduced BigQuery as the data warehouse solution on Google Cloud. We discussed partitioning and clustering in BigQuery as techniques for improving query performance. Also we talked about EL, ELT, and ETL and how these relate to data lakes and data warehouses. Finally, we presented some reference architectures on GCP for streaming and batch data pipelines. The hope is that these reference architectures can serve as a starting point for your data pipeline.