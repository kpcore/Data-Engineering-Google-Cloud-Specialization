## 1. Exam Tips #3

	Touchstone concepts
	A touchstone concept is a complex or key idea -- something that you would learn in a class AFTER you have learned all the basic dependent concepts. They are used in 	this course because they are a very efficient way for you to learn where you have confidence and where more preparation might be needed.
	This approach is based on the Deeper Learning method of adult learning.
	Example
	People seem to be able to relate well to this example.
	Touchstone: "Unlike other vendor clouds, a subnet spans zones, enabling VMs with adjacent IPs to exist in separate zones, making design for availability easier to 	accomplish since the VMs can share tagged firewall rules."
	To understand the above statement, the basic dependent knowledge that must already be understood includes, Regions, Zones, Subnets, IP Addresses, and Firewall Rules.
	These basic concepts are not taught or reviewed in this course. They are taught in the training courses in this specialization and in the corresponding learning track 	in instructor led training.
	
	Advice: Evaluate the dependent basic concepts
	Assess your confidence with each touchstone concept as it is presented. Don't expect to be taught the basic concept. If you don't understand the touchstone at all, or if you don't feel confident in your knowledge of it, or if you feel there are specific elements of it that you don't understand or are not confident about -- take 	note!
	This is an area where more preparation can be of benefit for you.
	Also -- note where you are confident, know the material, and the dependent concepts on which the touchstone is based. These areas require less preparation for you. So 	noting what you know well can help make your preparation activities more efficient.

## 2. Designing data processing systems

* Designing data processing systems includes designing flexible data representations, designing data pipelines, and designing data processing infrastructure. You're going to see that these three items show up in the first part of the exam with similar but not identical considerations. The same questions or interest show up in different contexts, data representation, pipelines, processing infrastructure. For example, innovations in the technology could make the data representation of a chosen solution outdated, the data processing pipeline might have been implemented in a very involved transformations now available as a single efficient command, and the infrastructure could be replaced by a service with more desirable qualities. However, as you'll see, there are additional concerns with each part. For example, system availability is important to pipeline processing, but not data representation, and capacity is important to processing, but not the abstract pipeline or the representation. Think about data engineering and Google Cloud as a platform consisting of components that can be assembled into solutions. Let's review the elements of GCP that form the data engineering platform. Storage and databases, services that enable storing and retrieving data, different storage and retrieval methods that make them more efficient for specific use cases. Server-based processing, services that enable application code and software to run that can make use of stored data to perform operations, actions, and transformations producing results. Integrated services, combined storage and scalable processing in a framework designed to process data rather than general applications, more efficient and flexible than isolated server database solutions. Artificial intelligence, methods to help identify, tag, categorize, and predict three actions that are very hard or impossible to accomplish in data processing without machine learning. Pre and post-processing services, working with data and pipelines before processing, such as data cleanup, or after processing, such as data visualization. Pre and post-processing are important parts of a data processing solution. Infrastructure services, all the framework services that connect and integrate data processing and IT elements into a complete solution. Messaging, systems, data import, export, security, monitoring, and so forth. Storage and database systems are designed and optimized for storing and retrieving. They're are not really built to do data transformation. It's assumed in their design that the computing power necessary to perform transformations on the data is external to the storage or database. The organization method and access method of each of these services is efficient for specific cases. For example, a Cloud SQL database is very good at storing consistent individual transactions, but it's not really optimized for storing large amounts of unstructured data like video files. Database services perform minimal operations on the data within the context of the access method, for example, SQL queries can aggregate, accumulate, count, and summarize results of a search query. Here's an exam tip, know the differences between Cloud SQL and Cloud Spanner and when to use each. Service differentiators include access methods, the cost or speed of specific actions, sizes of data, and how data is organized and stored. Details and differences between the data technologies are discussed later in this course. An exam tip, know how to identify technologies backwards from their properties. For example, which data technology offers the fastest ingestive data? Which one might you use for ingestive streaming data? Managed services are ones where you can see the individual instance or cluster. Exam tip, managed services still have some IT overhead. It doesn't completely eliminate the overhead or manual procedures, but it minimizes them compared with on-prem solutions. Serverless services remove more of the IT responsibility, so managing the underlying servers is not part of your overhead and the individual instances are not visible. A more recent addition to this list is Cloud Firestore. Cloud Firestore is a NoSQL document database built for automatic scaling. It offers high performance and ease of application development, and it includes a data store compatibility mode. As mentioned, storage and databases provide limited processing capabilities, and what they do offer is in the context of search and retrieval. But if you need to perform more sophisticated actions and transformations on the data, you'll need data processing software and computing power. So where do you get these resources? You could use any of these computing platforms to write your own application or parts of an application that you storage your database services. You could install open-source software such as MySQL, an open-source database, or Hadoop, an open source data processing platform on Compute Engine. Build-your-own solutions are driven mostly by business requirements. They generally involve more IT overhead than using a Cloud platform service. These three data processing services feature in almost every data engineering solution. Each overlaps with the other, meaning that some work could be accomplished in either two or three of these services. Advanced solutions may use one, two or all three. Data processing services combine storage and compute and automate the storage and compute aspects of data processing through abstractions. For example, in Cloud Dataproc, the data abstraction with Spark is a resilient distributed dataset, or RDD, and the processing abstraction is a directed acyclic graph, DAG. In BigQuery, the abstractions are table and query, and in Dataflow, the abstractions are PCollection and pipeline. Implementing storage and processing as abstractions enables the underlying systems to adapt to the workload, and the user data engineer to focus on the data and business problems that they're trying to solve. There's great potential value and product or process innovation using machine learning. Machine learning can make unstructured data, such as logs useful by identifying or categorizing the data and thereby enabling business intelligence. Recognizing an instance of something that exist is closely related to predicting a future instance based on past experience. Machine learning is used for identifying, categorizing, and predicting. It can make unstructured data useful. Your exam tip is to understand the array of machine learning technologies offered on TCP, and when you might want to use each. A data engineering solution involves data ingest, management during processing, analysis, and visualization. These elements can be critical to the business requirements. Here are a few services that you should be generally familiar with. Data transfer services operate online and a data transfer appliance is a shippable device that's used for synchronizing data in the Cloud with an external source. Cloud Data Studio is used for visualization of data after it has been processed. Cloud Dataprep is used to prepare or condition data and to prepare pipelines before processing data. Cloud Datalab is a notebook that is a self-contained workspace that holds code, executes the code, and displays results. Dialogflow is a service for creating chatbots. It uses AI to provide a method for direct human interaction with data. Your exam tip here is to familiarize yourself with infrastructure services that show up commonly in data engineering solutions. Often they're employed because of key features they provide. For example, Cloud Pub/Sub can hold a message for up to seven days providing resiliency to data engineering solutions that otherwise would be very difficult to implement. Every service in Google Cloud platform could be used in a data engineering solution. However, some of the most common and important services are shown here. Cloud Pub/Sub, a messaging service, features in virtually all live or streaming data solutions because it decouples data arrival from data ingest. Cloud VPN, Partner Interconnect or Dedicated Interconnect, play a role whenever there's data on premise, it must be transmitted to services in the Cloud. Cloud IAM, firewall rules, and key management are critical to some verticals, such as the health care and financial industries. Every solution need to be monitored and managed, which usually involves panels displayed in Cloud Console and data sent to Stackdriver monitoring. It's a good idea to examine sample solutions that use data processing or data engineering technologies and pay attention to the infrastructure components of the solution. It's important to know what the services contribute to the data solutions and to be familiar with key features and options. There are a lot of details that I wouldn't memorize, for example, the exact number of IAP supported by a specific instance is something I would expect to look up and not know. Also, the cost of a particular instance type compared with another instance type, the actual values, is not something I would expect I'd need to know as a data engineer. I would look these details up if I needed them. However, the fact that an enforce standard instance has higher IAPs than an N1 standard instance, or that the N4 standard cost more than an N1 standard, are concepts that I would need to know as a data engineer.

## 3. Designing flexible data representations

*  The key concept we'll explore, is understanding how data is stored, and therefore how it's processed. There are different abstractions for storing data. If you store data in one abstraction instead of another, it makes different processes easier or faster. For example, if you store data in a file system, it makes it easier to retrieve that data by name. If you store data in a database, it makes it easier to find data by logic such as SQL. If you store data in a processing system it makes it easier and faster to transform the data not just retrieve it. The data engineer needs to be familiar with basic concepts and terminology of data representation. For example, if a problem is described using the terms rows and columns, since those concepts are used in SQL, you might be thinking about a SQL database such as Cloud SQL or Cloud Spanner. If an exam question describes an entity and a kind which are concepts used in Cloud Datastore, and you don't know what they are, you'll have a difficult time answering the question. You won't have time or resources to look these up during the exam, you need to know them going in. So, exam tip is that it's good to know, how data is stored and what purpose or use case is the storage or database optimized for. Flat serialized data is easy to work with but it lacks structure and therefore meaning. If you want to represent data that has meaningful relationships, you need a method that not only represents the data but also the relationships. CSV, which stands for comma-separated values is a simple file format used to store tabular data. XML, which stands for eXtensible Markup Language was designed to store and transport data and was designed to be self-descriptive. JSON, which stands for JavaScript Object Notation is a lightweight data interchange format based on name-value pairs and an ordered list of values, which maps easily to common objects in many programming languages. Networking transmits serial data as a stream of bits, zeros and ones and data is stored as bits. That means, if you have a data object with a meaningful structure to it you need some method to flatten and serialize the data first so that it's just zeros and ones. Then it can be transmitted and stored and when it's retrieved, the data needs to be de-serialized to restore the structure into a meaningful data object. One example of software that does this is Avro. Avro is a remote procedure call and data serialization framework developed within Apache's Hadoop project. It uses JSON for defining data types and protocols and serializes data in a compact binary format. It's primary uses in Apache Hadoop where it can provide both a serialization format for persistent data and a wire format for communication between Hadoop nodes and from client programs to the Hadoop services. It helps to understand the data type supported in different representations systems. For example there's a data type in modern SQL called Numeric. Numeric is similar to floating point. However, it provides a 38 digit value with nine digits to represent the location of the decimal point. Numeric is very good at storing common fractions associated with money. Numeric avoids the rounding error that occurs in a full floating point representation. So, it's used primarily for financial transactions. Now, why did I mention the numeric data type? Because to understand numeric you have to already know the difference between integer and floating-point numbers. You already have to know about rounding errors that can occur when performing math on some kinds of floating point data representations. So, if you understand this you understand a lot of the other items that you ought to know for SQL and data engineering. You should also make sure you're familiar with these basic data types. Your data in BigQuery is in tables in a dataset. Here's an example of the abstractions associated with a particular technology. You should already know that every resource in GCP exist inside a project and besides security and access control, a project is what links usage of a resource to a credit card. It's what makes up resource billable. Then in BigQuery data stored inside datasets, and datasets contain tables, and tables contain columns. When you process the data BigQuery creates a job. Often the job runs a SQL query, although there are some update maintenance activity supported using data manipulation language or DML. Exam tip," know the hierarchy of objects within a data technology and how they relate to one another". BigQuery is called a columnar store. Meaning that it's designed for processing columns not rows. Column processing is very cheap and fast in BigQuery and row processing is slow and expensive. Most queries only work on a small number of fields, and BigQuery only needs to read those relevant columns to execute a query. Since each column has data of the same type, BigQuery could compress the column data much more effectively. You can stream append data easily to BigQuery tables but you can't easily change existing values. Replicating the data three times also helps the system determine optimal compute nodes to do filtering mixing and so forth. You treat your data in Cloud Dataproc and Spark as a single entity but Spark knows the truth. Your data is stored in Resilient Distributed Datasets or RDDs. RDDs are an abstraction that hides the complicated details of how data is located and replicated in a cluster. Spark partitions data in memory across the cluster and knows how to recover the data through an RDDs lineage, should anything go wrong. Spark has the ability to direct processing to occur where there are processing resources available. Data partitioning, data replication, data recovery, pipelining of processing, are all automated by Spark so you don't have to worry about them. Here's an exam tip, "you should know how different services store data, and how each method is optimized for specific use cases as previously mentioned but also understand the key value of the approach". In this case RDDs hide complexity and allow Spark to make decisions on your behalf. There are a number of concepts that you should know about Cloud Dataflow. Your data in data flow is represented in PCollections. The pipeline shown in this example reads data from BigQuery, does a bunch of processing and writes it's output to cloud storage. In Dataflow each step is a transformation and the collection of transforms makes a pipeline. The entire pipeline is executed by a program called a runner. For development there's a local runner, and for production there's a Cloud Runner. When the pipeline is running on the Cloud each step, each transform, is applied to a PCollection and results in a PCollection. So, the PCollection is a unit of data that traverses the pipeline and each step scales elastically. The idea is to write Python or Java Code and deploy it to Cloud Dataflow, which then executes the pipeline in a scalable serverless context. Unlike Cloud Dataproc, there's no need to launch a cluster or scale the cluster, that's handled automatically. Here are some key concepts from Dataflow, that a data engineer should know: in a Cloud Dataflow pipeline, all the data is stored in a PCollection. The input data is a PCollection. Transformations make changes to a PCollection and then output another PCollection. A PCollection is immutable. That means you don't modify it. That's one of the secrets of its speed. Every time you pass data through a transformation it creates another PCollection. You should be familiar with all the information we've covered in the last few slides but most importantly you should know that a PCollection is immutable and that it's one source of the speed and Cloud Dataflow Pipeline processing. Cloud Dataflow is designed to use the same pipeline, the same operations, the same code for both batch and stream processing. Remember that batch data is also called bounded data and it's usually a file. Batch data has a finite end. Streaming data is also called unbounded data and it might be dynamically generated. For example, it might be generated by sensors or by sales transactions. Streaming data just keeps going. Day after day, year after year with no defined end. Algorithms that rely on a finite end won't work with streaming data. One example is a simple average, you add up all the values and divide by the total number of values. That's fine with batch data because eventually, you'll have all the values. But that doesn't work with streaming data because there may be no end. So, you never know when to divide or what number to use. So what DataFlow does, is it allows you to define a period or window and to calculate the average within that window. That's an example of how both kinds of data can be processed with the same single block of code. Filtering and grouping, are also supported. Many Hadoop workloads can be run more easily and are easier to maintain with Cloud Dataflow. But PCollections and RDDs are not identical. So, existing code has to be redesigned and adapted to run in the Cloud Dataflow pipeline. This can be a consideration because it can add time and expense to a project. Your data in TensorFlow is represented in tensors. Where does the name TensorFlow come from? Well, the flow is a pipeline just like we discussed in Cloud Dataflow but the data object in TensorFlow is not a PCollection but something called a tensor. A Tensor is a special mathematical object that unifies scalars, vectors, and matrixes. Tensor zero is just a single value, a scalar. Tensor one is a vector. Having direction and magnitude. Tensor two is a matrix. Tensor three is a cube-shape. Tensors are very good at representing certain kinds of math functions such as coefficients in an equation, and TensorFlow makes it possible to work with tensor data objects of any dimension. TensorFlow is the open source code that you use to create machine learning models. A tensor is a powerful abstraction because it relates different kinds of data types. There are transformations in tensor algebra that apply to any dimension or rank of tensor. So, it makes solving some problems much easier.

## 4. Design data pipelines

* The next section of the exam guide is designing data pipelines. You already know how the data is represented in Cloud Dataproc, in Spark, it's in the RDD. And in cloud data floats, in p collection and in BigQuery, the data's in data set in tables. And you know that a pipeline is some kinds of sequence of actions or operations to be performed on the data representation. But each service handles a pipeline differently, Cloud Dataproc is a managed Hadoop service. And there are number of things you should know including standard software and the Hadoop ecosystem and components of Hadoop. However, the main thing you should know about Cloud Dataproc is how to use it differently from standard Hadoop. If you store your data external from the cluster, storing HDFS-type data in cloud storage and storing HBase-type data in Cloud Bigtable. Then you can shut your cluster down when you're not actually processing a job, that's very important. What are the two problems with Hadoop? First, trying to tweak all of its settings so it can run efficiently with multiple different kinds of jobs, and second, trying to cost justify utilization. So you search for users to increase your utilization, and that means tuning the cluster. And then if you succeed in making it efficient, it's probably time to grow the cluster. You can break out of that cycle of Cloud Dataproc by storing the data externally. And starting up a cluster and running it for one type of work and then shut it down when you're done. Then you have a stateless Cloud Dataproc cluster, it typically takes only about 90 seconds for the cluster to start up and become active. Cloud Dataproc supports Hadoop, Pig, Hive, and Spark. One exam tip, Spark is important because it does part of its pipeline processing in memory rather than copying from disk. For some applications, this makes Spark extremely fast. With a Spark pipeline, you have two different kinds of operations, transforms and actions. Spark builds its pipeline used an abstraction called a directed graph. Each transform builds additional nodes into the graph but spark doesn't execute the pipeline until it sees an action. Very simply, Spark waits until it has the whole story, all the information. This allows Spark to choose the best way to distribute the work and run the pipeline. The process of waiting on transforms and executing on actions is called, lazy execution. For a transformation, the input is an RDD and the output is an RDD. When Spark sees a transformation, it registers it in the directed graph and then it waits. An action triggers Spark to process the pipeline, the output is usually a result format, such as a text file, rather than an RDD. Transformations and actions are API calls that reference the functions you want them to perform. Anonymous functions in Python, lambda functions, are commonly used to make the API calls. There are self-contained ways to make a request to Spark, each one is limited to a single specific purpose. They're defined inline, making the sequence of the code easier to read and understand. And because the code is used in only one place, the function doesn't need a name and it doesn't clutter the namespace. An interesting and opposite approach where the system tries to process the data as soon as it's received is called, eager execution. TensorFlow, for example, can use both lazy and eager approaches. You can use Cloud Dataproc in BigQuery to gather in several ways. BigQuery is great at running SQL queries, but what it isn't built for is modifying data, real data-processing work. So if you need to do some kind of analysis that's really hard to accomplish in SQL. Sometimes the answer is to extract the data from BigQuery into Cloud Dataproc and let Spark run the analysis. Also, if you needed to alter or process the data, you might read from BigQuery into Cloud Dataproc, process the data, and write it back out to another dataset in BigQuery. Here's another tip, if the situation you're analyzing has data in BigQuery, and perhaps the business logic is better expressed in terms of functional code rather than SQL. You may want to run a Spark job on the data. Cloud Dataproc has connectors to all kinds of GCP resources. You can read from GCP sources, and write to GCP sources, and use Cloud Dataproc as the interconnecting glue. You can also run open source software from the Hadoop ecosystem on a cluster. It would be wise to be at least familiar with the most popular Hadoop software and to know whether alternative services exist in the cloud. For example, Kafka has a messaging service, and the alternative on GCP would be Cloud Pub/Sub. Do you know what the alternative on GCP is to the open-source HBase? That's right, it's Cloud Bigtable and alternative to HTFS, cloud storage. Installing and running Hadoop open source software on Cloud Dataproc cluster is also available. Use initialization actions, which are init scripts, to load, install, and customize software. The cluster itself has limited properties that you can modify. But if you use cloud data proc as suggested, starting a cluster for each kind of work, you won't need to tweak the properties the way you would with Data Center Hadoop. Here is a tip about modifying the Cloud Dataproc cluster, if you need to modify the cluster, consider whether you have the right data-processing solution. There are so many services available on Google Cloud, you might be able to use a service rather than hosting your own on the cluster. If you're migrating Data Center Hadoop to Cloud Dataproc, you may already have customized Hadoop settings that you would to apply to the cluster. You may want to customize some cluster configurations so that it'd work similarly. That's supported in a limited way by cluster properties. Security in Cloud Dataproc is controlled by access to the cluster as a resource.

## 5. Cloud Dataflow pipelines

* Here's some things you should know about Cloud Dataflow. You can write pipeline code in Java or Python. You can use the open source Apache Beam API to define the pipeline and submit it to Cloud Dataflow. Then Cloud Dataflow provides the execution framework. Parallel tasks are automatically scaled by the framework and the same code does real-time streaming and batch processing. One great thing about Cloud Dataflow is that you can get input from many sources and write output to many sinks but the pipeline code in-between remains the same. Cloud Dataflow supports side inputs. That's where you can take data and transform it in one way and transform it in a different way in parallel so that the two can be used together in the same pipeline. Security and Cloud Dataflow is based on assigning roles that limit access to the Cloud dataflow resources. So, your exam tip is, for Cloud Dataflow users use roles to limit access to only dataflow resources not just the project. The dataflow pipeline not only appears in code, but also is displayed in the GCP Console as a diagram. Pipelines reveal the progression of a data-processing solution and the organization of steps which make it much easier to maintain than other code solutions. Each step of the pipeline does a filter, group transform, compare, join, and so on. Transforms, can be done in parallel. Here are some of the most commonly used Cloud Dataflow operations. Do you know which operations are potentially computationally expensive? GroupByKey for one, could consume resources on big data. This is one reason you might want to test your pipeline a few times on sample data to make sure you know how it scales before executing it production scale. Exam tip, a pipeline is a more maintainable way to organize data processing code than for example, an application running on an instance. Do you need to separate dataflow developers of pipelines from dataflow consumers, users of the pipelines? Templates create the single-step of indirection, that allows the two classes of users to have different access. Dataflow Templates enable a new development in execution workflow. The Templates helps separate the development activities and the developers, from the execution activities and the users. The user environment no longer has dependencies back to the development environment. The need for recompilation to run a job is limited. The new approach facilitates a scheduling of batch jobs and opens up more ways for users to submit jobs and more opportunities for automation. Your exam tip here is that Dataflow Templates open up new options for separation of work. That means better security and resource accountability.

## 6. BigQuery and Cloud Dataflow Solutions

* BigQuery is two services, a front-end service that does analysis, and a back-end service that does storage. It offers near real time analysis of massive datasets. The data storage is durable and inexpensive, and you could connect and work with different datasets to drive new insights and business value. BigQuery uses SQL for queries, so it's immediately usable by many data analysts. BigQuery is fast, but how fast is fast? Well, if you're using it with structure data for analytics, it can take a few seconds. BigQuery connects to many services for flexible ingest and output. And it supports nested and repeated fields for efficiency, and user-defined functions for extensibility. Exempt it, access control, and BigQuery is at the project and the data set level. Here is a major design tip, separate, compute, and processing from storage and database enables serverless operations. BigQuery has its own analytic SQL Query front-end available in console and from the command line with BQ. It's just a query engine, the back-end data warehouse part of BigQuery stores data in tables. But BigQuery also has a connector to Cloud Storage, this is commonly used to work directly with CSV files. BigQuery has a connector to Cloud Bigtable as well, if you need more capabilities than a query engine, consider Cloud Dataproc or or Cloud Dataflow. What makes this all possible is the cloud network with petabit speeds. It means that storing data in a service like cloud storage can be almost as fast and in some cases faster than storing the data locally, where it will be processed. In other words, the network turns the concept of Hadoop and HDFS upside down. It's more efficient, once again, to store the data separate from the processing resources. Now we're starting to explore how all these platform parts fit together to create really flexible and robust solutions. Cloud Dataproc can use cloud storage in place of HDFS for persistent data. If you use cloud storage you can, a, shut down the cluster when it's not actually processing data. And, b, start up a cluster per job or per category of work so you don't have to tune the cluster to encompass different kinds of jobs. Cloud Bigtable is a drop-in replacement for Hbase, again separating state from the cluster so the cluster can be shut down when not in use and startup to run a specific kind of job. Cloud Dataproc and Cloud Dataflow can output separate files as CSV files in Cloud Storage. In other words, you can have a distributed set of nodes or servers processing the data in parallel and writing the results out in separate small files. This is an easy way to accumulate distributed results for later collating. Access any storage service from any data processing service. Cloud Dataflow is an excellent ETL solution for BigQuery. Use Cloud Dataflow to aggregate data in support of common queries.

## 7. Design data processing infrastructure

* Now that you have all the pieces, let's start looking at how to put them together into data processing infrastructure. There are a few common assemblies of services and technologies. You'll see them used repeatedly in different context. Here's an example showing the many manual ingests solutions available. You can use the gsutil command line tool to load files into cloud storage. But if you want to load data into BigQuery, you need to be able to identify the structure. The BigQuery command line tool, bq, is good for uploading large data files and for scheduling data file uploads. You can use the command to create tables, define schemas, load data, and run queries. The BQ command is available on a Compute Engine instance in Cloud Shell or you can install it on any client machine as part of the Google Cloud Software Development Kit, or SDK. You can load data into BigQuery from the GCP console. You can stream data using Cloud Dataflow, and from Cloud Logging, or you can use POST calls from a program. And it's very convenient that BigQuery can automatically detect CSV and JSON format files. Another tip, think about data in terms of the three vs, volume, velocity, and variety. How much, how often, and how consistent? This will guide you to the best approach for ingesting the data. In brief, use gsutil for uploading files. Use the Storage Transfer Service when the data is in another location, such as another cloud. And use the Transfer Appliance when the data's too big to transfer electronically.

## 8. Cloud Pub/Sub Solutions

* The Cloud Pub/Sub message broker enables complete ingest solutions. It provides loose coupling between systems and long-lived connections between systems. Exam tip. You need to know how long Cloud Pub/Sub holds messages. It's up to seven days. There are more details about Cloud Pub/Sub that you should know. So if you're not familiar, you might want to review the documentation. Cloud Pub/Sub connects applications and services through a messaging infrastructure. Pub/Sub simplifies event distribution by replacing synchronous point-to-point connections with a single high availability asynchronous bus. You can avoid over-provisioning for traffic spikes with Pub/Sub. If you use Cloud Pub/Sub with Cloud Dataflow, you can get exactly once ordered processing. Cloud Pub/Sub handles exactly once delivery and Cloud Dataflow handles de-duplication ordering and windowing. Separation of duties enables a scalable solution that surpasses bottlenecks in competing messaging systems. This is the pattern you'll often see; Cloud Pub/Sub for data ingest, Cloud Dataflow for data processing and ETL, and BigQuery for interactive analysis. Exam tip; being able to recognize this pattern in case scenarios. This mobile gaming reference architecture illustrates the pattern at work. Popular mobile games can attract millions of players and generate terabytes of game-related data in a short burst of time. This creates pressure on the data processing infrastructure powering to provide timely actionable insights in a cost effective way.

## 9. Data representations, pipelines, and processing infrastructure

* First tip is to ask if the data is useful as is or if it needs to be transformed or cleaned. Second tip is to ask yourself, if this is going to be an ongoing process or a one-time activity. Attempting a Schema design on unstructured data can be useful and instructive to highlight what parts of the data have order or uniqueness to it and what parts are unbounded or optional. Here's a tip, you might need to revisit the data representation to make sure that the pipeline is efficient, example transforming the data on input might radically reduce processing time later in the pipeline. Tip, there might be more than one way to get the same results, example data product versus data flow versus bigquery. All might functionally produce the results desired, but the qualities of each will determine which is correct for the specific case.
